\chapter{Transformaciones lineales}

%-------------------- 3.1 notación
\begin{mynotacion}[\boldmath $F,V,W$]\,\\
    \begin{itemize}
	\item $\textbf{F}$ denota $\textbf{R}$ o $\textbf{C}$.
	\item $V$ y $W$ denota espacios vectoriales sobre $\textbf{F}$.
    \end{itemize}
\end{mynotacion}
\vspace{.5cm}

\mysection{El espacio vectorial de las Transformaciones lineales}
\vspace{.2cm}

\subsection*{Definición y ejemplos de Transformaciones lineales}

%-------------------- 3.2 definición
\begin{mydef}[Transformación lineal]\;\\\\
	Una \textbf{transformación lineal} de $V$ en $W$ es una función $T:V\to W$ con las siguientes propiedades:

	\begin{itemize}
	    \item \textbf{Aditividad}
		$$T(u+ v)=Tu+ Tv \;\mbox{para todo}\; u,v\in V;$$
	    \item \textbf{Homogeneidad}
		$$T(\lambda v)=\lambda(Tv) \;\mbox{para todo}\; \lambda\in \textbf{F}\; \mbox{y todo}\; v\in V.$$
	\end{itemize}
\end{mydef}

%-------------------- 3.3 notación
\begin{mynotacion}[\boldmath $\mathcal{L}\left(V,W\right)$]\; \\\\
    El conjunto de todas las transformaciones lineales de $V$ en $W$ se denota por $\mathcal{L}(V,W)$.
\end{mynotacion}

\setcounter{myteo}{4}
%-------------------- 3.5 Teorema
\begin{myteo}[Transformaciones lineales y bases del dominio]\,\\\\
    Suponga que $v_1,\ldots,v_n$ es una base de $V$ y $w_1,\ldots,w_n\in W$. Entonces, existe una única transformación lineal $T:V\to W$ tal que 
    $$T(v_j)=w_j$$
    para cada $j=1,\ldots,n$.\\\\
	Demostración.-\; Primero demostremos la existencia de una transformación lineal $T$, con la propiedad deseada. Defina $T:V\to W$ por
	$$T(c_1v_1+\cdots+c_nv_n)=c_1w_1+\cdots+c_nw_n.$$
	donde $c_1,\ldots,c_n$ son elementos arbitrarios de $\textbf{F}$. La lista $v_1,\ldots,v_n$ es una base de $V$, y por lo tanto, la ecuación anterior de hecho define una función $T$ para $V$ en $W$ (porque cada elemento de $V$ puede ser escrito de manera única en la forma $c_1v_1,\ldots,c_nv_n$). Para cada $j$, tomando $c_j=1$ y las otras $c's$ igual a $0$ demostramos la existencia de $T(v_j)=w_j$.\\
	Si $u,v\in V$ con $u=a_1v_1,\ldots,a_nv_n$ y $v=c_1v_1,\ldots,c_nv_n$, entonces
	$$
	\begin{array}{rcl}
	    T(v+u) &=& T\left[(a_1+c_1)v_1+\ldots+(a_n+a_n)v_n\right]\\\\
		   &=& (a_1+c_1)w_1+\ldots+(a_n+c_n)w_n\\\\
		   &=& (a_1w_1+\ldots+a_nw_n)+(c_1w_1+\ldots+c_nw_n)\\\\
		   &=& T(u)+T(v).
	\end{array}
	$$
	Similarmente, si $\lambda \in \textbf{F}$ y $v=c_1v_1+\cdots+c_nv_n$, entonces
	$$
	\begin{array}{rcl}
	    T(\lambda v) &=& T(\lambda c_1v_1+\cdots+\lambda c_nv_n)\\\\
			 &=&\lambda c_1w_1+\cdots+\lambda c_nw_n\\\\
			 &=&\lambda(c_1w_1+\cdots+c_nw_n)\\\\
			 &=&\lambda T(v).
	\end{array}
	$$
	Así, $T$ es una transformación lineal para $V$ en $W$.\\
	Para probar que es único, suponga que $T\in \mathcal{L}(V,W)$ y que $T(v_j)=w_j$ para $j=1,\ldots,n$. Sea $c_1,\ldots,c_n\in \textbf{F}$. La Homogeneidad de $T$ implica que $T(c_jv_j)=c_jw_j$ para $j=1,\ldots,n$. La Aditividad de $T$ implica que 
	$$T(c_1v_1+\cdots+c_nv_n)=c_1w_1+\cdots+c_nw_n.$$
	Por lo tanto, $T$ se determina de forma única en $\span(v_1,\ldots,v_n)$ para la ecuación de arriba. Porque $v_1,\ldots,v_n$ es una base de $V$, esto implica que $T$ es determinado únicamente en $V$.
\end{myteo}

\vspace{.5cm}

\subsection*{Operaciones algebraicas en \boldmath $\mathcal{L}(V,W)$}

\setcounter{mydef}{5}
%-------------------- 3.6 definición
\begin{mydef}[Adición y multiplicación escalar en \boldmath$\mathcal{L}\left(V,W\right)$]\,\\\\
    Suponga que $S,T\in \mathcal{L}(V,W)$ y $\lambda \in \textbf{F}$. La \textbf{suma} $S+T$ y el \textbf{producto} $\lambda T$ son transformaciones lineales para $V$ en $W$ definida por
    $$(S+T)(v)=S(v)+T(v) \quad \mbox{y} \quad (\lambda T)(v)=\lambda(Tv)$$
    para todo $v\in V$.
\end{mydef}

%-------------------- 3.7 teorema
\begin{myteo}[\boldmath$\mathcal{L}(V,W)$ es un espacio vectorial]\,\\\\
    Con las Operaciones de adición y multiplicación escalar como se definió, $\mathcal{L}\left(V,W\right)$ es un espacio vectorial.\\\\
	Demostración.-\; Verificaremos cada propiedad.

	\begin{itemize}

	    \item Conmutatividad.- Sean $S,T\in \mathcal{L}(V,W)$ y $v\in V$, tenemos
		$$(S+T)(v)=S(v)+T(v)=T(v)+S(v)=(T+S)(v).$$
		Por lo tanto, la adición es comunutativa.\\

	    \item Asociatividad.- Saen $R,S,T\in \mathcal{L}(V,W)$ y $v\in V$, tenemos
		$$
		\begin{array}{rcl}
		    \left[(R+S)+T\right](v) &=& (R+S)(v)+T(v)\\\\
					    &=& R(v)+S(v)+T(v)\\\\
					    &=& R(v)+\left[S(v)+T(v)\right]\\\\
		\end{array}
		$$
		Por lo que la adición es asociativa. Luego, sean $a,b\in \textbf{F}$, entonces
		$$\left[(ab)T\right](v)=(ab)T(v)=a\left[bT(v)\right]=\left[a(bT)\right](v).$$
		Por lo tanto, la multiplicación es asociativa.\\

	    \item Identidad aditiva.- Sea $0\in \mathcal{L}(V,W)$ denotado como transformación cero, sean también $T\in \mathcal{L}(V,W)$ y $v\in V$. Entonces,
		$$(T+0)(v)=T(v)+0(v)=T(v).$$
		Por lo tanto, la transformación cero es la identidad aditiva.\\

	    \item Inverso aditivo.- Sean $T\in \mathcal{L}(V,W)$ y $v\in V$, y definamos a $(-T)\in \mathcal{L}(V,W)$ por $(-T)(v)=-T(v)$, entonces
		$$\left[T+(-T)\right](v)=T(v)+(-T)(v)=T(v)-T(v)=0,$$
		Por lo que, $(-T)$ es el inverso aditivo para cada $T\in \mathcal{L}(V,W)$.\\

	    \item Identidad multiplicativa.- Sea $T\in \mathcal{L}(V,W)$. Entonces,
		$$(1T)(v)=1(T(v))=T(v).$$
		Así la identidad multiplicativa de $\textbf{F}$ es la identidad multiplicativa de la multiplicación escalar.\\

	    \item Propiedad distributiva.- Sean $S,T\in \mathcal{L}(V,W)$, $a,b\in \textbf{F}$ y $v\in V$. Entonces,
		$$
		\begin{array}{rcl}
		    \left[a(S+T)\right](v) &=& a(Sv+Tv)\\\\
					   &=& aS(v)+aT(v)\\\\
					   &=& (aS)(v)+(aT)(v)\\\\
		\end{array}
		$$
		\begin{center}
		    y
		\end{center}

		$$
		\begin{array}{rcl}
		    \left[(a+b)T\right](v) &=& (a+b)T(v)\\\\
					   &=& aT(v)+bT(v)\\\\
					   &=& (aT)(v)+(bT)(v).\\\\
		\end{array}
		$$
		
	\end{itemize}

	Por lo tanto, $\mathcal{L}(V,W)$ es un espacio vectorial.
\end{myteo}


Por lo general, no tiene sentido multiplicar dos elementos de un espacio vectorial, pero para algunos pares de combinaciones lineales existe un producto útil. Necesitaremos un tercer espacio vectorial, así que para el resto de esta sección supongamos que $U$ es un espacio vectorial sobre $\textbf{F}$.

%-------------------- 3.8 definición
\begin{mydef}[Producto de combinaciones lineales]\,\\\\
    Si $T\in \mathcal{L}(U,V)$ y $S\in \mathcal{L}(V,W)$, entonces el producto $ST\in \mathcal{L}(U,W)$ es definido por
    $$(ST)(u)=S(Tu)$$
    para $u\in U$.
\end{mydef}

En otras palabras, $ST$ es solo la composición habitual $S\circ T$ de dos funciones, pero cuando ambas funciones son lineales, la mayoría de los matemáticos escriben $ST$ en lugar de $S\circ T$. Debe verificar que $ST$ es de hecho una transformación lineal de $U$ a $W$ siempre que $T\in \mathcal{L}(U,V)$ y $S\in \mathcal{L}(V,W)$. Tenga en cuenta que $ST$ se define solo cuando $T$ se transforma en el dominio de $S$.

%-------------------- 3.9 teorema
\begin{myteo}[Propiedades algebraicas de producto de transformaciones lineales]\,\\\\
    \textbf{Asociatividad}
    $$(T_1T_2)T_3 = T_1(T_2T_3)$$
    siempre que $T_1$, $T_2$ y $T_3$ sean transformaciones lineales tales que los productos tengan sentido (lo que significa que $T_3$ se transforma en el dominio de $T_2$, y $T_2$ se transfora en el dominio de $T_1$).\\\\
	Demostración.-\; Para $x$ en el dominio de $T_3$, tenemos
	$$
	\begin{array}{rcl}
	    \left[(T_1T_2)T_3\right](x) &=& (T_1T_2)\left[T_3(x)\right]\\\\
					&=& T_1\left[T_2\left[T_3(x)\right]\right]\\\\
					&=& T_1\left[(T_2T_3)(x)\right]\\\\
					&=& \left[T_1(T_2T_3)\right](x).
	\end{array}
	$$

    \textbf{Identidad}
    $$TI=IT=T$$
    siempre que $T\in \mathcal{L}(V,W)$ (el primer $I$ es la transformación de indentidad en $V$, y el segundo $I$ es la transformación de identidad en $W$).\\\\
	Demostración.-\; Para $v\in V$, se tiene
	$$
	\begin{array}{rcl}
	    TI(v) &=& T\left[I(v)\right]\\\\
				 &=& T(v)\\\\
				 &=& I\left[T(v)\right]\\\\
				 &=& IT(v).\\\\
	\end{array}
	$$
	Por lo tanto, $TI=IT=I$.\\

    \textbf{Propiedades distributivas}
    \begin{center}
	$(S_1+S_2)T=S_1T+S_2T\quad$ y $\quad S(T_1+T_2)=ST_1+ST_2$
    \end{center}
    siempre que $T,T_1,T_2\in \mathcal{L}(U,V)$ y $S,S_1,S_2\in \mathcal{L}(V,W)$.\\\\
	Demostración.-\; Para $u\in U$, se tiene
	$$
	\begin{array}{rcl}
	    \left[(S_1+S_2)T\right](u) &=& (S_1+S_2)\left[T(u)\right]\\\\
				       &=& S_1\left[T(u)\right]+S_2\left[T(u)\right]\\\\
				       &=& S_1T(u)+S_2T(u)\\\\
				       &=& (S_1T+S_2T)(u).\\\\
	\end{array}
	$$

	\begin{center}
	    y
	\end{center}

	$$
	\begin{array}{rcl}
	    \left[T(S_1+S_2)\right](u) &=& T\left[(S_1+S_2)(u)\right]\\\\
				       &=& T\left[S_1(u)+S_2(u)\right]\\\\
				       &=& T\left[S_1(u)\right]+T\left[S_2(u)\right]\\\\
				       &=& (TS_1+TS_2)(u).\\\\
	\end{array}
	$$
\end{myteo}


La multiplicación de aplicaciones lineales no es conmutativa. En otras palabras, no es necesariamente cierto que $ST=TS$, incluso si ambos lados de la ecuación tienen sentido.

%-------------------- 3.10 Ejemplo
\begin{myejem}
    Suponga $D\in \mathcal{L}\left[\mathcal{P}(\textbf{R}),\mathcal{P}(\textbf{R})\right]$ es la transformación de diferenciación definido en el ejemplo 3.4 y $T\in \mathcal{L}\left[\mathcal{P}(\textbf{R}),\mathcal{P}(\textbf{R})\right]$ es la multiplicación por la transformación $x^2$ definida tempranamente en esta sección. Muestre que $TD\neq DT$.\\\\
	Demostración.-\;  Se tiene 
	$$\left[(TD)p\right](x)=x^2p'(x) \quad \mbox{pero}\quad \left[(DT)p\right](x)=x^2p'(x)+2xp(x).$$
	En otras palabras, no es lo mismo derivar y luego multiplicar por  $x^2$ que multiplicar por $x^2$ y luego derivar.
\end{myejem}

%-------------------- 3.11 Teorema
\begin{myteo}[Transformaciones lineales toman \boldmath$0$ a $0$]\,\\\\
    Suponga $T$ es una transformación lineal para $V$ en $W$. Entonces $T(0)=0$.\\\\
	Demostración.-\; Por la aditividad, se tiene
	$$T(0)=T(0+0)=T(0)+T(0).$$
	Agregue el inverso aditivo de $T(0)$ cada lado de la ecuación anterior para concluir que $T(0)=0$.
\end{myteo}

\begin{lema}
Suponga $T\in \mathcal{L}(V,W)$ y $v_1,\ldots, v_m$ una lista de vectores en $V$ tal que $Tv_1,\ldots,Tv_m$ es una lista linealmente independiente en $W$. Demostrar que $v_1,\ldots,v_m$ es linealmente independiente.\\\\
    Demostración.-\; Sea para $c_i\in \textbf{F}$ tal que 
    $$c_1v_1+c_2v_2+\cdots+c_nv_n=0.$$
    Luego, multiplicamos por $T$ a ambos lados de la ecuación anterior,
    $$T(c_1v_1+c_2v_2+\cdots+c_nv_n)=T(0).$$
    Por la definición 1.6 y el teorema 3.11 tenemos que
    $$c_1Tv_1+c_2Tv_2+\cdots+c_nTv_n=0.$$
    Entonces, como $Tv_1,\ldots,Tv_m$ es una lista linealmente independiente en $W$.
\end{lema}


\mysection{Espacios Nulos y Rangos}
\;\\
\subsection*{Espacio Nulo (kernel) e Inyectividad}


%--------------------Definición 3.12
\begin{mydef}[Espacio nulo, null \boldmath $T$]\,\\\\
    Para $T\in \mathcal{L}(V,W)$, el \textbf{espacio nulo} de $T$ denotado por null $T$ es el subconjunto de $V$ formado por aquellos vectores que $T$ transforma a $0$:
    $$\mbox{null } T = \left\{v\in V : Tv=0\right\}.$$
\end{mydef}

Algunos matemáticos usan el termino \textbf{kernel} en lugar de \textbf{espacio nulo}. La palabra "null" significa cero.\\\\

El siguiente resultado demuestra que el espacio nulo o kernel de cada transformación lineal es un subespacio del dominio. En particular, $0$ está en el espacio nulo de cada transformación lineal.

\setcounter{myteo}{13}
%--------------------Teorema 3.14
\begin{myteo}[El espacio nulo es un subespacio]\,\\\\
    Suponga que $T\in \mathcal{L}(V,W)$. Entonces, null $T$ es un subespacio de $V$.\\\\
	Demostración.-\; Ya que $T$ es una transformación lineal, entonces sabemos por 3.11 que $T(0)=0$. Por lo tanto, $0\in \mynull T.$\\
	Supongamos ahora que $u,v\in \mynull T$. Entonces,
	$$T(u+v)=Tu+Tv=0+0=0.$$
	De ahí, $u+v\in \mbox{null } T$. Así null $T$ es cerrado bajo la adición.\\
	Luego, supongamos que $u\in \mynull T$ y $\lambda \in \textbf{F}$
	$$T(\lambda u)=\lambda T u=\lambda 0 = 0.$$
	Por lo que, $\lambda u \in \mynull T$. Así, null $T$ es cerrado bajo la multiplicación por escalares. Por 1.34, null $T$ es un subespacio de $V.$
\end{myteo}

%--------------------Definición 3.15
\begin{mydef}[Inyectiva]\,\\\\
    Una función $T:V\to W$ es llamada \textbf{inyectiva} si $Tu=Tv$ implica $u=v$.\\

    o $T$ es inyectiva si $u\neq v$ implica que $Tu\neq Tv$.\\\\
\end{mydef}

Muchos matemáticos usan el termino \textbf{uno a uno}.\\

El siguiente resultado dice que podemos comprobar si una transformación lineal es inyectiva al verificar si $0$ es el único vector que se asigna a $0$.

%--------------------Teorema 3.16
\begin{myteo}[Inyectividad es equivalente decir que el espacio nulo es igual a \boldmath$\left\{0\right\}$]\,\\\\
    Sea $T\in \mathcal{L}(V,W)$. Entonces, $T$ es inyectiva si y solo si null $T=\left\{0\right\}$.\\\\
	Demostración.-\; Primero suponga que $T$ es inyectiva. Queremos demostrar que null $T$ = $\left\{0\right\}$. Sabemos por 3.11 que $\left\{0\right\}\subset \mbox{null }T$. Para probar que $\mynull T \subset \left\{0\right\}$, suponga $v\in \mbox{null }T$. Entonces,
	$$T(v)=0=T(0).$$
	Ya que $T$ es inyectiva, implica que $v=0$. Así, podemos concluir que null $T=\left\{0\right\}$. Como queriamos.\\

	Para probar la implicación en la otra dirección. Si null $T=\left\{0\right\}$, entonces demostrarmos que $T$ es inyectiva. Para esto, suponga que $u,v\in V$ y $Tu=Tv$, de donde
	$$0=Tu-Tv=T(u-v)$$
	Así, $u-v$ está en null $T$, el cual es igual a $\left\{0\right\}$. Por lo tanto, $u-v=0$, implica que $u=v$. Concluimos que, $T$ es inyectiva.
\end{myteo}
\vspace{.5cm}

\subsection*{Rango y sobreyectividad}

Damos un nombre al conjunto de resultados de una función.

%-------------------- Definición 3.17
\begin{mydef}[Rango]\,\\\\
    Para $T$ una función de $V$ en $W$, el \textbf{rango} de $T$ es el subconjunto de $W$ que consta de aquellos vectores que son de la forma $Tv$ para algunos $v\in V$:
$$\range T=\left\{Tv:v\in V\right\}.$$
\end{mydef}

Algunos matemáticos usan la palabra \textbf{imagen} en lugar de rango.\\

EL siguiente resultado muestra que el rango de cada transformación lineal es un subespacio del espacio vectorial en el que se esta transformando.

\setcounter{myteo}{18}
%--------------------Teorema 3.19
\begin{myteo}[El rango es un subespacio]\,\\\\
    Si $T\in \mathcal{L}(V,W)$, entonces el rango de $T$ es un subespacio de $W$.\\\\
	Demostración.-\; Suponga que $T\in \mathcal{L}(V,W)$. Entonces por 3.11, $T(0)=0$, lo que implica que $0\in \range T$.\\
	Si $w_1,w_2\in$ range $T$, entonces existe $v_1,v_2\in V$ tal que $Tv_1=w_1$ y $Tv_2=w_2$. Así,
	$$T(v-1+v-2)=Tv_1+v_2=w_1+w_2.$$
	Ya que $w_1+w_2\in$ rango $T$. Por lo tanto, rango $T$ es cerrado bajo la adición.\\
	Si $w\in$ rango $T$ y $\lambda\in \textbf{F}$, entonces existe $v\in V$ tal que $T_v=w$.
	Por lo que, 
	$$T(\lambda v)=\lambda Tv=\lambda w.$$
	Así, $\lambda w\in \range T$. Por lo tanto, rango $T$ es cerrado bajo la multiplicación por escalares. Por 1.34, el  $\range T$ es un subespacio de $W$.
\end{myteo}

%--------------------Definición 3.20
\begin{mydef}[Sobreyectiva]\,\\\\
	Una función $T:V\to W$ es llamada \textbf{sobreyectiva} si su rango es igual a $W$.\\\\
\end{mydef}

Que una transformación lineal sea sobreyectiva depende del espacio vectorial al que se proyecte.\\

%-------------------- Lema 1.2
\begin{lema}
Suponga $v_1,\ldots, v_n$ genera $V$ y $\mathcal{L}(V,W)$. Demostrar que la lista $Tv_1,\ldots,Tv_n$ genera $\range T$.\\\\
    Demostración.-\; Suponga que $(v_1,\ldots,v_n)$ genera $V$ y $T\in \mathcal{L}(V,W)$ es sobreyectiva. Sea $w\in W$. Ya que, $T$ es sobreyectiva existe $v\in V$ tal que $Tv=w$. Luego, por el hecho de que $(v_1,\ldots,v_n)$ genera $V$, existe $a_1,\ldots,a_n\in F$ tal que
    $$v=a_1v_1+\cdots+a_nv_n.$$
    Multiplicando $T$ a ambos lados,
    $$Tv=a_1Tv_1+\cdots+a_nTv_n.$$
    Que $Tv=w$, entonces $w\in \span(Tv_1,\ldots,Tv_n)$. Sabemos que $w$ es un vector arbitrario de $W$, lo implica que $Tv_1,\ldots,Tv_n$ genera $W$.
\end{lema}


\subsection*{Teorema fundamental de las transformaciones lineales}

\setcounter{myteo}{21}
%--------------------Teorema 3.22
\begin{myteo}[Teorema fundamental de las transformaciones lineales]\,\\\\
	Suponga que V es de dimensión finita y $T\in \mathcal{L}(V,W)$. Entonces, $T$ es de dimensión finita y 
	$$\dim V = \dim \mynull T + \dim \range T.$$
	    Demostración.-\; Sea $u_1,\ldots,u_m$ una base de null $T$; en consecuencia $\dim$ null $T=m$. Por 2.33, la lista linealmente independiente $u_1,\ldots,u_m$ puede extenderse a una base
	    $$u_1,\ldots,u_m,v_1,\ldots,v_n$$
	    de $V$. Así la $\dim V=m+n$. Para completar la prueba, sólo necesitamos demostrar que rango $T$ es de dimensión finita y la dim rango $T=n$. Es decir, mostrarmos que  $Tv_1,\ldots,Tv_n$ es una base del rango de $T$.\\

	    Sea $v\in V$. Ya que, $u_1,\ldots,u_m,v_1,\ldots,v_n$ genera $V$. Entonces,
	    $$v=a_1u_1+\cdots+a_mu_m+b_1v_1+\cdots+b_nv_n.$$
	    donde las $a's$ y $b's$ son en $\textbf{F}$. Aplicando $T$ a ambos lados de la ecuación, obtenemos
	    $$T_v=b_1Tv_1+\ldots+b_nTv_n.$$
	    El termino con la forma $Tu_j$ desaparece, porque cada $u_j$ esta en null $T$. De donde, la última ecuación implica que $Tv_1,\ldots,Tv_n$ genera rango $T$. En particular, rango $T$ es de dimensión finita.\\

	    Ahora, demostremos que $Tv_1,\ldots,Tv_n$ es linealmente independiente. Supongamos $c_1,\ldots,c_n\in \textbf{F}$ y 
	    $$c_1Tv_1+\cdots+c_ntv_n=0.$$
	    Entonces,
	    $$T\left(c_1v_1+\cdots +c_nv_n\right)=0.$$
	    Por lo tanto,
	    $$c_1v_1+\cdots c_nv_n \mynull T.$$
	    Luego, ya que $u_1,\ldots,u_m$ genera $\mynull T$, podemos escribir
	    $$c_1v_1+\cdots + c_nv_n = d_1u_1+\cdots + d_mu_m.$$
	    para $d's$ en $\textbf{F}$. Por el hecho de que $u_1,\ldots,u_m,v_1,\ldots,v_n$ es linealmente independiente. Entonces,  todos los $c's$ y $d's$ son $0$. Por lo tanto, $Tv_1,\ldots,Tv_n$ es linealmente independiente y por ende es una base del rango $T$, como queriamos demostrar.

\end{myteo}

Ahora podemos demostrar que ninguna transformación lineal desde un espacio vectorial de dimensión finita hacia un espacio vectorial "más pequeño" puede ser inyectivo, donde "más pequeño" se mide por la dimensión.

%-------------------- teorema 3.23
\begin{myteo}[Una transformación a un espacio de menor dimensión no es inyectiva]\,\\\\
    Suponga que $V$ y $W$ son espacios vectoriales de dimensión finita tales que $\dim V>\dim W$. Entonces, ninguna transformación lineal de $V$ en W es inyectiva.\\\\
	Demostración.-\; Sea $T\in \mathcal{L}(V,W)$. Entonces,
	$$
	\begin{array}{rcl}
	    \dim \mynull T &=& \dim V - \dim \range T\\\\
				 &\geq & \dim V - \dim W\\\\
				 &>& 0.
	\end{array}
	$$
	Donde la ecuación de arriba viene dado por el teorema fundalmental de las transformaciones lineales (3.22). La desigualdad anterior establece que $\dim$ null $T > 0$. Esto significa que $\null T$ contiene vectores distintos a cero. Por 3.16 concluimos que $T$ es no inyectiva.
\end{myteo}

El siguiente resultado muestra que ninguna transformación lineal de un espacio vectorial de dimensión finita a un espacio vectorial "más grande" puede ser sobreyectivo, donde "más grande" se mide por dimensión.

%-------------------- teorema 3.24
\begin{myteo}[Una transformación a un espacio de mayor dimensión no es suryectiva]\,\\\\
    Suponga que $V$ y $W$ son espacios vectoriales de dimensión finita tales que $\dim V<\dim W$. Entonces, ninguna transformación lineal de $V$ en $W$ es sobreyectiva.\\\\
	Demostración.-\; Sea $T\in \mathcal{L}(V,W)$. Entonces,
	$$
	\begin{array}{rcl}
	    \dim \range T &=& \dim V - \dim \mynull T\\\\
				  &\leq & \dim V\\\\
				  &<& \dim W.
	\end{array}
	$$
	Donde la ecuación de arriba viene dado por el teorema fundalmental de las transformaciones lineales (3.22). La desigualdad anterior establece que range $T < \dim W$. Esto significa que range $T$ no puede ser igual a $W$.  Por lo tanto, $T$ es no sobreyectiva.\\\\
\end{myteo}

Como veremos a continuación, 3.23 y 3.24 tienen importantes consecuencias en la teoría de ecuaciones lineales. La idea aquí es expresar cuestiones sobre sistemas de ecuaciones lineales en términos de transformaciones lineales.

%-------------------- ejemplo 3.25
\begin{myejem}
    Reformule en términos de transformaciones lineales la pregunta de si un sistema homogéneo de ecuaciones lineales tiene una solución distinta de cero.\\\\
	Respuesta.-\; Sean los enteros $m$ y $n$ y sea $A_{j,k}\in \textbf{F}$ para $j=1,\ldots,m$ y $k=1,\ldots,n$. Considere el sistema homogéneo de ecuaciones lineales
	$$
	\begin{array}{rcl}
	    \displaystyle\sum_{k=1}^n A_{1,k}x_k &=& 0\\
						 &\vdots&\\
	     \displaystyle\sum_{k=1}^n A_{m,k}x_k &=& 0.
	\end{array}
	$$
	Está claro que $x_1=\cdots=x_n=0$ es la solución al sistema de ecuaciones; la cuestión aquí es si existen otras soluciones.\\

	Defina $T:\textbf{F}^n \to \textbf{F}^m$ por
	$$T(x_1,\ldots,x_n)=\left(\sum_{k=1}^n A_{1,k}x_k,\ldots,\sum_{k=1}^n A_{m,k}x_k\right).$$
	La ecuación $T(x_1,\ldots, x_n)=0$ (el $0$ aquí es la identidad aditiva en $\textbf{F}^m$, es decir, la lista de longitud $m$ de todos los $0$) es la misma que el sistema homogéneo de ecuaciones lineales anterior.\\
	Así pues, queremos saber si null $T$ es estrictamente mayor que $\left\{0\right\}$. En otras palabras, podemos reformular nuestra pregunta sobre las soluciones no nulas de la siguiente manera (por 3.16): ¿Qué condición asegura que $T$ no es inyectiva?
\end{myejem}

%-------------------- teorema 3.26
\begin{myteo}[Sistemas homogéneos de ecuaciones lineales]\,\\\\
    Un sistema homogéneo de ecuaciones lineales con más variables que ecuaciones tienen soluciones distintas de cero.\\\\
	Demostración.-\; Usemos la notación y el resultado de arriba. De donde, $T$ es una transformación lineal de $\textbf{F}^n$ en $\textbf{F}^m$, y tenemos un sistema homogéneo de $m$ ecuaciones lineales con $n$ variables $x_1,\ldots,x_n$. Por 3.23 vemos que $T$ no es inyectiva si $n>m$.
\end{myteo}

\setcounter{myteo}{28}
%-------------------- teorema 3.29
\begin{myteo}[Sistemas no homogéneos de ecuaciones lineales]\,\\\\
    Un sistema no homogéneo de ecuaciones lineales con más ecuaciones que variables no tienen solución para alguna elección de los términos constantes.\\\\
	Demostración.-\; $T$ es una transformación lineal para $\textbf{F}^n$ en $\textbf{F}^m$, y tenemos un sistema de $m$ ecuaciones con $n$ variables $x_1,\ldots,x_n$. Por 3.24 vemos que $T$ es no sobreyectiva si $n<m$.
\end{myteo}



\mysection{Matrices}

\subsection*{Representando una transformación lineal por una matriz}

Sabemos que si $v_1,\ldots,v_n$ es una base de $V$ y $T:V\to W$ es lineal, entonces los valores de $Tv_1,\ldots,Tv_n$ determina los valores de $T$ en vectores arbitrarios en $V$ (3.5). Como veremos, las matrices se usan como un método eficiente para registrar los valor de las $Tv_j's$ en términos de una base de $W$.

%--------------------teorema 3.30
\begin{mydef}[Matriz, \boldmath$A_{j,k}$]\;\\\\
    Sea $m$ y $n$ que denota enteros positivos. Un \textbf{matriz} $A$ de $m$ por $n$ es un arreglo rectangular de elementos de $\textbf{F}$ con $m$ filas y $n$ columnas:
    $$
    A=
    \begin{pmatrix}
	A_{1,1} & \cdots & A_{1,n}\\
	\vdots &  & \vdots\\
	A_{m,1} & \cdots & A_{m,n}
    \end{pmatrix}
    $$
    La notación $A_{j,k}$ denota la entrada en la fila $j$, columna $k$ de $A$. En otras palabras, el primer indice se refiere al número de fila y el segundo indice se refiere al número de columna.
\end{mydef}

Ahora, veamos la definición clave de esta sección

\setcounter{mydef}{31}
%--------------------definición 3.32
\begin{mydef}[Matriz de una transformación lineal, \boldmath$\mathcal{M}\left(T\right)$]\;\\\\
    Suponga que $T\in \mathcal{L}(V,W)$ y $v_1,\ldots,v_n$ es una base de $V$ y $w_1,\ldots,w_m$ es una base de $W$. La matriz de $T$ con respecto a estas bases es la matriz $m$ por $n$, $\mathcal{M}(T)$, cuyas entradas $A_{j,k}$ están definidas por
    $$Tv_k = A_{1,k}w_1+\cdots+A_{m,k}w_m.$$
    Si las bases no están claras por el contexto, entonces la notación $\mathcal{M}\left(T,(v_1,\ldots,v_n),(w_1,\ldots,w_m)\right)$  es usada.
\end{mydef}

Para recordar cómo se construye $\mathcal{M}(T)$ a partir de $T$, puedes escribir en la parte superior de la matriz los vectores base $v_1,\ldots,v_n$ para el dominio y a la izquierda los vectores base $w_1,\ldots,w_m$ para el espacio vectorial al que transforma $T$, como sigue:
$$
\mathcal{M}(T)=
    \begin{array}{cc}
	&v_1 \quad \cdots\quad v_k \quad \cdots \quad v_n\\\\
	\begin{array}{c}
	    w_1\\\\
	    \vdots\\\\
	    w_m
	\end{array}
	&
	\begin{pmatrix}
	    \qquad\qquad\qquad&A_{1,k}&\qquad\qquad\qquad\\\\
		  &\vdots&\\\\
		  &A_{m,k}&
	\end{pmatrix}
    \end{array}
$$

La columna $k$-enésimo de $\mathcal{M}(T)$ consiste en los escalares necesarios para escribir $Tv_k$ como una combinación lineal de $w_1,\ldots,w_m$:
$$Tv_k=\sum_{j=1}^m A_{j,k}w_j.$$

Es decir, $Tv_{k}$ puede calcularse a partir de $\mathcal{M}(T)$ multiplicando cada entrada de la columna $k$ por la correspondiente $w_j$ de la columna de la izquierda, y sumando después los vectores resultantes.\\\\


\subsection*{Adición y multiplicación escalar de matrices}

Para el resto de esta sección, supongamos que V y W son de dimensión finita y que se ha elegido una base para cada uno de estos espacios vectoriales. Así, para cada transformación lineal de V a W, podemos hablar de su matriz (con respecto a las bases elegidas, por supuesto).

\setcounter{mydef}{34}
%--------------------teorema 3.35
\begin{mydef}[Adición de matrices]\;\\\\
    La \textbf{suma de dos matrices de un mismo tamaño} que se obtiene sumando las entradas correspondientes de las matrices:
    $$
    \begin{pmatrix}
	A_{1,1}&\cdots&A_{1,n}\\
	\vdots&&\vdots\\
	A_{m,1}&\cdots&A_{m,n}
    \end{pmatrix}
    +
    \begin{pmatrix}
	C_{1,1}&\cdots&C_{1,n}\\
	\vdots&&\vdots\\
	C_{m,1}&\cdots&C_{m,n}
    \end{pmatrix}
    =
    \begin{pmatrix}
	A_{1,1}+C_{1,1}&\cdots&A_{1,n}+C_{1,n}\\
	\vdots&&\vdots\\
	A_{m,1}+C_{m,1}&\cdots&A_{m,n}+C_{m,n}
    \end{pmatrix}
    $$
    En otras palabras, $(A+C)_{j,k}=A_{j,k}+C_{j,k}$.
\end{mydef}

En el siguiente resultado, se supone que se utilizan las mismas bases para las tres transformaciones lineales $S + T, S ,$ y $T$.

%--------------------teorema 3.36
\begin{myteo}[La matriz de la suma de transformaciones lineales]\;\\\\
    Suponga $S,T\in \mathcal{L}(V,W)$. Entonces, $\mathcal{M}(S+T)=\mathcal{M}(S)+\mathcal{M}(T)$.\\\\
    Demostración.-\; Sean $v_1,\ldots, v_n$ una base de $V$ y $w_1,\ldots,w_m$ una base de $W$. Además las entradas de $\mathcal{M}(S)$, $\mathcal{M}(T)$ y $\mathcal{S+T}$ son $A$, $B$ y $C$ respectivamente; definidas por,
    $$
    \begin{array}{rcccl}
	Sv_k&=&A_{1,k}w_1+\cdots+A_{m,k}w_m&=&\displaystyle\sum_{j=1}^m A_{j,k}w_j,\\\\
	Tv_k&=&B_{1,k}w_1+\cdots+B_{m,k}w_m&=&\displaystyle\sum_{j=1}^m B_{j,k}w_j,\\\\
	(S+T)v_k&=&C_{1,k}w_1+\cdots+C_{m,k}w_m&=&\displaystyle\sum_{j=1}^m C_{j,k}w_j.
    \end{array}
    $$
    Por la adición de un conjunto de transformaciones lineales:
    $$
    \begin{array}{rcl}
	(S+T)v_k&=& Sv_k+Tv_k\\\\
	C_{1,k}w_1+\cdots + C_{m,k}w_m&=&\left(A_{1,k}w_1+\cdots +A_{m,k}w_m\right)+\left(B_{1,k}w_1+\cdots +B_{m,k}w_m\right).
    \end{array}
    $$
    Luego, por la definición 3.35, 
    $$(A+B)_{j,k}=A_{j,k}+B_{j,k}$$
    De esta manera, $C=(A+B)_{j,k}$. Por lo tanto,
    $$\sum_{j=1}^m C_{j,k}w_j=\sum_{j=1}^m (A+B)_{j,k}w_j=\sum_{j=1}^m A_{j,k}w_j+\sum_{j=1}^m B_{j,k}w_j.$$
    Así,
    $$\mathcal{M}(S+T)=\mathcal{M}(S)+\mathcal{M}(T).$$
\end{myteo}

%--------------------teorema 3.37
\begin{mydef}[Multiplicación escalar de una matriz]\;\\\\
    El producto de un escalar y una matriz es la matriz que se obtiene multiplicando cada entrada de la matriz por el escalar:
    $$
    \lambda
	\begin{pmatrix}
	    A_{1,1}&\cdots&A_{1,n}\\
	    \vdots&&\vdots\\
	    A_{m,1}&\cdots&A_{m,n}
	\end{pmatrix}
	=
	\begin{pmatrix}
	    \lambda A_{1,1}&\cdots&\lambda A_{1,n}\\
	    \vdots&&\vdots\\
	    \lambda A_{m,1}&\cdots&\lambda A_{m,n}
	\end{pmatrix}
    $$
    En otras palabras, $(\lambda A)_{j,k}=\lambda A_{j,k}$.
\end{mydef}

En el siguiente resultado, se supone que se utilizan las mismas bases para ambas transformaciones lineales $\lambda T$ y $T$.

%--------------------teorema 3.38
\begin{myteo}[La matriz de un escalar por una transformación lineal]\;\\\\
    Suponga $\lambda \in \textbf{F}$ y $T\in \mathcal{L}(V,W)$. Entonces, $\mathcal{M}(\lambda T)=\lambda \mathcal{M}(T)$.\\\\
	Demostración.-\; Sean $v_1,\ldots, v_n$ una base de $V$ y $\lambda \in \textbf{F}$. Además las entradas de $\mathcal{M}(T)$ son $A$ definida por,
	$$Tv_k=A_{1,k}w_1+\cdots+A_{m,k}w_m$$
	Entonces, por la multiplicación escalar de un conjunto de transformaciones lineales:
	$$
	\begin{array}{rcl}
	(\lambda T)v_k&=& \lambda(Tv_k)\\\\
	\left(\lambda A_{1,k}\right)w_1 + \cdots +\left(\lambda A_{m,k}\right) w_m &=&\lambda\left(A_{1,k}w_1+\cdots +A_{m,k}w_m\right)\\\\
	\end{array}
	$$

	Se deduce que las entradas en la fila $k$, columna $k$ de $M(T)$ con respecto a esta base y el escalar son $\lambda A_{j,k}$. Luego, por la definición 3.38, 
	$$(\lambda A)_{j,k}=\lambda A_{j,k}$$
	De donde, concluimos que $\mathcal{M}(T) =  \lambda \mathcal{M}(T)$.
\end{myteo}

Dado que la suma y la multiplicación escalar ya se han definido para las matrices, no debería sorprenderte que esté a punto de aparecer un espacio vectorial. Sólo necesitamos un poco de notación para que este nuevo espacio vectorial tenga un nombre.

%--------------------definición 3.39
\begin{mydef}[\boldmath$\textbf{F}^{m,n}$]\;\\\\
    Para $m$ y $n$ enteros positivos, el conjunto de todos las matrices $m\times n$ con entradas en \textbf{F} se denomina $\textbf{F}^{m,n}$.
\end{mydef}

%--------------------teorema 3.40
\begin{myteo}[\boldmath $\dim \textbf{F}^{m,n}=mn$]\,\\\\
    Suponga que $m$ y $n$ son enteros positivos. Con la adición y la multiplicación escalar ya definidas, $\textbf{F}^{m,n}$ es un espacio vectorial con dimensión $mn$.
\end{myteo}


\section{Multiplicación de matrices}

%--------------------definición 3.41
\begin{mydef}[Multiplicación de matrices]\,\\\\
    Suponga $A$ es una matriz $m\times n$ y $C$ es una matriz $n\times p$. Entonces $AC$ es definida por la matriz $m\times p$ cuyas entradas en la fila $j$, columna $k$, están dadas por la siguiente ecuación:
    $$(AC)_{j,k}=\sum_{r=1}^n A_{j,r}C_{r,k}.$$
    En otras palabras, la entrada en la fila $j$, columna $k$ de $AC$ se calcula tomando la fila $j$ de $A$ y la columna $k$ de $C$, multiplicando las entradas correspondientes y luego sumando.
\end{mydef}

Tenga en cuenta que definimos el producto de dos matrices solo cuando el número de columnas de la primera matriz es igual al número de filas de la segunda matriz.\\

La multiplicación de matrices no es conmutativa. En otras palabras, $AC$ no es necesariamente igual a $CA$. Pero si es distributiva y asociativa.

%--------------------lema 1.1
\begin{lema}
    Demostrar que la propiedad distributiva es cierta para la adición y multiplicación de matrices. En otras palabras, suponga que $A,B$ y $C$ son matrices cuyas dimensiones son tal que $A(B+C)$ tengan sentido. Demostrar que $AB+AC$ tiene sentido y $A(B+C)=AB+AC$.\\\\
	Demostración.-\; Ya que, $A(B+C)$ tiene sentido, por la matriz de la suma de transformaciones lineales $B$ y $C$ tienen el mismo tamaño. Además, por 1.38 el número de columnas $n$ de $A$ debe ser igual al número de filas de $B$ y $C$. Todo esto significa que $AB+AC$ tiene sentido.\\
	Para demostrar que $A(B+C)=AB+AC$, usaremos la definición de adición de matrices, la definición de multiplicación de matrices y la propiedad distributiva de la multiplicación escalar en $\textbf{F}$. En particular, sea $a_{j,k}$, $b_{j,k}$y $c_{j,k}$ denotado como las entrada en la fila $j$, columna $k$ de $A,B$ y $C$, respectivamente. La entrada en la fila $j$, columna $k$ de $B+C$ es $b_{j,k}+c_{j,k}$. Por lo que la entrada en la fila $j$, columna $k$ de $A(B+C)$ es 
	$$\sum_{r=1}^n a_{j,r}(b_{r,k}+c_{r,k})=\sum_{r=1}^n a_{j,r}b_{r,k}+\sum_{r=1}^n a_{j,r}c_{r,k}.$$
	Esto es igual a la entrada en la fila $j$, columna $k$ de $AB+AC$. Por lo tanto, $A(B+C)=AB+AC$.
\end{lema}

%-------------------- lema 1.2
\begin{lema}
    Demostrar que la multiplicación de matrices es asociativa. En otras palabras, suponga que $A,B$ y $C$ son matrices cuyas dimensiones son tales que $(AB)C$ tiene sentido. Demostrar que $A(BC)$ tiene sentido y $(AB)C=A(BC)$.\\\\
<<<<<<< HEAD
    Demostración.-\; Sean $A,B$ y $C$ matrices $m\times n$, $n\times p$ y $p\times q$ matrices respectivamente. Entonces $A(BC)$ y $(AB)C$ tienen sentido. Ahora, por definición de multiplicación de matrices (1.41), se tiene para las entradas $(i,j)$ de $A(BC)$ y $(AB)C$,
    $$A(BC)=\sum_{r=1}^n A_{ij}\left(\sum_{k=1}^p B_{jk}C_{kl}\right) = \sum_{j=1}^n\sum_{k=1}^p A_{ij}B_{jk}C_{kl} = \displaystyle\sum_{j=1}^p\sum_{k=1}^n A_{ij}B_{jk}C_{kl}=\sum_{k=1}^p\left(\sum_{j=1}^n A_{ij}B_{jk}\right)C_{kl}=(AB)C$$
    Por lo tanto, $A(BC)=(AB)C$.
=======
    Demostración.-\; Suponga que $A$ es una matriz $m\times n$, $B$ una matriz $n\times p$ y $C$ una matriz $p\times q$, donde el tamaño de $(AB)C$ tenga sentido. Sean $R\in \mathcal{L}\left(\textbf{F}^m,\textbf{F}^n\right)$, $S\in \mathcal{L}\left(\textbf{F}^n,\textbf{F}^p\right)$ y $T\in \mathcal{L}\left(\textbf{F}^p,\textbf{F}^q\right)$, tales que $\mathcal{M}(R)=A, \mathcal{M}(S)=B,\mathcal{M}(T)=C$, existan. Ahora,
    $$
    \begin{array}{rcl}
	(AB)C&=& \left[\mathcal{M}(R)\mathcal{M}(S)\right]\mathcal{M}(T)\\\\
	     &=& \mathcal{M}(RS)\mathcal{M}(T)\\\\
	     &=& \mathcal{M}\left[(RS)T\right]\\\\
	     &=& \mathcal{M}(R)\mathcal{M}(ST)\\\\
	     &=& \mathcal{M}(R)\left[\mathcal{M}(S)\mathcal{M}(T)\right]\\\\
	     &=& A(BC).
    \end{array}
    $$
>>>>>>> 150134c06fc1487854696f050b1ab9cb75e234dc
\end{lema}

\setcounter{myteo}{42}
%--------------------teorema 3.43
\begin{myteo}[La matriz del producto de transformaciones lineales]\;\\\\
    Si $T\in \mathcal{L}(U,V)$ y $S\in \mathcal{L}(V,W)$, entonces $\mathcal{M}(ST)=\mathcal{M}(S)\mathcal{M}(T)$.\\\\
<<<<<<< HEAD
	Demostración.-\; Suponga, que $v_1,\ldots,v_n$ es una base de $V$ y $w_1,\ldots,w_m$ es una base de $W$ y que $u_1,\ldots,u_p$ es una base de $U$.\\
	Considere la transformación lineal $T: U\to V$ y $S: V\to W$. La composición $ST$ es una transformación lineal de $U$ en $W$. Ahora, suponga $\mathcal{M}(S)=A$ y $\mathcal{M}(T)=C$. Para $1\leq k\leq p$, tenemos
$$
\begin{array}{rcl}
    (ST)u_k &=& \displaystyle S\left(\sum_{r=1}^n C_{r,k}v_r\right)\\\\
	    &=& \displaystyle \sum_{r=1}^n C_{r,k}Sv_r\\\\
	    &=& \displaystyle\sum_{r=1}^n C_{r,k}\sum_{j=1}^m A_{j,r}w_j\\\\
	    &=& \displaystyle\sum_{j=1}^m\left(\sum_{r=1}^n A_{j,r}C_{r,k}\right)w_j.
\end{array}
$$	

Así, $\mathcal{M}(ST)$ es la matriz $m\times n$ cuyas entradas en la fila $j$, columna $k$, son iguales a
$$\sum_{r=1}^n A_{j,r}C_{r,k}.$$

Debemos tomar encuenta que $\mathcal{M}(ST)$ consiste en los escalares necesarios para escribir $(ST)u_k$ cómo una combinación lineal de $w_k$. Ahora bien, por definción de multiplicación de matraces $1.41$ para las entradas (j,k) de $\mathcal{M}(S)\mathcal{M}(T)$ definida como:
$$\mathcal{M}(S)\mathcal{M}(T)=\sum_{r=1}^n A_{j,r}C_{r,k}.$$
Demostramos que,
$$\mathcal{M}(ST)=\mathcal{M}(S)\mathcal{M}(T).$$
=======
	Demostración.-\; Sean $v_1,\ldots,v_n$ base de $V$, $w_1,\ldots,w_m$ base de $W$ y $u_1,\ldots,u_p$ base de $U$. Suponga, $\mathcal{M}(S)=A$ y $\mathcal{M}(T)=C$. Para $1\leq k\leq p$, tenemos
	$$
	\begin{array}{rcl}
	    (ST)u_k &=& \displaystyle S\left(\sum_{r=1}^n C_{r,k}v_r\right)\\\\
		    &=& \displaystyle \sum_{r=1}^n C_{r,k}Sv_r\\\\
		    &=& \displaystyle\sum_{r=1}^n C_{r,k}\sum_{j=1}^m A_{j,r}w_j\\\\
		    &=& \displaystyle\sum_{j=1}^m\left(\sum_{r=1}^n A_{j,r}C_{r,k}\right)w_j.
	\end{array}
	$$	

	Así, $\mathcal{M}(ST)$ es la matriz $m\times n$ cuyas entradas en la fija $j$,  columna $k$, son iguales a
	$$\sum_{r=1}^n A_{j,r}C_{r,k}.$$

	Ahora, 
	$$
	\begin{array}{rcl}
	    S u_k  T u_k &=& \displaystyle\sum_{j=1}^{m} A_{j,k}w_j \sum_{r=1}^{n} C_{r,k}v_r\\\\
	\end{array}
	$$
>>>>>>> 150134c06fc1487854696f050b1ab9cb75e234dc
\end{myteo}

%--------------------Notación 3.44
\begin{mynotacion}[\boldmath$A_{(j,\cdot)},A_{(\cdot,k)}$]\,\\\\
    Suponga que $A$ es una matriz $m\times n$.
    \begin{itemize}
	\item Si $1\leq j\leq m$, entonces $A_{j,\cdot}$ denota la matriz $1\times n$ que consiste en la fila $j$ de $A$.
	\item Si $1\leq k\leq n$, entonces $A_{\cdot,k}$ denota la matriz $m\times 1$ que consiste en la columna $k$ de $A$.
    \end{itemize}
\end{mynotacion}

\setcounter{myteo}{46}
%--------------------teorema 3.45
\begin{myteo}[La entrada del producto de la matriz es igual a la fila por la columna]\,\\\\
    Suponga que $A$ es una matriz $m\times n$ y $C$ es una matriz $n\times p$. Entonces, 
    $$(AC)_{j,k}=A_{j,\cdot}C_{\cdot,k}$$
    para $1\leq j\leq m$ y $1\leq k\leq p$.\\\\
	Demostración.-\; Dado que $1\leq j\leq m$ y $1\leq k\leq p$, por la notación $3.44$, tenemos
	$$
	\begin{array}{ccc}
	    A_{j,\cdot} & \mbox{ es una matriz } & m\times 1.\\\\
	    C_{\cdot,k} & \mbox{ es una matriz } & 1\times p.
	\end{array}
	$$
	Respectivamente. Por la definición de multiplicación de matrices (1.41), concluimos que,
	$$(AC)_{j,k}=\sum_{r=1}^1 A_{j,r}C_{r,k}=A_{j,1}C_{1,k}=A_{j,\cdot}C_{\cdot,k}.$$
\end{myteo}
\vspace{.5cm}

Otra forma de pensar en las Multiplicaciones de matrices:

\setcounter{myteo}{48}
%--------------------teorema 3.46
\begin{myteo}[La columna del producto de la matriz es igual a la matriz por la columna]\,\\\\
    Suponga que $A$ es una matriz $m\times n$ y $C$ es una matriz $n\times p$. Entonces,
    $$(AC)_{\cdot,k}=AC_{\cdot,k}$$
    para $1\leq k\leq p$.\\\\
	Demostración.-\; Dado que $1\leq k\leq p$. Por la notación $3.44$, tenemos
	$$C_{\cdot, k} \mbox{ es una matriz } n\times 1.$$
	Luego, por la definición de multiplicación de matrices (1.41),
	$$AC_{\cdot,k}=\sum_{r=1}^n A_{j,r}C_{r,1}=A_{j,1}C_{1,1}+A_{j,2}C_{2,1}+\cdots+A_{j,n}C_{n,1}$$
	Ya que esta suma es igual a una columna de $AC$, concluimos que
	$$AC_{\cdot,k}=(AC)_{\cdot,k}.$$
\end{myteo}

\setcounter{mydef}{51}
%--------------------Definición 3.47
\begin{mydef}[Combinación lineal de columnas]\,\\\\
    Suponga que $A$ es una matriz $m\times n$ y $c=\begin{pmatrix}c_1\\\vdots\\c_n\end{pmatrix}$ es una matriz $n\times 1$. Entonces
    $$Ac=c_1A_{\cdot,1}+\cdots+c_nA_{\cdot,n}.$$
    En otras palabras, $Ac$ es una combinación lineal de las columnas de $A$, con los escalares que multiplican las columnas que viene de $c$.
\end{mydef} 

\mysection{Invertibilidad y espacios vectoriales isomorfos}
\vspace{0.5cm}
\subsection*{Transformaciones lineales invertibles}

%--------------------definición 3.53
\begin{mydef}[Invertible, inverso]\,\\
    \begin{itemize}
	\item Una transformación lineal $T\in \mathcal{L}(V,W)$ es llamada \textbf{invertible} si existe una transformación lineal $S\in \mathcal{L}(W,V)$ tal que $ST$ es igual a la transformación identidad en $V$ y $TS$ es igual a la transformación identidad en $W$.
	\item Una transformación lineal $S\in \mathcal{L}(W,V)$ que satisface $ST=I$ y $TS=I$ se llama \textbf{inverso} de $T$ (nótese que el primer $I$ es la transformación identidad en $V$ y el segundo $I$ es la transformación identidad en $W$).
    \end{itemize}
\end{mydef}

%--------------------teorema 3.54
\begin{myteo}[El inverso es único]\,\\\\
    Una transformación lineal invertible tiene un inverso único.\\\\
	Demostración.-\; Suponga que $T\in \mathcal{L}(V,W)$ es invertible y $S_1$ y $S_2$ son inversos de $T$. Entonces,
	$$S_1=S_1I=S_1(TS_2)=(S_1T)S_2=IS_2=S_2.$$
	Por lo tanto, $S_1=S_2$.
\end{myteo}

%--------------------Notación 3.55
\begin{mynotacion}[\boldmath$T^{-1}$]\,\\\\
    Si $T$ es invertible, entonces el inverso de $T$ se denota por $T^{-1}$. En otras palabras, si $T\in \mathcal{L}(V,W)$ es invertible, entonces $T^{-1}$ es el único elemento de $\mathcal{L}(W,V)$ tal que $T^{-1}T=I$ y $TT^{-1}=I.$
\end{mynotacion}

%-------------------- Lema 1.5
\begin{lema}
    Suponga que $T\in \mathcal{L}(U,V)$ y $S\in \mathcal{L}(V,W)$ son ambos transformaciones lineales invertibles. Demostrar que $ST\in \mathcal{L}(U,W)$ es invertible y que $(ST)^{-1}=T^{-1}S^{-1}$.\\\\
	Demostración.-\; Para todo $u\in U$ tenemos que,
	$$
	\begin{array}{rcl}
	    (T^{-1}S^{-1}ST)(u) & = & T^{-1}(S^{-1}(S(T(u))))\\\\
				&=& T^{-1}(S^{-1}S)(T(u))\\\\
				&=& T^{-1}(T(u))\\\\
				&=& u.
	\end{array}
	$$
	Donde, $T^{-1}S^{-1}ST=I$. Por otro lado, para todo $w\in W$ tenemos que,
	$$
	\begin{array}{rcl}
	    (ST)(T^{-1}S^{-1}(w)) & = & S(T(T^{-1}S^{-1}(w)))\\\\
				  & = & S(I(S^{-1}(w)))\\\\
				  & = & S(S^{-1}(w))\\\\
				  & = & w.
	\end{array}
	$$
	Donde, $ST(T^{-1}S^{-1})=I$. Por lo tanto, $ST$ es invertible e implica que $(ST)^{-1}=T^{-1}S^{-1}$.
\end{lema}

%--------------------teorema 3.56
\begin{myteo}[La invertibilidad es equivalente a la inyectabilidad y a la sobreyectividad]\,\\\\
    Una transformación lineal es invertible si y sólo si es inyectiva y sobreyectiva.\\\\
	Demostración.-\; Suponga que $T\in \mathcal{L}(V,W)$ Necesitamos mostrar que $T$ es invertible si y sólo si es inyectiva y sobreyectiva. Primero, supongamos que $T$ es invertible. Para demostrar que $T$ es inyectiva, supongamos que $u,v\in V$ y $Tu=Tv$. Entonces,
	$$u=T^{-1}\left(Tu\right)=T^{-1}\left(Tv\right)=v,$$
	por lo que $u=v$. Así, $T$ es inyectiva. \\
	Probemos ahora que $T$ es sobreyectiva. Para ello, sea $w\in W$. Entonces, $w=T\left(T^{-1}w\right)$, lo que muestra que $w$ está en el rango de $T$. Así, $\range T= W.$ Por lo tanto, $T$ es subyectiva.\\

	Por otro lado, supongamos que $T$ es inyectiva y subyectiva. Queremos probar que $T$ es invertible. Para cada $w\in W$, definamos $Sw$ cómo el único elemento de $V$ tal que $T(Sw)=w$ (la existencia y unicidad de tal elemento se derivan de la sobreyectividad e inyectividad de $T$). Claramente $T\circ S$ es igual a la indentidad en $W$. Luego, para probar que $S\circ T$ es igual a la transformación identidad en $V$, sea $v\in V$. Entonces,
	$$T\left[(S\circ T)v\right] = (T\circ S)(Tv)=I(Tv)=Tv.$$
	Esta ecuación implica que $(S\circ T)v=v$, ya que $T$ es inyectiva. Así, $S\circ T$ es igual a la transformación identidad en $V$. \\
	Para completar esta demostración, necesitamos mostrar que $S$ es una transformación lineal. Para esto, supongamos que $w_1,w_2\in W$. Entonces,
	$$T(Sw_1+Sw_2)=T(Sw_1)+T(Sw_2)=w_1+w_2.$$
	Así, $Sw_1+Sw_2$ es el único elemento de $V$ que $T$ transforma en $w_1+w_2$. Por la definición de $S$ (basada en 1.5), implica que $S(w_1+w_2)=Sw_1+Sw_2$. Por lo que $S$ satisface la propiedad aditiva requerida por la linealidad.\\
	Para la propiedad homogénea, supongamos que $w\in W$ y $\lambda \in \textbf{F}$. Entonces,
	$$T(\lambda Sw) = \lambda T(Sw)=\lambda w.$$
	Por lo tanto, $\lambda Sw$ es el único elemento de $V$ que $T$ transforma a $\lambda w$. Por la definición de $S$, esto implica que $S(\lambda w)=\lambda Sw$. Concluimos que $S$ satisface la propiedad homogénea requerida por la linealidad.
\end{myteo}

\vspace{.5cm}

\subsection*{Espacios vectoriales isomorfos}

\setcounter{mydef}{57}
%--------------------definición 3.58
\begin{mydef}[Isomorfismo, isomorfo]\,\\
    \begin{itemize}
	\item Un Isomorfismo es una transformación lineal invertible.
	\item Dos espacios vectoriales son llamados isomorfos si existe un isomorfismo de un espacio vectorial sobre el otro.
    \end{itemize}
\end{mydef}

Piense en un isomorfismo $T:V\to W$ como un cambio de etiqueta de $v\in V$ a $Tv\in W$. Lo que explica porqué dos espacios vectoriales isomorfos tienen las mismas propiedades de espacio vectorial. Los términos "isomorfismo" y "transformación lineal invertible" significan lo mismo. Usaremos "isomorfismo" cuando enfaticemos que los dos espacios son esencialmente iguales. \\
La palabra griega "iso" significa igual; la palabra griega "morfo" significa forma. Por lo tanto, isomorfo literalmente significa forma igual.

%--------------------teorema 3.59
\begin{myteo}[La dimensión muestra si los espacios vectoriales son isomorfos]\;\\\\
    Dos espacios vectoriales de dimensión finita sobre \textbf{F} son isomorfos si y sólo si tienen la misma dimensión.\\\\	
	Demostración.-\; Primero suponga que $V$ y $W$ son espacios vectores de dimensión finita isomorfos. De donde, existe un isomorfismo $T$ de $V$ sobre $W$. Ya que, $T$ es invertible, tenemos $\mynull T=\left\{0\right\}$ y $\range T = W$. Así, $\dim\mynull T=0$. Por el teorema fundamental de transformaciones lineales 3.22, tenemos
	$$\dim V = \dim\mynull T + \dim\range T = 0 + \dim W = \dim W.$$
	Es decir, $\dim V = \dim W$.\\
	Para probar la reciproca, supongamos que $V$ y $W$ son espacios vectoriales de dimensión finita con alguna e igual dimensión. Sean $v_1,\ldots,v_n$ una base de $V$ y $w_1,\ldots,w_n$ una base de $W$. También sea $T\in \mathcal{L}(V,W)$ definida por
	$$T\left(c_1v_1+\cdots+c_nv_n\right)=c_1w_1+\cdots+c_nw_n.$$
	Entonces, $T$ es una transformación lineal bien definida, ya que $v_1,\ldots, v_n$ es una base de $V$ (3.5). Luego, $T$ es sobreyectiva, debido a que $w_1,\ldots, w_n$ genera $W$ (lema 1.2). Además, $\mynull T=\left\{0\right\}$, porque $w_1,\ldots,w_m$ es linealmente independiente (3.12); así $T$ es inyectiva. Por 3.56 y 3.58a, vemos que $T$ es un isomorfismo. Por lo tanto, por 3.58b $V$ y $W$ son isomorfos.
\end{myteo}

El resultado anterior implica que cada espacio vectorial de dimensión finita $V$ es isomorfo a $\textbf{F}^n$, donde $n=\dim V$. Si $v_1,\ldots,v_n$ es una base de $V$ y $w_1,\ldots,w_m$ es una base de $W$, entonces para cada $T\in \mathcal{L}(V,W)$, tenemos una matriz $\mathcal{M}(T)\in \textbf{F}^{m,n}$. En otras palabras, una vez que las bases fueron fijadas para $V$ y $W$, $\mathcal{M}$ se convierte en una función de $\mathcal{L}(V,W)$ para $\textbf{F}^{m,n}$. Observemos que $3.36$ y $3.38$ prueban que $\mathcal{M}$ es una transformación lineal. Esta transformación lineal en realidad es invertible, como se mostraremos ahora.

%--------------------teorema 3.60
\begin{myteo}[\boldmath$\mathcal{L}(V,W)$ y $\textbf{F}^{m,n}$ son isomorfos]\,\\\\
    Suponga que $v_1,\ldots,v_n$ es una base de $V$ y $w_1,\ldots,w_m$ es una base de $W$. Entonces, $\mathcal{M}$ es un isomorfismo entre $\mathcal{L}(V,W)$ y $\textbf{F}^{m,n}$.\\\\
    Demostración.-\; Notemos que $\mathcal{M}$ es lineal. Necesitamos probar que $\mathcal{M}$ es inyectiva y sobreyectiva. Si $T\in \mathcal{L}(V,W)$ y $\mathcal{M}(T)=0$. Entonces, $Tv_k=0$ para $k=1,\ldots,n$. Ya que $v_1,\ldots,v_n$ es una base de $V$, esto implica que $T=0$. Por 3.16, $\mathcal{M}$ es inyectiva.\\
    Para probar que $\mathcal{M}$ es sobreyectiva, supongamos que $A\in \textbf{F}^{m,n}$. Sea $T$ la transformación lineal de $V$ en $W$ tal que
    $$Tv_k=\sum_{j=1}^m A_{j,k}w_j$$
    para $k=1,\ldots,n$. Obviamente $\mathcal{M}(T)$ es igual a $A$, y por lo tanto el rango  de $\mathcal{M}$ es igual a $\textbf{F}^{m,n}$.
\end{myteo}

%--------------------teorema 3.61
\begin{myteo}[\boldmath$\dim(V,W)=(\dim V)(\dim W)$]\,\\\\
    Suponga que $V$ y $W$ son de dimensión finita. Entonces, $\mathcal{L}(V,W)$ es de dimensión finita y
    $$\dim\mathcal{L}(V,W)=(\dim V)(\dim W).$$\\
    Demostración.-\; Sean $\dim V = n$ y $\dim W=m$. Dado que $\mathcal{L}(V,W)$ y $F^{m,n}$ son isomorfos (3.60) y que $\dim\textbf{F}^{m,n}=mn$ (3.40), tenemos 
	$$\dim\mathcal{L}(V,W)=\dim\textbf{F}^{m,n}=mn.$$
	Luego por 3.59, implica que el espacio vectorial $V$ es isomorfo a $\textbf{F}^n$, y el espacio vectorial $W$ es isomorfo a $\textbf{F}^m$; es decir,
	$$(\dim V)(\dim W) = \dim\textbf{F}^n \dim\textbf{F}^m=nm=mn.$$
	Por lo tanto,
	$$\dim\mathcal{L}(V,W)=(\dim V)(\dim W).$$
\end{myteo}

\subsection{Transformación lineal como multiplicación de matrices}

%--------------------Definición 3.62
\begin{mydef}[Matriz de un vector, \boldmath$\mathcal{M}\left(v\right)$]\;\\\\
    Suponga que $v\in V$ y $v_1,\ldots,v_n$ es una base de $V$. La \textbf{matriz de} $v$ con respecto de esta base es la matriz $n\times 1$,
    $$\mathcal{M}(v)=
	\begin{pmatrix}
	    c_1\\
	    \vdots\\
	    c_n
	\end{pmatrix}
    $$
    donde $c_1,\ldots,c_n$ son escalares tales que
    $$v=c_1v_1+\cdots+c_nv_n.$$
\end{mydef}

La matriz $\mathcal{M}(v)$ de un vector $v\in V$ depende de la base $v_1,\ldots,v_n$ de $V$, así como de $v$. Sin embargo, la base debe quedar clara por el contexto y por tanto no se incluye en la notación.\\

Ocasionalmente queremos pensar en los elementos de $V$ como si fueran matrices $n$ por $1$ reetiquetadas. Una vez elegida $v_1,\ldots,v_n$, la función $\mathcal{M}$ que lleva $v\in V$ a $\mathcal{M}(v)$ es un isomorfismo de $V$ sobre $F^{n,1}$ que implementa este reetiquetado.\\

Recordemos que si $A$ es una matriz $m$ por $n$, entonces $A_{\cdot,k}$ denota la columna $k$ de $A$, pensada como una matriz $m$ por $1$. En el siguiente resultado, $\mathcal{M}(v_k)$ se calcula con respecto a la base $w_1, \ldots, w_m$ de $W$.

\setcounter{myteo}{63}
%--------------------teorema 3.63
\begin{myteo}[\boldmath $\mathcal{M}(T)_{\cdot,k}=\mathcal{M}(v_k)$]\,\\\\
    Suponga que $T\in \mathcal{L}(V,W)$ y $v_1,\ldots,v_n$ es una base de $V$ y $w_1,\ldots,w_m$ es una base de $W$. Sea $1\leq k\leq n$. Entonces, la columna $k$-ésima de $\mathcal{M}(T)$, denotado por $\mathcal{M}(T)_{\cdot,k}$, es igual a $\mathcal{M}(v_k)$.\\\\
	Demostración.-\; La demostración se sigue directamene de las definiciones de $\mathcal{M}(T)$ y $\mathcal{M}(v_k)$.
\end{myteo}

%--------------------teorema 3.64
\begin{myteo}[Las transformaciones lineales se comportan como la multiplicación de matrices]\,\\\\
    Suponga que $T\in \mathcal{L}(V,W)$ y $v\in V$. Sean $v_1,\ldots,v_n$ una base de $V$ y $w_1,\ldots,w_m$ una base de $W$. Entonces,
    $$\mathcal{M}(Tv)=\mathcal{M}(T)\mathcal{M}(v).$$\\
	Demostración.-\; Suponga que $v=c_1v_1+\cdots+c_nv_n$, donde $c_1,\ldots,c_n\in \textbf{F}$. Así, 
	$$Tv=c_1Tv_1+\cdots+c_nTv_n.$$
	Por lo tanto,
	$$
	\begin{array}{rcl}
	    \mathcal{M}(Tv)&=&c_1\mathcal{M}(Tv_1)+\cdots+c_n\mathcal{M}(Tv_n)\\\\
			   &=& c_1\mathcal{M}(T)_{\cdot,1}+\cdots+c_n\mathcal{M}(T)_{\cdot,n}\\\\
			   &=& \mathcal{M}(T)\mathcal{M}(v).
	\end{array}
	$$
	donde la primera igualdad se deduce de 3.66 y de la linealidad de $\mathcal{M}$, la segunda igualdad se deduce de 3.64, y la última igualdad se deduce de 3.52.
\end{myteo}

si $T\in \mathcal{L}(V,W)$ e identificamos $v\in V$ con $\mathcal{M}(v)\in \textbf{F}^{n,1}$ , entonces el resultado anterior dice que podemos identificar $Tv$ con $\mathcal{M}(T)\mathcal{M}(v)$.\\

Dado que el resultado anterior nos permite pensar (mediante isomorfismos) en cada mapa lineal como una multiplicación en $\textbf{F}^{n,1}$ por alguna matriz $A$, hay que tener en cuenta que la matriz $A$ específica depende no sólo del mapa lineal, sino también de la elección de las bases. 


\subsection{Operadores}

\setcounter{myteo}{66}
%--------------------definición 3.67
\begin{mydef}[Operador, \boldmath $\mathcal{L}(V)$]\;\\
    \begin{itemize}
	\item Una transformación lineal de un espacio vectorial hacia si mismo se llama un \textbf{operador}.
	\item La notación $\mathcal{L}(V)$ denota el conjunto de todos los operadores en $V$. En otras palabras, $\mathcal{L}(V) = \mathcal{L}(V,V)$.
    \end{itemize}
\end{mydef}

\setcounter{myteo}{68}
%--------------------teorema 3.68
\begin{myteo}[La inyectividad es equivalente a la subjetividad en dimensiones finitas]\;\\\\
    Suponga que $V$ es de dimensión finita y $T\in \mathcal{L}(V)$. Entonces, los siguientes enunciados son equivalentes:
    \begin{enumerate}[(a)]
	\item $T$ es invertible.
	\item $T$ es inyectiva.
	\item $T$ es sobreyetiva.
    \end{enumerate}
    \vspace{.5cm}
	Demostración.- Claramente (a) implica (b). Ahora supongamos que (b) se cumple, de modo que $T$ es inyectiva. De donde, $\mynull T=\left\{0\right\}$ por 3.16. Luego, por el teorema fundamental de transformaciones lineales (3.22) se tiene
	$$\dim \range T = \dim V - \dim \mynull T = \dim V - 0 = \dim V.$$
	Por lo que $\range T$ es igual a $V$. Así, $T$ es sobreyectiva. De esta manera $(b)$ implica (c).\\
	Ahora, supongamos que (c) se cumple, de modo que $T$ es sobreyectiva. De donde, $\range T=V$. Por el teorema fundamental de transformaciones lineales (3.22) se tiene
	$$\dim \mynull T = \dim V - \dim \range T = 0.$$
	Por lo tanto, $\mynull T$ es igual a $\left\{0\right\}$. Lo que concluimos que $T$ es invertible (ya que sabemos que $T$ es sobreyectiva). Por último, (c) implica (a) completando la prueba.
\end{myteo}

El siguiente ejemplo ilustra el poder del resultado anterior. Aunque es posible demostrar el resultado en el siguiente ejemplo sin utilizar álgebra lineal, la demostración utilizando álgebra lineal es más clara y sencilla.

%--------------------Ejemplo 3.70
\begin{myejem}
    Demostrar que para cada polinomio $q\in \mathcal{P}(\textbf{R})$, existe un polinomio $p\in \mathcal{P}(\textbf{R})$ con $\left[\left(x^2+5x+7\right)p\right]''=q$.\\\\
	Solución.-\; Supongamos que $q\in \mathcal{P}_m(\textbf{R})$. Definamos $T:\mathcal{P}_m(\textbf{R})\to \mathcal{P}_m(\textbf{R})$ por
	$$Tp=\left[\left(x^2+5x+7\right)p\right]''.$$
	Multiplicando un polinomio distintos de cero por $\left(x^2+5x+7\right)$ aumentando el grado en 2, y luego diferenciando dos veces se reduce el grado en 2. Por lo tanto $T$ es de hecho un operador en $\mathcal{P}(\textbf{R})$.\\
	Todo polinomio cuya segunda derivada es igual a $0$ es de la forma $ax+b$ donde $a,b\in \textbf{R}$. Por lo que, $\mynull T=\left\{0\right\}$ Así, $T$ es inyectivo.\\
	Ahora, 3.69 implica que $T$ es sobreyectivo. Concluimos que existe un polinomio $p\in \mathcal{P}_m(\textbf{R})$ tal que $\left[\left(x^2+5x+7\right)p\right]''=q,$ como queríamos.
\end{myejem}


% -------------------- Lema 3.6
\begin{lema}
    Suponga que $V$ es de dimensión finita y $S,T\in \mathcal{L}(V)$. Demostrar que $ST$ es invertible , si y sólo si, ambos $S$ y $T$ son invertibles.\\\\
	Demostración.-\; 
\end{lema}


