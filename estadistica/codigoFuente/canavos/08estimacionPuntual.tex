\chapter{Estimación puntual y por intervalo}

\setcounter{section}{1}
\section{Propiedades deseables de los estimadores puntuales}

Se conoce la familia de distribuciones a partir de la cual se obtiene la muestra, pero no puede identificarse el miembro de interés de esta, ya que no se conoce el valor del parámetro. Este último tiene que estimarse con base en los datos de la muestra. \\

El estimador de un parámetro $\theta$ debe tener una distribución de muestro concentrada alrededor de $\theta$ y la varianza del estimador debe ser la menor posible. Para ampliar las propiedades anteriores, considérese la siguiente. Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de tamaño $n$ proveniente de una distribución con función de densidad $f(x,\theta)$, y sea $T=u(X_1,X_2,\ldots,X_n)$ cualquier estadística. El problema es encontrar una función $u$ que sea la que proporcione la mejor estimación de $\theta$. Al buscar el mejor estimador de $\theta$ se hará uso de una cantidad muy importante que recibe el nombre de error cuadrático medio de un estimador.

%-------------------- definición 8.1
\begin{def.}
    Sea $T$ cualquier estimador de un parámetro desconocido $\theta$. Se define el error cuadrático medio de $T$ como el valor esperado del cuadrado de la diferencia entre $T$ y $\theta$.
\end{def.}

Para cualquier estadística $T$, se denotará el error cuadrático medio por $ECM(T)$; de esta forma
$$ECM(T)=E(T-\theta)^2.$$

Puede verse la razón del porque el error cuadrático medio es una cantidad importante para enjuiciar a los posibles estimadores de $\theta$ mediante el desarrollo de $ECM(T)=E(T-\theta)^2$; este es,

$$
\begin{array}{rcl}
    ECM(T) &=& \E(T^2-2\theta T+\theta^2)\\
	   &=& \E(T^2)-2\theta\E(T)+\theta^2\\
	   &=& \Var(T)+E^2(T)-2\theta\E(T)+\theta^2\\
	   &=& \Var(T)+\left[\theta-E(T)\right]^2.
\end{array}
$$

El error cuadrático medio de cualquier estimador es la suma de dos cantidades no negativas: una es la varianza del estimador y la otra es el cuadrado del sesgo del estimador. Estas dos cantidades se encuentran relacionadas en forma directa con las propiedades deseables de un estimador. La varianza de un estimador debe ser lo más pequeña posible mientras que la distribución de muestreo debe concentrarse del valor del parámetro. Por lo que debemos seleccionar el mejor estimador de $\theta$; es decir, la estadística que tenga el error cuadrático medio más pequeño posible entre todos los estimadores factibles de $\theta$. \\

\subsection{Estimadores insesgados}
El término $\theta-E(T)$ recibe el nombre de sesgo del estimador.

%-------------------- definición 8.2
\begin{def.}
    Se dice que la estadística $T=u(X_1,X_2,\ldots,X_n)$ es un estimador insesgado del parámetro $\theta$, si $E(T)=\theta$ para todos los posibles valores de $\theta$. De esta forma, para cualquier estimador insesgado de $\theta$, la distribución de muestreo de $T$ se encuentra centrada alrededor de $\theta$ y $ECM(T)=Var(T)$.
\end{def.}

En la sección 7.4, demostramos que sin importar la distribución de la población de interés, $E(\overline{X})=\mu$. Por lo tanto, la media muestral es un estimador insesgado de la media de la población $\mu$ para todo los valores de $\mu$. De hecho, si $X_1,X_2,\ldots,X_n$ es una muestra aleatoria de la distribución de $X$ con media $\mu$, entonces cualquier $X_i$ de la muestra es un estimador insesgado de $\mu$, dado que $E(X_i)=\mu$ para todo $i=1,2,\ldots, n$. Además si una estadística $T$ es cualquier combinación lineal de las variables aleatorias de la muestra de manera tal que
$$T=a_iX_1+a_2X_2+\cdots+a_nX_n$$
en donde $\sum_{i=1}^n a_i=1$, entonces $T$ es un estimador insesgado de $\mu$ dado que

$$
\begin{array}{rcl}
    E(T) &=& \displaystyle\sum_{i=1}^n a_iE(X_i)\\\\
	 &=& \displaystyle\sum_{i=1}^n a_i\mu\\\\
	 &=& \mu.
\end{array}
$$

En la sección 7.5 se demostró que si la varianza muestral $S^2$ está dada por $S^2=\sum_{i=1}^n(X_i-\overline{X})^2/(n-1)$. Entonces, cuando se muestrea una distribución normal, $E\left(S^2\right)=\sigma^2$. Lo que demostraremos que es insesgado sin importar cuál sea la distribución de la población de interés. Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de alguna distribución con una función de densidad no específica. De esta manera, $E(X_i)=\mu$ y $\Var(X_i)=\sigma^2$ para toda $i=1,2,\ldots,n.$ Entonces, 

$$
\begin{array}{rcl}
    \E\left(S^2\right) &=& \E\left[\displaystyle\sum_{i=1}^n \dfrac{(X_i-\overline{X})^2}{n-1}\right]\\\\
		       &=&(n-1)^{-1}\E\left\{\displaystyle\sum_{i=1}^n \left[(X_i-\mu)-(\overline{X}-\mu)\right]^2\right\}\\\\
		       &=& (n-1)^{-1}\E\left\{\displaystyle\sum_{i=1}^n \left[(X_i-\mu)^2-n\left(\overline{X}-\mu\right)^2\right]\right\}\\\\
		       &=& (n-1)^{-1}\left[\displaystyle\sum_{i=1}^n \E(X_i-\mu)^2-n\E\left(\overline{X}-\mu\right)^2\right]\\\\
\end{array}
$$
pero por definición $\E(X_i-\mu)^2=\Var(X_i)=\sigma^2$ y $E\left(\overline{X}-\mu\right)^2=\Var(\overline{X})=\sigma^2/n$. Por lo tanto,
$$
\begin{array}{rcl}
    E\left(S^2\right)&=&(n-1)^{-1}\left[\dfrac{n\sigma^2-\left(n\sigma^2\right)}{n}\right]\\\\
		     &=& \dfrac{\sigma^2(n-1)}{n-1}\\\\
		     &=& \sigma^2.
\end{array}
$$

En otras palabras, $S^2$ es un estimador insesgado de $\sigma^2$ sólo cuando el divisor es igual a $n-1$. Es la razón por la que se divide por $n-1$ en lugar de $n$. $S$ no hará un estimador insesgado de $\sigma$.


\subsection{Estimadores consistentes}

Es razonable esperar que un buen estimador de un parámetro $\theta$ sea cada vez menor conforme crece el tamaño de la muestra. Se tendrá un mejor estimador de $\theta$ si se basa en $30$ observaciones que si lo hace con sólo cinco.

%-------------------- definición 8.3
\begin{def.}
    Sea $T$ el estimador de un parámetro $\theta$, y sea $T_1,T_2,\ldots,T_n$ una secuencia de estimadores que representan a $T$ con base en muestras de tamaño $1,2,\ldots, n$, respectivamente. Se dice que $T$ es un estimador consistente (sencillo) para $\theta$ si
    $$\lim_{n\to \infty}P(|T_n-\theta|\leq \epsilon)=1$$
    para todos los valores de $\theta$ y $\epsilon>0$.
    El requisito constituye lo que se denomina convergencia en probabilidad. Es decir, si un estimador es consistente, converge en probabilidad al valor del parámetro que está intentando estimar conforme el tamaño de la muestra crece. Esto implica que la varianza de un estimador consistente $T_n$ disminuye conforme $n$ crece, y la media de $T_n$ tiende hacia donde $n$ crece. Para demostrar que $\overline{X}$ es un estimador consistente de $\mu$, primero se enuncia el teorema de desigualdad de Tchebysheff.
\end{def.}

%-------------------- teorema 8.1
\begin{teo}
    Sea $X$ una variable aleatoria con una función (densidad) de probabilidad $f(x)$ de manera tal que tanto $\E(X)=\mu$ como $Var(X)=\sigma^2$ tienen un valor finito. Entonces
    $$P(|X-\mu|\leq k\sigma)\geq 1-\dfrac{1}{k^2}$$
    o
    $$P(|X-\mu|>k\sigma)\leq \dfrac{1}{k^2}$$
    para cualquier constante $k\geq 1$.
\end{teo}

Este teorema permite determinar los límites de las probabilidades de variables aleatorias discretas y continuas sin tener que especificar sus funciones (densidades) de probabilidad. Asegura que la probabilidad de que una variable aleatoria se aleje no más de $k$ desviaciones estándar de la media, es mayor o igual a $1/k^2$ para algún valor de $k\geq 1$. Por ejemplo
$$P(|X-\mu|\leq 2\sigma)\geq 1-\dfrac{1}{4}$$
y
$$P(|X-\mu|\leq 3\sigma)\geq 1-\dfrac{1}{9}.$$
para cualquier variable aleatoria $X$ con media $\mu$ y varianza $\sigma^2$ finitas.

%-------------------- teorema 8.2
\begin{teo}
    Sean $X_1,X_2,\ldots,X_n$, $n$ variables aleatorias IID, tales que $\E(X_i)=\mu$ y $\Var(X_i)=\sigma^2$ tienen un valor finito para $i=1,2,\ldots,n.$ Entonces, $\overline{X}_n=\sum_{i=1}^n X_i/n$ es un estimador consistente de $\mu$.\\\\
	Demostración.-\; Se quiere demostrar que
	$$\lim_{n\to \infty}P(|\overline{X}_n-\mu|\leq \epsilon)=1.$$
	Dado que $\overline{X}_n$ es una variable aleatoria tal que $\E(\overline{X}_n)=\mu$ y $\Var(\overline{X}_n)=\sigma^2/n$, se deduce del teorema de Tchebysheff que
	$$P(|\overline{X}_n-\mu|>k\sigma/\sqrt{n})\leq 1/k^2.$$
    Sea $k$ una constante positiva igual a $\epsilon\sqrt{n}/\sigma$, en donde $\epsilon$ es un número real positivo. Entonces,
	$$P(|\overline{X}_n-\mu|>\epsilon)\leq \dfrac{\sigma^2}{n\epsilon^2}.$$
	Dado que $\sigma^2$ tiene un valor finito, tomando el límite de esta expresión conforme $n$ tiende al infinito se tiene
	$$\lim_{n\to \infty}P(|\overline{X}_n-\mu|> \epsilon)=0.$$
	Por lo tanto, se concluye que
	$$\lim_{n\to \infty}P(|\overline{X}_n-\mu|\leq \epsilon)=1.$$
	y $\overline{X}_n$ es un estimador consistente de $\mu$.
\end{teo}

El teorema 8.2 también se conoce como la ley de los grandes números.\\\\


\subsection{Estimadores insesgados de varianza mínima}

Para un parámetro que posee un error cuadrático medio mínimo es difícil determinar un estimador para todo los posibles valores del parámetro. Pero podremos analizar uno que tenga un error cuadrático medio mínimo. Si se tiene una varianza mínima para todos los valores posibles de $\theta$. Este estimador recibe el nombre de estimador insesgado de varianza mínima uniforme (VMU)

%-------------------- definición 8.4
\begin{def.}
    Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de una distribución cuya función (densidad) de probabilidad es $f(x;\theta)$. Sea la estadística $T=u(X_1,X_2,\ldots,X_n)$ un estimador de $\theta$ tal que $\E(T)=\theta$ y $Var(T)$ es menor que la varianza de cualquier otro estimador insesgado de $\theta$ para todo los posibles valores de $\theta$. Se dice entonces que $T$ es un estimador insesgado de varianza mínima de $\theta$.
\end{def.}

La varianza de un estimador insesgado es la cantidad más importante para decidir que tan bueno es el estimador para estimar un parámetro $\theta$. Por ejemplo, sean $T_1$ y $T_2$ cualesquiera dos estimadores insesgados de $\theta$. Se dice que $T_1$ es un estimador más eficiente de $\theta$ que $T_2$ si $\Var(T_1)\leq \Var(T_2)$. Es muy común utilizar el cociente $\Var(T_1)/\Var(T_2)$ para determinar la eficiencia relativa de $T_2$ respecto de $T_1$. Si los estimadores son sesgados, se emplea sus errores cuadráticos medios para determinar las eficiencias relativas.\\


%-------------------- teorema 8.3
\begin{teo}[Cota inferior de Cramérr-Rao]
    Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de una distribución con una función (densidad) de probabilidad $f(x;\theta)$. Si $T$ es un estimador insesgado de $\theta$, entonces la varianza de $T$ debe satisfacer la siguiente desigualdad
    $$\Var(T)\geq \dfrac{1}{n\E\left[\left(\dfrac{\partial \ln f(X;\theta)}{\partial \theta}\right)^2\right]}.$$
\end{teo}

El teorema 8.3 establece un límite inferior para la varianza de un estimador de $\theta$. Para un estimador insesgado cuya varianza se apega a la cota inferior de Cramér-Rao, se tiene la siguiente definición.

%-------------------- definición 8.5
\begin{def.}
    Si $T$ es cualquier estimador insesgado del parámetro $\theta$ tal que
    $$Var(T)\geq \dfrac{1}{n\E\left[\left(\dfrac{\partial \ln f(X;\theta)}{\partial \theta}\right)^2\right]},$$
    entonces se dice que $T$ es un estimador eficiente de $\theta$.
\end{def.}
\vspace{.5cm}

\subsection{Estadísticas suficientes}
Una estadística suficiente para un parámetro $\theta$ es aquella que utiliza toda la información contenida en la muestra aleatoria con respecto a $\theta$. La utilidad de una estadística suficiente recae en el hecho de que si un estimador insesgado de un parámetro $\theta$ es una función de una estadística suficiente, entonces tendrá la varianza más pequeña de entre todos los estimadores insesgado de $\theta$ que no se encuentren basados en una estadística suficiente. De hecho, si existe el estimador eficiente de $\theta$, se encontrará que este es una estadística suficiente. Un critero para terminar una estadística suficiente está dado por el siguiente teorema, el cual se conoce como teorema de factorización de Neyman.

%-------------------- teorema 8.4
\begin{teo}
    Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de una distribución con una función de densidad de probabilidad $f(x;\theta)$. Se dice que la estadística $T=u(X_1,X_2,\ldots,X_n)$ es una estadística suficiente para $\theta$ si y sólo si la función de verosimilitud  puede factorizarse de la siguiente manera:
    $$L(x_1,x_2,\ldots,x_n\; \theta)=h(t;\theta)g(x_1,x_2,\ldots,x_n)$$
    para cualquier valor $t=u(x_1,x_2,\ldots,x_n)$ de $T$ y en donde $g(x_1,x_2,\ldots,x_n)$ no contiene al parámetro $\theta$.
\end{teo}

\section{Métodos de estimación puntual}

Se estudiará cómo obtener estimadores que, de manera general, tenga buenas propiedades. Específicamente se considerarán los métodos de máxima verosimilitud y el de momentos. 

\subsection{Método por máxima verosimilitud}

El método de estimación por máxima verosimilitud, selecciona como estimador a aquél valor del parámetro que tiene la propiedad de maximizar el valor de la probabilidad de la muestra aleatoria observada. En otras palabras, el método de máxima verosimilitud consiste en encontrar el valor del parámetro que maximiza la función de verosimilitud.

%-------------------- definición 8.6
\begin{def.}
    Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de una distribución con función (densidad) de probabilidad $f(x;\theta)$, y sea $L(x_1,x_2,\ldots,x_n;\theta)$ la verosimilitud de la muestra como función de $\theta$. Si $t=u(x_1,x_2,\ldots,x_n)$ es el valor de $\theta$ para el cual el valor de la función de verosimilitud es máxima, entonces $T=u(X_1,X_2,\ldots,X_n)$ es el estimador de máxima verosimilitud de $\theta$, y $t$ es el estimador de máxima verosimilitud.
\end{def.}

Tiene la propiedad deseable de proporcionar estimadores que son funciones de estadísticas suficientes, siempre y cuando el estimador MV sea único. Además, el método MV proporciona el estimador eficiente, si es que existe. Sin, embargo los estimadores MV son generalmente sesgados. El procedimiento para obtener este tipo de estimadores es relativamente directo. Debido a la naturaleza de la función de verosimilitud se escoge, por lo común, maximizar el logaritmo natural de $L(\theta)$. Esto es, en muchas ocasiones más fácil obtener el estimador MV maximizando $\ln L(\theta)$ que $L(\theta)$.

\setcounter{ejem}{5}
%-------------------- Ejemplo 8.1
\begin{ejem}
    En un experimento binomial se observan $X=x$ éxitos en $n$ ensayos. Obtener el estimador de máxima verosimilitud del parámetro binomial $p$.\\\\
	Respuesta.-\; En este caso la función de verosimilitud es idéntica a la probabilidad de que $X=x$; de esta forma
	$$L(x;p)=\dfrac{n!}{(n-x)!x!}p^x(1-p)^{n-x},\quad 0\leq p\leq 1.$$
	Entonces,
	$$\ln L(x;p)=\ln(n!)-\ln\left[(n-x)!\right]-\ln(x!)+(n-x)\ln(1-p).$$
	Para encontrar el valor de $p$, para el cual $\ln L(x;p)$ tiene un valor máximo, se toma la primera derivada con respecto a $p$ y se iguala a cero:
	$$\dfrac{\partial \left[\ln L(x;p)\right]}{\partial p}=\dfrac{x}{p}-\dfrac{n-x}{1-p}=0.$$
	Después de resolver para $p$, se obtiene el estimador MV de $p$ el cual recibe el nombre de proporción muestral $X/n$, y el estimador MV es $p=x/n$. Para confirmar que este valor maximiza a $\ln L(x;p)$, se toma la segunda derivada con respecto a $p$ y se evalúa en $x/n$:
	$$\dfrac{\partial^2\left[\ln L(x;p)\right]}{\partial p^2}=-\dfrac{np(1-p)+(x-np)(1-2p)}{\left[p(1-p)\right]^2}$$
	y
	$$\dfrac{\partial^2\left[\ln L(x;p)\right]}{\partial p^2}\bigg|_{x/n} = -\dfrac{x}{(x/n)^2\left[1-(x/n)\right]}\qquad x/n<1,$$
	lo que confirma el resultado, dado que la segunda derivada es negativa. Para un ejemplo específico, si se observan $x=5$ con base en $25$ ensayos independientes, el estimador MC de $p$ es $0.2.$
\end{ejem}

%-------------------- Ejemplo 8.7
\begin{ejem}
    Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de una distribución normal con una función de densidad de probabilidad
    $$f(x;\mu,\sigma^2)=\dfrac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^2}{2\sigma^2}}.$$
    Determinar los estimadores de $\mu$ y $\sigma^2$.\\\\
	Respuesta.-\; Para este problema se procederá de la misma forma que en el caso de un solo parámetro. Dado que la función de verosimilitud depende tanto de $\mu$ como de $\sigma^2$, los estimadores MV de $\mu$ y $\sigma^2$ son los valores para los cuales la función de verosimilitud tiene un valor máximo. De acuerdo con lo anterior
	$$
	\begin{array}{rcl}
	    L(x_1,x_2,\ldots,x_n;\mu,\sigma^2) &=& \dfrac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x_1-\mu)^2}{2\sigma^2}}\cdots e^{\frac{-(x_n-\mu)^2}{2\sigma^2}}\\\\
					       &=& \left(2\pi \sigma^2\right)^{-\frac{n}{2}}e^{-\frac{1}{2\sigma^2}\sum\limits_{i=1}^n(x_i-\mu)^2},
	\end{array}
	$$
	y
	$$\ln \left[L\left(x_1,x_2,\ldots,x_n;\mu,\sigma^2\right)\right]=-\dfrac{n}{2}\ln (2\pi)-\dfrac{n}{2}\ln \left(\sigma^2\right)-\dfrac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2.$$

	Después de obtener las primeras derivadas parciales con respecto a $\mu$ y con respecto a $\sigma^2$ e ingualándolas a cero, se tiene 
	$$\dfrac{\partial\left[\ln L\left(\mu,\sigma^2\right)\right]}{\partial \mu}=-\dfrac{2}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)=0$$
	y
	$$\dfrac{\partial\left[\ln L\left(\mu,\sigma^2\right)\right]}{\partial \sigma^2}=-\dfrac{n}{2\sigma^2}+\dfrac{1}{2\sigma^4}\sum_{i=1}^n (x_i-\mu)^2=0.$$
	Resolviendo la primera ecuación para $\mu$, sustituyendo este valor en la segunda y resolviendo para $\sigma^2$, se tiene
	$$\hat{\mu}=\sum_{i=1}^n \dfrac{x_i}{n}=\overline{x}$$
	y
	$$\hat{\sigma}^2 = \sum_{i=1}^n \dfrac{\left(x_i-\overline{x}\right)^2}{n}.$$
\end{ejem}

Si tomamos la segunda derivada notaremos que estos valores maximizan la función de verosimilitud, ellos son los estimadores MV de $\mu$ y $\sigma^2$.\\

Notemos que se ha introducido la acostumbrada notación de sombrero, $\hat{\cdot}$ , para denotar un estimador MV. También notemos que el estimador MV de $\sigma^2$ es sesgado, confirmándose de esta manera un comentario anterior en el sentido en el que los estimadores MV no necesariamente son insesgados.\\

El método de máxima verosimilitud posee otra propiedad deseable conocida como propiedad de invarianza. Sea $\hat{\theta}=u(X_1,X_2,\ldots,X_n)$ el estimador de máxima verosimilitud de $\theta$. Si $g(\theta)$ es una función univaluada de $\theta$. Por ejemplo, dado que, cuando se muestrea una distribución normal, el estimador MV de $\sigma^2$ es
$$\hat{\sigma}^2 = \dfrac{1}{n}\sum_{i=1}^n \left(x_i-\overline{x}\right)^2,$$
por la propiedad de invarianza, el estimador MV de la desviación estándar $\sigma$ es
$$\hat{\sigma}=\left[\dfrac{1}{n}\sum_{i=1}^n (x_i-\overline{x})^2\right]^{\frac{1}{2}}.$$
Como ejemplo adicional de la propiedad de invarianza, el estimador MV de la función de confiabilidad Weibull es
$$\hat{R}(t)=e^{-\frac{t}{\hat{\theta}}^\alpha}$$
donde $\hat{\theta}$ es el estimador MV del parámetro de escalar $\theta$.


\subsection{Método de los momentos}

%-------------------- Definición 8.7
\begin{def.}
    Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de una distribución con función (densidad) de probabilidad $f(x;\theta)$. El r-ésimo momento alrededor de cero se define como
    $$M_r'=\dfrac{1}{n}\sum_{i=1}^n X_i^r.$$
\end{def.}

El método de los momentos proporciona una alternativa razonable cuando no se pueden determinar los estimadores de máxima verosimilitud. Recuérdese que los parámetros son, en general, funciones de los momentos teóricos. En esencia, el método se implementa igualando tantos momentos muestrales con los correspondientes momentos teóricos tantas veces como sea necesario para determinar un estimador de momentos para un parámetro desconocido.

\subsection{Estimación por máxima verosimilitud para muestras censuradas}
En algunas situaciones de muestreo, en forma especial en las pruebas de duración, el procedimiento de prueba puede terminar antes de proporcionar una muestra aleatoria completa. Una prueba típica de duración consiste en artículos iguales (tales como componentes eléctricos o mecánicos) seleccionados en forma aleatoria de un proceso y operando en un medio cuidadosamente controlado hasta que el articulo falla. Si la prueba de duración se termina sólo cuando todas las unidades de la muestra han fallado, se dice que la muestra aleatoria de tiempos está completa. Pero por varias razones la prueba termina ya sea después de un lapso de tiempo predeterminado $x_0$ o después de que falla un determinado número de unidades $m\leq n$. Las dos condiciones producen muestras censuradas.
