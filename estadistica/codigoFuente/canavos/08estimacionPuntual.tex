\chapter{Estimación puntual y por intervalo}

\setcounter{section}{1}
\section{Propiedades deseables de los estimadores puntuales}

Se conoce la familia de distribuciones a partir de la cual se obtiene la muestra, pero no puede identificarse el miembro de interés de esta, ya que no se conoce el valor del parámetro. Este último tiene que estimarse con base en los datos de la muestra. \\

El estimador de un parámetro $\theta$ debe tener una distribución de muestro concentrada alrededor de $\theta$ y la varianza del estimador debe ser la menor posible. Para ampliar las propiedades anteriores, considérese la siguiente. Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de tamaño $n$ proveniente de una distribución con función de densidad $f(x,\theta)$, y sea $T=u(X_1,X_2,\ldots,X_n)$ cualquier estadística. El problema es encontrar una función $u$ que sea la que proporcione la mejor estimación de $\theta$. Al buscar el mejor estimador de $\theta$ se hará uso de una cantidad muy importante que recibe el nombre de error cuadrático medio de un estimador.

%-------------------- definición 8.1
\begin{def.}
    Sea $T$ cualquier estimador de un parámetro desconocido $\theta$. Se define el error cuadrático medio de $T$ como el valor esperado del cuadrado de la diferencia entre $T$ y $\theta$.
\end{def.}

Para cualquier estadística $T$, se denotará el error cuadrático medio por $ECM(T)$; de esta forma
$$ECM(T)=E(T-\theta)^2.$$

Puede verse la razón del porque el error cuadrático medio es una cantidad importante para enjuiciar a los posibles estimadores de $\theta$ mediante el desarrollo de $ECM(T)=E(T-\theta)^2$; este es,

$$
\begin{array}{rcl}
    ECM(T) &=& \E(T^2-2\theta T+\theta^2)\\
	   &=& \E(T^2)-2\theta\E(T)+\theta^2\\
	   &=& \Var(T)+E^2(T)-2\theta\E(T)+\theta^2\\
	   &=& \Var(T)+\left[\theta-E(T)\right]^2.
\end{array}
$$

El error cuadrático medio de cualquier estimador es la suma de dos cantidades no negativas: una es la varianza del estimador y la otra es el cuadrado del sesgo del estimador. Estas dos cantidades se encuentran relacionadas en forma directa con las propiedades deseables de un estimador. La varianza de un estimador debe ser lo más pequeña posible mientras que la distribución de muestreo debe concentrarse del valor del parámetro. Por lo que debemos seleccionar el mejor estimador de $\theta$; es decir, la estadística que tenga el error cuadrático medio más pequeño posible entre todos los estimadores factibles de $\theta$. \\

\subsection{Estimadores insesgados}
El término $\theta-E(T)$ recibe el nombre de sesgo del estimador.

%-------------------- definición 8.2
\begin{def.}
    Se dice que la estadística $T=u(X_1,X_2,\ldots,X_n)$ es un estimador insesgado del parámetro $\theta$, si $E(T)=\theta$ para todos los posibles valores de $\theta$. De esta forma, para cualquier estimador insesgado de $\theta$, la distribución de muestreo de $T$ se encuentra centrada alrededor de $\theta$ y $ECM(T)=Var(T)$.
\end{def.}

En la sección 7.4, demostramos que sin importar la distribución de la población de interés, $E(\overline{X})=\mu$. Por lo tanto, la media muestral es un estimador insesgado de la media de la población $\mu$ para todo los valores de $\mu$. De hecho, si $X_1,X_2,\ldots,X_n$ es una muestra aleatoria de la distribución de $X$ con media $\mu$, entonces cualquier $X_i$ de la muestra es un estimador insesgado de $\mu$, dado que $E(X_i)=\mu$ para todo $i=1,2,\ldots, n$. Además si una estadística $T$ es cualquier combinación lineal de las variables aleatorias de la muestra de manera tal que
$$T=a_iX_1+a_2X_2+\cdots+a_nX_n$$
en donde $\sum_{i=1}^n a_i=1$, entonces $T$ es un estimador insesgado de $\mu$ dado que

$$
\begin{array}{rcl}
    E(T) &=& \displaystyle\sum_{i=1}^n a_iE(X_i)\\\\
	 &=& \displaystyle\sum_{i=1}^n a_i\mu\\\\
	 &=& \mu.
\end{array}
$$

En la sección 7.5 se demostró que si la varianza muestral $S^2$ está dada por $S^2=\sum_{i=1}^n(X_i-\overline{X})^2/(n-1)$. Entonces, cuando se muestrea una distribución normal, $E\left(S^2\right)=\sigma^2$. Lo que demostraremos que es insesgado sin importar cuál sea la distribución de la población de interés. Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de alguna distribución con una función de densidad no específica. De esta manera, $E(X_i)=\mu$ y $\Var(X_i)=\sigma^2$ para toda $i=1,2,\ldots,n.$ Entonces, 

$$
\begin{array}{rcl}
    \E\left(S^2\right) &=& \E\left[\displaystyle\sum_{i=1}^n \dfrac{(X_i-\overline{X})^2}{n-1}\right]\\\\
		       &=&(n-1)^{-1}\E\left\{\displaystyle\sum_{i=1}^n \left[(X_i-\mu)-(\overline{X}-\mu)\right]^2\right\}\\\\
		       &=& (n-1)^{-1}\E\left\{\displaystyle\sum_{i=1}^n \left[(X_i-\mu)^2-n\left(\overline{X}-\mu\right)^2\right]\right\}\\\\
		       &=& (n-1)^{-1}\left[\displaystyle\sum_{i=1}^n \E(X_i-\mu)^2-n\E\left(\overline{X}-\mu\right)^2\right]\\\\
\end{array}
$$
pero por definición $\E(X_i-\mu)^2=\Var(X_i)=\sigma^2$ y $E\left(\overline{X}-\mu\right)^2=\Var(\overline{X})=\sigma^2/n$. Por lo tanto,
$$
\begin{array}{rcl}
    E\left(S^2\right)&=&(n-1)^{-1}\left[\dfrac{n\sigma^2-\left(n\sigma^2\right)}{n}\right]\\\\
		     &=& \dfrac{\sigma^2(n-1)}{n-1}\\\\
		     &=& \sigma^2.
\end{array}
$$

En otras palabras, $S^2$ es un estimador insesgado de $\sigma^2$ sólo cuando el divisor es igual a $n-1$. Es la razón por la que se divide por $n-1$ en lugar de $n$. $S$ no hará un estimador insesgado de $\sigma$.


\subsection{Estimadores consistentes}

Es razonable esperar que un buen estimador de un parámetro $\theta$ sea cada vez menor conforme crece el tamaño de la muestra. Se tendrá un mejor estimador de $\theta$ si se basa en $30$ observaciones que si lo hace con sólo cinco.

%-------------------- definición 8.3
\begin{def.}
    Sea $T$ el estimador de un parámetro $\theta$, y sea $T_1,T_2,\ldots,T_n$ una secuencia de estimadores que representan a $T$ con base en muestras de tamaño $1,2,\ldots, n$, respectivamente. Se dice que $T$ es un estimador consistente (sencillo) para $\theta$ si
    $$\lim_{n\to \infty}P(|T_n-\theta|\leq \epsilon)=1$$
    para todos los valores de $\theta$ y $\epsilon>0$.
    El requisito constituye lo que se denomina convergencia en probabilidad. Es decir, si un estimador es consistente, converge en probabilidad al valor del parámetro que está intentando estimar conforme el tamaño de la muestra crece. Esto implica que la varianza de un estimador consistente $T_n$ disminuye conforme $n$ crece, y la media de $T_n$ tiende hacia donde $n$ crece. Para demostrar que $\overline{X}$ es un estimador consistente de $\mu$, primero se enuncia el teorema de desigualdad de Tchebysheff.
\end{def.}

%-------------------- teorema 8.1
\begin{teo}
    Sea $X$ una variable aleatoria con una función (densidad) de probabilidad $f(x)$ de manera tal que tanto $\E(X)=\mu$ como $Var(X)=\sigma^2$ tienen un valor finito. Entonces
    $$P(|X-\mu|\leq k\sigma)\geq 1-\dfrac{1}{k^2}$$
    o
    $$P(|X-\mu|>k\sigma)\leq \dfrac{1}{k^2}$$
    para cualquier constante $k\geq 1$.
\end{teo}

Este teorema permite determinar los límites de las probabilidades de variables aleatorias discretas y continuas sin tener que especificar sus funciones (densidades) de probabilidad. Asegura que la probabilidad de que una variable aleatoria se aleje no más de $k$ desviaciones estándar de la media, es mayor o igual a $1/k^2$ para algún valor de $k\geq 1$. Por ejemplo
$$P(|X-\mu|\leq 2\sigma)\geq 1-\dfrac{1}{4}$$
y
$$P(|X-\mu|\leq 3\sigma)\geq 1-\dfrac{1}{9}.$$
para cualquier variable aleatoria $X$ con media $\mu$ y varianza $\sigma^2$ finitas.

%-------------------- teorema 8.2
\begin{teo}
    Sean $X_1,X_2,\ldots,X_n$, $n$ variables aleatorias IID, tales que $\E(X_i)=\mu$ y $\Var(X_i)=\sigma^2$ tienen un valor finito para $i=1,2,\ldots,n.$ Entonces, $\overline{X}_n=\sum_{i=1}^n X_i/n$ es un estimador consistente de $\mu$.\\\\
	Demostración.-\; Se quiere demostrar que
	$$\lim_{n\to \infty}P(|\overline{X}_n-\mu|\leq \epsilon)=1.$$
	Dado que $\overline{X}_n$ es una variable aleatoria tal que $\E(\overline{X}_n)=\mu$ y $\Var(\overline{X}_n)=\sigma^2/n$, se deduce del teorema de Tchebysheff que
	$$P(|\overline{X}_n-\mu|>k\sigma/\sqrt{n})\leq 1/k^2.$$
    Sea $k$ una constante positiva igual a $\epsilon\sqrt{n}/\sigma$, en donde $\epsilon$ es un número real positivo. Entonces,
	$$P(|\overline{X}_n-\mu|>\epsilon)\leq \dfrac{\sigma^2}{n\epsilon^2}.$$
	Dado que $\sigma^2$ tiene un valor finito, tomando el límite de esta expresión conforme $n$ tiende al infinito se tiene
	$$\lim_{n\to \infty}P(|\overline{X}_n-\mu|> \epsilon)=0.$$
	Por lo tanto, se concluye que
	$$\lim_{n\to \infty}P(|\overline{X}_n-\mu|\leq \epsilon)=1.$$
	y $\overline{X}_n$ es un estimador consistente de $\mu$.
\end{teo}

El teorema 8.2 también se conoce como la ley de los grandes números.\\\\


\subsection{Estimadores insesgados de varianza mínima}

Para un parámetro que posee un error cuadrático medio mínimo es difícil determinar un estimador para todo los posibles valores del parámetro. Pero podremos analizar uno que tenga un error cuadrático medio mínimo. Si se tiene una varianza mínima para todos los valores posibles de $\theta$. Este estimador recibe el nombre de estimador insesgado de varianza mínima uniforme (VMU)

%-------------------- definición 8.4
\begin{def.}
    Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de una distribución cuya función (densidad) de probabilidad es $f(x;\theta)$. Sea la estadística $T=u(X_1,X_2,\ldots,X_n)$ un estimador de $\theta$ tal que $\E(T)=\theta$ y $Var(T)$ es menor que la varianza de cualquier otro estimador insesgado de $\theta$ para todo los posibles valores de $\theta$. Se dice entonces que $T$ es un estimador insesgado de varianza mínima de $\theta$.
\end{def.}

La varianza de un estimador insesgado es la cantidad más importante para decidir que tan bueno es el estimador para estimar un parámetro $\theta$. Por ejemplo, sean $T_1$ y $T_2$ cualesquiera dos estimadores insesgados de $\theta$. Se dice que $T_1$ es un estimador más eficiente de $\theta$ que $T_2$ si $\Var(T_1)\leq \Var(T_2)$. Es muy común utilizar el cociente $\Var(T_1)/\Var(T_2)$ para determinar la eficiencia relativa de $T_2$ respecto de $T_1$. Si los estimadores son sesgados, se emplea sus errores cuadráticos medios para determinar las eficiencias relativas.\\


%-------------------- teorema 8.3
\begin{teo}[Cota inferior de Cramérr-Rao]
    Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de una distribución con una función (densidad) de probabilidad $f(x;\theta)$. Si $T$ es un estimador insesgado de $\theta$, entonces la varianza de $T$ debe satisfacer la siguiente desigualdad
    $$\Var(T)\geq \dfrac{1}{n\E\left[\left(\dfrac{\partial \ln f(X;\theta)}{\partial \theta}\right)^2\right]}.$$
\end{teo}

El teorema 8.3 establece un límite inferior para la varianza de un estimador de $\theta$. Para un estimador insesgado cuya varianza se apega a la cota inferior de Cramér-Rao, se tiene la siguiente definición.

%-------------------- definición 8.5
\begin{def.}
    Si $T$ es cualquier estimador insesgado del parámetro $\theta$ tal que
    $$Var(T)\geq \dfrac{1}{n\E\left[\left(\dfrac{\partial \ln f(X;\theta)}{\partial \theta}\right)^2\right]},$$
    entonces se dice que $T$ es un estimador eficiente de $\theta$.
\end{def.}
\vspace{.5cm}

\subsection{Estadísticas suficientes}
Una estadística suficiente para un parámetro $\theta$ es aquella que utiliza toda la información contenida en la muestra aleatoria con respecto a $\theta$. La utilidad de una estadística suficiente recae en el hecho de que si un estimador insesgado de un parámetro $\theta$ es una función de una estadística suficiente, entonces tendrá la varianza más pequeña de entre todos los estimadores insesgado de $\theta$ que no se encuentren basados en una estadística suficiente. De hecho, si existe el estimador eficiente de $\theta$, se encontrará que este es una estadística suficiente. Un critero para terminar una estadística suficiente está dado por el siguiente teorema, el cual se conoce como teorema de factorización de Neyman.

%-------------------- teorema 8.4
\begin{teo}
    Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de una distribución con una función de densidad de probabilidad $f(x;\theta)$. Se dice que la estadística $T=u(X_1,X_2,\ldots,X_n)$ es una estadística suficiente para $\theta$ si y sólo si la función de verosimilitud  puede factorizarse de la siguiente manera:
    $$L(x_1,x_2,\ldots,x_n\; \theta)=h(t;\theta)g(x_1,x_2,\ldots,x_n)$$
    para cualquier valor $t=u(x_1,x_2,\ldots,x_n)$ de $T$ y en donde $g(x_1,x_2,\ldots,x_n)$ no contiene al parámetro $\theta$.
\end{teo}

\section{Métodos de estimación puntual}
