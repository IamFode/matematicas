\chapter{Variables aleatorias y distribución de probabilidad}

\section{El concepto de variables aleatorias}

%-------------------- Definición 3.1
\begin{tcolorbox}[colframe=white]
    \begin{def.} Sea $S$ un espacio muestral sobre el que se encuentra definida una función de probabilidad. Sea $X$ una función de valor real definida sobre $S$, de manera que transforme los resultados de $S$ en puntos sobre la recta de los reales. Se dice entonces que $X$ es un variable aleatoria.
	\end{def.}
\end{tcolorbox}

%-------------------- Definición 3.2
\begin{tcolorbox}[colframe=white]
	\begin{def.} Se dice que una variable aleatoria $X$ es discreta si el número de valores que puede tomar es contable (ya sea finito o infinito), y si éstos pueden arreglarse en una secuencia que corresponde con los enteros positivos.
	\end{def.}
\end{tcolorbox}
%-------------------- Definición 3.3
\begin{tcolorbox}[colframe=white]
    	\begin{def.} Se dice que una variable aleatoria $X$ es continua si sus valores consisten en uno o más intervalos de la recta de los reales.
	\end{def.}
\end{tcolorbox}

\section{Distribuciones de  probabilidad de variables aleatorias discretas}

%-------------------- Definición 3.4
\begin{tcolorbox}[colframe=white]
    \begin{def.} Sea $X$ una variable aleatoria discreta. Se llamará a $p(x) = P(X=x)$ función de probabilidad de la variable aleatoria $X$, si satisface las siguientes propiedades:
	\begin{enumerate}[\bfseries 1.]
	    \item $p(x)\geq 0$  para todos los valores $x$ de $X$;
	    \item $\sum\limits_x p(x)=1.$
	\end{enumerate}
	\end{def.}
\end{tcolorbox}

%-------------------- Definición 3.5
\begin{tcolorbox}[colframe=white]
	\begin{def.}
	    La función de distribución acumulativa de la variable aleatoria $X$ es la probabilidad de que $X$ sea menor o igual a un valor específico de $x$ y está dada por:
	    $$F(x) = P(X\leq x) = \sum\limits_{x_i \leq x} p(x_i)$$
	\end{def.}
\end{tcolorbox}

En general, la función de distribución acumulativa $F(x)$ de una variable aleatoria discreta es una función no decreciente de los valores de $X$, de tal manera que:
\begin{enumerate}[\bfseries 1.]
    \item $0\leq F(x) \leq 1$ para cualquier $x$;
    \item $F(x_i)\geq F(x_j)$ si $x_i\geq x_j;$
    \item $P(X>x) = 1 - F(x).$
    \item $P(X=x) = F(x) - F(x-1);$
    \item $P(x_i \geq X \geq x_j) = F(x_j) - F(x_i - 1)$
\end{enumerate}

\section{Distribuciones de probabilidad de variables aleatorias continuas}

%-------------------- Definición 3.6
\begin{tcolorbox}[colframe=white]
    \begin{def.}
	\begin{enumerate}[\bfseries 1.]
	    \item $f(x)\geq 0$, $-\infty<x<\infty$,
	    \item $\displaystyle\int_{-\infty}^\infty f(x) \;dx$ y 
	    \item $P(a\leq X \leq b) = \displaystyle\int_{a}^{b} f(x) \; dx$
	\end{enumerate}
    \end{def.}
\end{tcolorbox}

Para la función de distribución acumulativa $F(x)$ se tiene:
\begin{tcolorbox}[colframe=white]
    $$P(X\leq x) = F(x) = \int_{-\infty}^x f(t) \; dt$$
\end{tcolorbox}

Dado que para cualquier varible aleatoria continua $X$,
$$P(X=x) = \inf_x^x f(t)\; dt = 0, \qquad \Longrightarrow \qquad P(X\leq x) = P(X<x) = F(x)$$\\

La distribución acumulativa $F(x)$ es una función lisa no decreciente de los valores de la v.a. con las siguiente propiedades:
\begin{enumerate}[\bfseries 1.]
    \item $F(-\infty) = 0;$
    \item $F(\infty) = 1$;
    \item $P(a<X<b) = F(b) - F(a)$
    \item $dF(x)/dx = f(x).$
\end{enumerate}

\section{Valor esperado de una variable aleatoria}

%-------------------- Definición 3.7
\begin{tcolorbox}[colframe=white]
    \begin{def.}
	El valor esperado de una variable aleatoria $X$ es el promedio o valor medio de $X$ y está dado por:
	$$\begin{array}{rcll}
	    E(X)& = & \sum\limits_{x} xp(x) & \mbox{Si $x$ es discreta, o}\\\\
	    E(X) & = & \displaystyle\int_{-\infty}^\infty x f(x) \; dx & \mbox{Si $x$ es continua.}\\\\
	\end{array}$$
	En donde $p(x)$ y $f(x)$ son las funciones de probabilidad y de densidad de probabilidad, respectivamente.\\\\	
	En general, el valor esperado de una función $g(x)$ de la variables aleatoria $X$, está dado por:
	$$\begin{array}{rcll}
	    E(g(X))& = & \sum\limits_{x} g(x) p(x) & \mbox{Si $x$ es discreta, o}\\\\
	    E(g(X)) & = & \displaystyle\int_{-\infty}^\infty g(x)x f(x) \; dx & \mbox{Si $x$ es continua.}\\\\
	\end{array}$$
    \end{def.}
\end{tcolorbox}

\subsection{Propiedades}
\begin{enumerate}[\bfseries 1.]

    %---------- 1.
    \item El valor esperado de una constante $c$ es el valor de la constante.
	$$E(c) = \displaystyle\int_{-\infty}^\infty cf(x)\; dx = c\int_{-\infty}^{\infty} f(x) \; dx = c$$
    
    %---------- 2.
    \item El valor esperado de la cantidad $aX+b,$ en donde $a$ y $b$ son constantes, es el producto de $a$ por el valor esperado de $x$ más $b$.
	$$E(aX+b) = \displaystyle\int_{-\infty}^\infty (ax+b)f(x)\; dx = a\int_{-\infty}^\infty xf(x)\; dx + b \int_{-\infty}^\infty f(x)\; dx = aE(X)+b$$

    %---------- 3.
    \item El valor esperado de la suma de dos funciones $g(X)$ y $h(X)$ de $X$ es la suma de los valores esperados de $g(X)$ y $h(X)$.
	$$E\left[g(X) + h(X)\right] = \displaystyle\int_{-\infty}^\infty \left[g(x) + h(x)\right]\; dx \int_{-\infty}^\infty g(x)f(x)\; dx + \int_{-\infty}^infty h(x)f(x)\; dx = E\left[g(X)\right] + E\left[h(X)\right]$$
\end{enumerate}

\section{Momentos de una variable aleatoria}
 Los momentos de una variable aleatoria $X$ son los valores esperados de ciertas funciones de $X$. 

 %-------------------- Definición 3.8
 \begin{tcolorbox}[colframe=white]
     \begin{def.}
	 Sea $X$ una variable aleatoria. El r-ésimo momento de $X$ alrededor de cero se define por:
	 $$\begin{array}{rcll}
	     \mu_r^{'} & = & E(X^r) = \sum\limits_{x} x^r p(x) & \mbox{Si $x$ es discreta, o}\\\\
	     \mu_r^{'} & = & E(X^r) =  \displaystyle\int_{-\infty}^\infty x^r f(x)\; dx & \mbox{Si $x$ es continua.}\\\\
	 \end{array}$$
     \end{def.}
 \end{tcolorbox}
 El primer momento al rededor del cero es la media o valor esperado de la variable aleatoria. y se denota por $\mu$; de ésta manera se tiene que $\mu_1^{'} = \mu = E(X)$. 

 %-------------------- Definición 3.9
 \begin{tcolorbox}[colframe=white]
     \begin{def.}
	 Sea $X$ una variable aleatoria. El r-ésimo momento central de $X$ o el r-ésimo momento alrededor de la media de $X$ se define por:
	 $$\begin{array}{rcll}
	     \mu_r & = & E(X-\,u)^r = \sum_{x}(x-\mu)^r p(x) &  \mbox{Si $x$ es discreta, o}\\\\
	     \mu_r & = & E(X-\,u)^r = \displaystyle\int_{-\infty}^\infty (x-\mu)^r f(x)\; dx & \mbox{Si $x$ es continua.}\\\\
	 \end{array}$$
     \end{def.}
 \end{tcolorbox}

 El momento central de cero de cualquier variable aleatoria es uno, dado que:
 $$\mu_0 = E(X-\mu)^0 = E(1) = 1$$
 De manera similar, el primer momento central de cualquier variables aleatoria es cero, dado que:
 $$\mu_1 = E(E-\mu)^0 = E(X) - \mu = 0$$
 El segundo momento central:
 $$\mu_2 = E(X-\mu)^2$$
 Recibe el nombre de varianza de la variable aleatoria. Puesto que:
 $$\begin{array}{rcl}
     \mu_2 = Var(X) &=& E(X-\mu)^2\\
		    &=&E(X^2 - 2X\mu + \mu^2)\\
		    &=&E(X^2) - 2\mu^2 + \mu^2\\
		    &=&\mu_2^{'} - \mu^2\\
 \end{array}$$

 La varianza de cualquier variable aleatoria es el segundo momento alrededor del origen menos el cuadrado de la media. Generalmente se denota por $\sigma^2$\\\\
 Es útil notar que la varianza de una variable aleatoria $X$ es invariable; es decir, $Var(X+b) = Var(X)$ para cualquier constante $b$. De manera más general, se demostrará que $Var(aX+b) = a^2Var(X)$ para cualquiera dos constantes $a$ y $b$. Por definición,\\

 $$\begin{array}{rcl}
     Var(aX+b)&=&E(aX+b)^2 - E^2(aX+b)\\\\
	      &=&E(a^2X^2-2abX + b^2)-\left[a E(X)+b\right]^2\\\\
	      &=&a^2E(X)^2 + 2abE(X) + b^2 - a^2E^2(X) - 2abE(X) - b^2\\\\
	      &=&a^2 E(X)^2 - a^2E^2(X)\\\\
	      &=&a^2\left[E(X)^2 - E^2(X)\right]\\\\
	      &=&a^2Var(X)\\\\
 \end{array}$$

 Una medida que compara la dispersión relativa de dos distribuciones de probabilidad es el coeficiente de variación, que está definido por:
 $$V=\dfrac{\sigma}{\mu}$$
 Expresa la magnitud de la dispersión de una variable aleatoria con respecto a su valor esperado.\\\\

 El tercer momento central
 $$\mu_3 = E(X-\mu)^3$$
 esta relacionado con la asimetría de la distribución de probabilidad de $X$.\\\\

 Cualquier momento central de una variable aleatoria $X$ puede expresarse en términos de los momentos de ésta, alrededor de cero. Por definición:
 $$u_r = E(X-\mu)^r$$
 pero la expansión de $(X-\mu)^r$ puede expresarse como:
 $$(X-\mu)^r = \sum_{i=0}^r (-1)^i \dfrac{r!}{(r-i)!i!}\mu^i x^{r-i} = \sum_{i=0}^r (-1)^i \dfrac{r!}{(r-i)!i!}\mu^i E(X^{r-i}) = \sum_{i=0}^r (-1)^i \dfrac{r!}{(r-i)!i!}\mu^i \mu^{'}_{r-i}$$

 En particular,
 $$\mu_3 = \mu_3^{'} - 3\mu\mu^{'}_2 + 2\mu^3$$

 Para las distribuciones de probabilidad que presentan un sólo pico, si $\mu_3 < 0$, se dice que la distribución es asimétrica negativamente, si $\mu_3 > 0$, la distribución es asimétrica positivamente y si $\mu_3 = 0$, la distribución recibe el nombre de simétrica.\\
 Una medida más apropiada de la asimetría, es el tercer momento estandarizado, dado por:
 $$\alpha_3 = \dfrac{\mu_3}{(\mu_2)^{3/2}}$$
 Que recibe el nombre de coeficiente de asimetría. Una distribución de probabilidad es asimétrica positiva, negativa o simétrica si $\alpha_3 > 0$, $\alpha_3<0$ o $\alpha_3=0$ respectivamente.\\\\

 El cuarto momento central, 
 $$\mu_4 = E(X-\mu)^4 = \mu^{'}_4 - 4\mu \mu^{'}_3 + 6\mu^2 \mu^{'}_2 - 3 \mu^4$$
 Es una medida de qué tan puntiaguda es la distribución de probabilidad y recibe el nombre de curtosis. Al igual que para el tercer momento, es preferible emplear el cuarto momento estandarizado,
 $$\alpha_4  = \dfrac{\mu_4}{\mu^2_2}$$
 Si $\alpha_4>3$, la distribución de probabilidad presenta un pico relativamente alto y recibe el nombre de letocúrtica, si $\alpha_4<3$, la distribución es relativamente plana y recibe el nombre de platicúrtica y si $\alpha_4 = 3$, la distribución no presenta un pico muy alto i muy bajo y recibe el nombre de mesocúrtica. \\\\

En este momento se considerará el concepto de variable aleatoria estandarizada. 
Pagina 73


