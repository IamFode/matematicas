\chapter{Muestras aleatorias y distribuciones de muestreo}

\begin{def.}
    Si las variables aleatorias $X_1$, $X_2,\ldots, X_n$ tiene la misma función (densidad) de probabilidad que la de la distribución de la población y su función (distribución) conjunta de probabilidad es igual al producto de las marginales, entonces $X_1,X_2,\ldots , X_n$ forman un conjunto de $n$ variables aleatorias independientes e idénticamente distribuidas (IID) que constituyen una muestra aleatoria de la población.
\end{def.}

En el contexto ed la definición 7.1, la función (densidad) conjunta de probabilidad de $X_1,x_2,\ldots,X_n$ es la función de verosimilitud de la muestra dada por
\begin{tcolorbox}
    $$ L(x;\theta)=\prod\limits_{i=1}^n f(x_i; \theta),$$
\end{tcolorbox}
    en donde $x=\left\{x_1,x_2,\ldots.x_n\right\}$ denota los datos muestreados. Cunado las realizaciones $x$ se conocen, $L(x;\theta)$ es una función del parámetro desconocido $\theta$. 

\begin{ejem}
    Se ilustrará el concepto de muestra aleatoria dada en el definición 7.1 midiante lo siguiente: Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de $n$ variables aleatorias IID de una población cuya distribución de probabilidad es exponencial con densidad
    $$f(x;\theta)=\dfrac{1}{\theta}e^{-x/\theta},\quad 0<x<\infty.$$
    Cuando se observa $X_1$ y se registra su realización $x_1$,
    $$f(x_1;\theta)=\dfrac{1}{\theta}e^{-x_1/\theta},\quad 0<x_1<\infty.$$
    Ahora se observa $X_2$ y se registra su realización $x_2$. Dado que $X_1$ y $X_2$ son estadísticamente independientes y tienen las mismas densidades marginales,
    $$f(x_2|x_1)=f(x_2\; \theta)=\dfrac{1}{\theta}e^{-x_2/\theta},\quad 0<x_2<\infty.$$
    La función de densidad conjunta de $X_1$ y $X_2$ es
    $$f(x_1,x_2;\theta)=f(x_1;\theta)f(x_2;\theta)=\dfrac{1}{\theta^2}e^{-(x_1+x_2)/\theta},\quad 0<x_i<\infty,\; i=1,2.$$
    Por lo tanto, se desprende que para una muestra aleatoria de tamaño $n$
    $$L(x_1,x_2,\ldots,x_n; \theta)=\dfrac{1}{\theta}e^{-(x_1+x_2+\ldots +x_n)/\theta},\quad 0<x_1<\infty,\; i=1,2,\ldots,n.$$
\end{ejem}

\setcounter{section}{2}
\section{Distribuciones de muestreo de estadísticas}


\begin{def.}
    Un parámetro es una caracterización numérica de las distribuciones de la población de manera que describe, parcial o completamente, la función de densidad de probabilidad de la característica de interés. Por ejemplo, cuando se especifica el valor del parámetro de escala exponencial $\theta$, se describe de manera completa la función de probabilidad
    $$f(x;\theta)=\dfrac{1}{\theta}e^{(-x/\theta)}.$$
    La oración describe de manera completa, sugiere que una vez que se conoce el valor de $\theta$ entonces, puede formularse cualquier proposición probabilistica de interes.
\end{def.}

Dado que los parámetros son prácticamente inherentes a todos los modelos de probabilidad, es imposible calcular las probabilidades deseadas sin un conocimiento del valor de estos. Es por esta razón que la noción de una estadística y su distribución de muestreo es muy importante en inferencia estadística.

Antes de dar la definición de una estadística, debe notarse que desde un punto de vista clásico (no bayesiano), un parámetro se considera como una cosntante fija cuyo valor se desconoce. Desde una perspectiva bayesiana un parámetro siempre es una variable aleatoria con algún tipo de distribución de probabilidad.

\begin{def.}
    Una estadística es cualquier función de las variables aleatorias que se observaron en la muestra de manera que esta función no contiene cantidades desconocidas.
\end{def.}

De manera general, denótese una estadística por $T=u(X)$. Dado que $T$ es una función de variables aleatorias, es en sí misma una variable aleatoria, y su valor específico $t=u(x)$ puede determinarse cuando se conozcan las realizaciones $x$ de $X$. Si se emplea una estadística $T$ para estimar un parámetro desconocido $\theta$, entonces $T$ recibe el nombre de \textbf{estimador} de $\theta$, y el valor específico de $t$ como un resultado de los datos muestrales recibe el nombre de \textbf{estimación} de $\theta$. Esto es, un estimador es una estadística que identifica al mecanismo funcional por medio del cual, una vez que las observaciones en la muestra se realizan, se obtiene una estimación.\\

Un parámetro es una constante pero una estadística es una variable aleatoria. Además, un valor del parámetro descrito describe de manera completa un modelo de probabilidad; ningún valor de la estadística puede desempeñar tal papel si cada uno de estos depende del valor de las observaciones de las muestras. 

%------------------- Definición 7.4
\begin{def.}
    La distribución de muestreo de una estadística $T$ es la distribución de probabilidad de $T$ que puede obtenerse como resultado de un número infinito de muestras aleatorias independientes, cada una de tamaño $n$, provenientes de la población de interés.
\end{def.}

La distribución de muestreo de una estadística hace posible este tipo de análisis de probabilidad, esencial para valorar el riesgo inherente cuando se formulan ingerencias.

%-------------------- Teorema 7.1
\begin{teo}
    Sea $X_1,X_2,\ldots, X_n$ un conjunto de $n$ variables aleatorias independientes cada una con función generadoras de momentos $m_{X_1}(t),m_{X_2}(t),\ldots , m_{X_n}(t).$
    Si 
    $$Y=a_1X_1+a_2X_2+\ldots + a_nX_n,$$
    en donde $a_1,a_2,\ldots,a_n$ son constantes, entonces:
    $$m_Y(t)=m_{X_i}(a_1t)m_{X_2}(a_2t)\cdots m_{X_n}(a_nt).$$
	Demostración.-\; Mediante el empleo de la definición y la hipótesis de independencia, se tiene
	$$\begin{array}{rcl}
	    m_Y(t)&=&E\left\{e^{\left[t\left(a_1X_1+a_2X_2+\ldots + a_nX_n\right)\right]}\right\}\\\\
		  &=&E\left\{e^{\left[t\left(a_1X_1\right)\right]}e^{\left[t\left(a_2X_2\right)\right]}\cdots e^{\left[t\left(a_nX_n\right)\right]}\right\}\\\\
		  &=&E\left\{e^{\left[t\left(a_1X_1\right)\right]}\right\}E\left\{e^{\left[t\left(a_2X_2\right)\right]}\right\}\cdots E\left\{e^{\left[t\left(a_nX_n\right)\right]}\right\}\\\\
		  &=&m_{X_1}(a_1t)m_{X_2}(a_2t)\cdots m_{X_n}(a_nt).
	\end{array}$$
\end{teo}

De esta forma, la función generadora de momentos de una combinación lineal de $n$ variables aleatorias independientes es el producto de las correspondientes funciones generadoras de momentos con argumentos iguales a las constantes de tiempo $t$.

%------------------- Teorema 7.2
\begin{teo}
    Sea $X_1,X_2,\ldots,X_n$ un conjunto de variables aleatorias independientes normalmente distribuidas con medias $E(X_i)=\mu_i$ y varianzas $Var(X_i)=\sigma^2_i$ para $i=1,2,\ldots n$. Si
    $$Y=a_1X_1+a_2X_2+\ldots a_nX_n,$$
    en donde $a_2,a_2,\ldots,a_n$ son constantes, entonces $Y$ es una variable aleatoria con distribución normal y media
    $$E(Y)=a_1\mu_1+a_2\mu_2+\ldots + a_nX_n$$
    y con varianza 
    $$Var(Y)=a_1^2\sigma_1^2+a_2^2\sigma_2^2 + \ldots + a_n^2\sigma_n^2.$$\\
	Demostración.-\; Dado que $X_i$ se encuentra normalmente distribuida, su función generadora de momentos es
	$$m_{X_i}(t)=e^{\left(\mu_i t + \frac{\sigma_i^2t^2}{2}\right)}.$$
	De acuerdo con el teorema 7.1, la función generadora de momentos de $Y$ es
	$$\begin{array}{rcl}
	    m_Y(t)&=&m_{X_1}(a_1t)m_{X_2}(a_2t)\cdots m_{X_n}(a_nt)\\\\
		  &=&e^{\left(\mu_1 a_1t + \frac{a_1^2\sigma_1^2t^2}{2}\right)}e^{\left(\mu_2a_2t + \frac{a_2^2\sigma_2^2t^2}{2}\right)}\cdots e^{\left(\mu_n a_nt + \frac{a_n^2\sigma_n^2t^2}{2}\right)}\\\\
		  &=&e^{\left[t\sum\limits_{i=1}^n a_i\mu_i+\frac{\left(t^2\sum\limits_{i=1}^n a_i^2\sigma_i^2\right)}{2}\right]}.
      \end{array}$$
      Por lo tanto, $Y$ se encuentra normalmente distribuida con media $\sum_{i=1}^na_i\mu_i$ y varianza $\sum_{i=1}^n a_i^2 \sigma_i^2.$
\end{teo}

Del teorema 7.2 se desprende que si $a_i=1$ para $i=1,2,\ldots,n$, entonces la suma de variables aleatorias independientes normalmente distribuidas también posee una distribución normal con media y varianza igual a la suma de las medias y las varianzas de cada una de las variables aleatorias. El resultado anterior se conoce como la propiedad aditiva de la distribución normal. Debe notarse que la hipótesis de normalidad no es necesaria para obtener las fórmulas de la media y la varianza de $Y$ en el teorema 7.2. De hecho, con base en el teorema 6.1, si $X_1,X_2,\ldots , X_n$ es un conjunto de $n$ variables aleatorias IID con medias $E(X_i)=\mu_i$ y varianzas $Var(X_i)=\sigma_i^2$, $i=1,2,\ldots , n$ entonces para $Y=a_1X_1+a_2X_2+\ldots +a_nX_n,$
$$E(Y)=\sum_{i=1}^{n}a_i\mu_i$$
y
$$Var(Y)=\sum_{i=1}^n a_i^2 \sigma_i^2.$$
en donde $a_1,a_2,\ldots,a_n$ son constantes.


\section{\boldmath La distribución de muestreo de $\overline{X}$}

Sea $X_1,X_2,\ldots , X_n$ una muestra aleatoria que consiste en $n$ variables aleatorias IID tales que $E(X_i)=\mu$ y $Var(X_i)=\sigma^2$ para toda $i=1,2,\ldots,n$. Entonces la estadística
$$\overline{X}=\dfrac{X_1+X_2+\ldots + X_n}{n}$$
se define como la media de las $n$ variables aleatorias IDD o, sencillamente, media muestral. Nótese que una vez que se conocen las realizaciones $x_1,x_2\ldots,x_n$ de $X_1,X_2,\ldots,X_n$, respectivamente, realización $\overline{x}$ de $\overline{X}$ se obtiene promediando los datos muestrales. \\
Si en, $E(Y)=\sum_{i=1}^{n}a_i\mu_i$ y $Var(Y)=\sum_{i=1}^n a_i^2 \sigma_i^2$, $a_i=1/n$, $i=1,2,\ldots,n$ entonces el valor esperado y la varianza de $\overline{X}$ son
\begin{tcolorbox}
$$E(\overline{X})=\sum_{i=1}^n \dfrac{1}{n}\mu=n\left(\dfrac{\mu}{n}\right)=\mu.$$
\end{tcolorbox}
y
\begin{tcolorbox}
$$\Var(\overline{X}) = \sum_{i=1}^n \dfrac{1}{n^2} \sigma^2 = n \left(\dfrac{\sigma^2}{n^2}\right)=\dfrac{\sigma^2}{n},$$
\end{tcolorbox}
respectivamente, en donde $\mu$ y $\sigma^2$ son la media y la varianza de la distribución de la población a partir de la cual se obtuvo la muestra. Luego de esta última ecuación de $Var(\overline{X})$ la desviación estándar de $\overline{X}$ es
\begin{tcolorbox}
    $$d.e.(\overline{X})=\dfrac{\sigma}{\sqrt{n}}.$$
\end{tcolorbox}
la cual, en algunas ocasiones recibe el nombre de \textbf{error estándar de la media}. Si el tamaño de la muestra crece, la precisión de la media muestral para estimar la media poblacional aumenta. 

%-------------------- Teorema 7.3
\begin{teo}
    Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria que consiste de $n$ variables aleatorias independientes y normalmente distribuidas con media $E(X_i)=\mu$ y varianzas $Var(X_i)=\sigma^2$, $i=1,2,\ldots,n$. Entonces la distribución de la media muestral $\overline{X}$ es normal con media $\mu$ y varianza $\sigma^2/n.$\\\\
	Demostración.-\; Este teorema es un corolario del teorema 7.2. Esto es, sea $a_i=1/n$; dado que las medias y las varianzas son iguales, respectivamente, la función generadora de momentos de $\overline{X}$ es
	$$\begin{array}{rcl}
	    m\overline{X}(t) &=&  e^{\left(t\displaystyle\sum_{i=1}^n \dfrac{1}{n}\mu+\dfrac{t^2\displaystyle\sum_{i=1}^n \frac{1}{n^2}\sigma^2}{2}\right)}\\\\
			     &=&e^{\left(t\mu+\dfrac{t^2\sigma^2}{2n}\right)},
	\end{array}$$
	que es la función generadora de momentos de una variable aleatoria normalmente distribuida con media $\mu$ y varianza $\sigma^2/n$. De esta forma, la \textbf{\boldmath función de densidad de probabilidad de $\overline{X}$ cuando se muestrea una población cuya distribución es normal}, está dada por
	\begin{tcolorbox}
	    $$f\left(\overline{x},\mu,\sigma/\sqrt{n}\right)=\dfrac{\sqrt{n}}{\sqrt{2\pi}\sigma}e^{\left[-\dfrac{n(\overline{x}-\mu)^2}{2\sigma^2}\right]}, \quad -\sigma<\overline{x}<\sigma.$$
	\end{tcolorbox}
\end{teo}

% -------------------  ejemplo 
\begin{ejem}

    Demostrar que si $X_1,X_2,\ldots,X_n$ son $n$ variables aleatorias independientes  exponencialmente distribuidas con función de densidad de probabilidad
    $$f(x;\theta)=\dfrac{1}{\theta}e^{-x/\theta}\qquad x>0,$$
    entre $\overline{X}$ tiene una distribución gama.\\\\
	Demostración.-\; Recuérdese que la función generadora de momentos de una variable aleatoria exponencialmente distribuida es $(1-\theta t)^{-1}$. De esta manera para cada $X_i$ de la muestra,
	$$m_{X_i}(t)=(1-\theta t)^{-1}.$$
	Del teorema 7.1 con $a_i=\frac{1}{n},\; i=1,2,\ldots , n$ se desprende que la función generadora de momentos de la media muestral $\overline{X}$ es 
	$$
	\begin{array}{rcl}
	    m_{\overline{X}}(t) &=& m_{X_i}\left(\dfrac{t}{n}\right)m_{X_2}\left(\dfrac{t}{n}\right)\ldots m_{X_n}\left(\dfrac{t}{n}\right)\\\\
				&=& \left[1-\left(\dfrac{\theta t}{n}\right)\right]^{-1}\left[1-\left(\dfrac{\theta t}{n}\right)\right]^{-1}\ldots \left[1-\left(\dfrac{\theta t}{n}\right)\right]^{-1}\\\\
				&=& \left[1-\left(\dfrac{\theta t}{n}\right)\right]^{-n}.
	\end{array}
	$$
	Pero la expresión anterior es la función generadora de momentos de una distribución gama con parámetro de forma $n$ y parámetro de escala $\theta/n$. De acuerdo con lo anterior, cuando se muestrea una población cuya distribución de probabilidad es exponencial, la densidad de probabilidad de $\overline{X}$ está dada por
	$$f(\overline{x};n,\theta/n)=\dfrac{n^n}{\Gamma (n) \theta^n}\overline{x}^{n-1}e^{-n\overline{x}/\theta},\qquad \overline{x}>0.$$
	Nótese que si en las expresiones $E(X)=\alpha\theta$ y $\Var(X)=\alpha\theta^2$ se reemplaza $\alpha$ con $n$ y $\theta$ con $\theta/n$ se obtiene
	\begin{tcolorbox}
	    $$E(\overline{X})=n\dfrac{\theta}{n}=\theta$$
	\end{tcolorbox}
	y
	\begin{tcolorbox}
	    $$Var(\overline{X})=n\dfrac{\theta^2}{n^2}=\dfrac{\theta^2}{n},$$
	\end{tcolorbox}
	como era de esperarse ya que $\theta$ y $\theta^2$ son la media y la varianza de una variable aleatoria con distribución exponencial.\\

	De la sección 5.5, recuérdese que si el parámetro de forma de una distribución gama tiene un valor grande, entonces los valores de probabilidad para una variable aleatoria gama pueden aproximarse, en forma adecuada, por una distribución normal. Dado que $\Gamma^m,$ muestrear una distribución exponencial con parámetro $\theta$, $\overline{X}$ tiene una distribución gama con media $\theta$, y desviación estándar $\dfrac{\theta}{\sqrt{n}}$. Entonces, para $n$ grande
	$$Z=\dfrac{\overline{X}-\theta}{\dfrac{\theta}{\sqrt{n}}}.$$
	es, en forma aproximada, $N(0,1)$.
\end{ejem}

Para ser que para un valor grande $n$, la distribución de $\overline{X}$ es aproximadamente normal. De hecho, no importa el tipo de modelo de probabilidad a partir del cual se obtenga la muestra; muestras la media y la varianza existan, la distribución de muestreo de $\overline{X}$ se encontrará aproximada por $N\left(\mu, \sigma/\sqrt{n}\right)$ para valores grandes de $n$. Lo anterior constituye uno de los más importantes teoremas en inferencia estadística, y se conoce como \textbf{teorema central del límite}.

% -------------------  Toeorema 7.4
\begin{teo}
    Sean $X_1,X_2,\ldots , X_n$ variables aleatorias $IID$ con una distribución de probabilidad no especificada y que tiene una media $\mu$ y varianza $\sigma^2$ finita. El promedio muestral $\overline{X}=\left(X_1+X_2+\ldots + X_n\right)/n$ tiene una distribución con media $\mu$ y varianza $\sigma^2/n$ que tiende hacia una distribución normal conforme $n$ tiende a $\infty$. En otras palabras, la variable aleatoria $(\overline{X}-\mu)/(\sigma/\sqrt{n})$ tiene como límite una distribución normal estándar.\\\\
    Demostración.-\; Se quiere demostrar que la función generadora de momentos de $(\overline{X}-\mu)/(\sigma/\sqrt{n})$ tiende a la de una distribución normal estándar conforme $n$ tiende al infinito. Sean
    $$Z_i=\dfrac{X_i-\mu}{\sigma},\qquad i=1,2,\ldots,n$$
    y
    $$Y=\dfrac{\overline{X}-\mu}{\dfrac{\sigma}{\sqrt{n}}}.$$
    Dado que 
    $$\dfrac{1}{n}\sum_{i=1}^n \left(\dfrac{X_i-\mu}{\dfrac{\sigma}{\sqrt{n}}}\right)=\dfrac{1}{n}\dfrac{1}{\dfrac{\sigma}{\sqrt{n}}}\sum_{i=1}^n (X_i-\mu)=\dfrac{1}{n}\dfrac{1}{\dfrac{\sigma}{\sqrt{n}}}(n\overline{X}-n\mu)=\dfrac{\overline{X}-\mu}{\dfrac{\sigma}{\sqrt{n}}}.$$
    Entonces,
    $$Y=\dfrac{1}{\sqrt{n}}\sum_{i=1}^n Z_i.$$
    Como resultado se tiene que la función generadora de momentos de $Y$ es igual a la función generadora de momentos de $(1/\sqrt{n})\sum_{i=1}^n Z_i$. Del teorema 7.1,
    $$
    \begin{array}{rcl}
	m_Y(t)&=&\left[m_{Z_i}(t/\sqrt{n})\right]^n\\\\
	      &=& \left\{E\left[e^{\frac{tZ_i}{\sqrt{n}}}\right]\right\}^n,
    \end{array}
    $$
    dado que las $Z_i$ son variables aleatorias independientes.\\
    Al expandir $(tZ_i/\sqrt{n})$ en una serie de Taylor:
    $$e^{\frac{tZ_i}{\sqrt{n}}}=1+\dfrac{t}{\sqrt{n}}Z_i + \dfrac{t^2}{2n}Z_i^2 + \dfrac{t^3}{3!n^{\frac{3}{2}}}Z_i^3 + \ldots .$$
    Si se toma los valores esperados y se recuerda que $E(Z_i)=0$ y $Var(Z_i)=1$, $i=1,2,\ldots,n$ se tiene
    $$E\left[e^{\frac{tZ_i}{\sqrt{n}}}\right]=1+\dfrac{t^2}{2n}+\dfrac{t^3}{3!n^{\frac{3}{2}}}E\left(Z_i^3\right)+\ldots.$$
    De acuerdo con lo anterior
    $$
    \begin{array}{rcl}
	m_Y(t) &=& \left[1+\dfrac{t^2}{2n}+\dfrac{t^3}{3!n^{\frac{3}{2}}}E\left(Z_i^3\right)+\ldots\right]^n\\\\
	       &=& \left\{1+\dfrac{1}{n}\left[\dfrac{t^2}{2}+\dfrac{t^3}{3!\sqrt{n}}E\left(Z_i^3\right)+\ldots\right]\right\}^n\\\\
	       &=& \left(1+\dfrac{u}{n}\right)^n
    \end{array}
    $$
    donde
    $$u=\dfrac{t^2}{2}+\dfrac{t^3}{3!\sqrt{n}}E\left(Z_i^3\right)+\ldots.$$
    Ahora
    $$\lim_{n\to \infty}m_Y(t)=\lim_{n\to \infty}\left(1+\dfrac{u}{n}\right)^n.$$
    Pero por definición 
    $$\lim_{n\to \infty}\left(1+\dfrac{u}{n}\right)=e^u.$$
    Lo anterior da como resultado una situación idéntica a la que se tiene en la demostración del teorema 5.1. Esto es, conforme $n\to \infty$, todos los términos en $u$, excepto el primero, tienden hacia cero debido a que todos tienen potencias positivas de $n$ en sus denominadores. Por lo tanto, se puede deducir que
    $$\lim_{x\to \infty}m_Y(t)=e^{\frac{t^2}{2}},$$
    o la distribución límite de $Y=(\overline{X}-\mu)/(\sigma/\sqrt{n})$ es la normal estándar para valores grandes de $n$.
\end{teo}

La esencia del teorema central del límite recae en el hecho de que para $n$ grande, la distribución $(\overline{X}-\mu)/(\sigma/\sqrt{n})$es, en forma aproximada, normal con media cero y desviación estándar uno, sin importar cuál sea el modelo de probabilidad a partir del que se obtuvo la muestra. Debe notarse que si el modelo de probabilidad de la población es semejantes a una distribución normal (esto es, si es simétrico y existe una concentración relativamente alta alrededor del punto de simetría), la aproximación normal será buena aun para pequeñas muestras. Por otro lado, si el modelo de la población tiene muy poco parecido a una distribución normal (por ejemplo existe una alta asimetría), la aproximación normal sólo será adecuada para valores relativamente grandes a $n$. En muchas casos, puede concluirse de forma segura, que la aproximación será buena mientras $n>30$. Por lo tanto, la variable aleatoria
$$Z=\dfrac{\overline{X}-\mu}{\dfrac{\sigma}{\sqrt{n}}}$$
se emplea para formular inferencias acerca de $\mu$ cuando se conoce el valor de la varianza población $\sigma^2la.$

\section{La distribución de muestro de \boldmath$S^2$}

Otra estadística importante empleada para formular inferencias con respecto a las varianzas de la población es la varianza denotada por $S^2$. Para comenzar, es necesario suponer que $\mu$ es conocida y $\sigma^2$ no. Así, $\sigma^2$ se encuentra definida por
$$S^2=\sum_{i=1}^n \dfrac{(X_i-\mu)^2}{n},$$
en donde $X_1,X_2,\ldots,X_n$ constituye una muestra aleatoria de una distribución normal con media $\mu$ y varianza $\sigma^2$ desconocida. Para determinar una distribución de muestreo que permita hacer inferencias sobre $\sigma^2$ con base en $S^2$ definida por (7.13), se enuncia y demuestra el siguiente teorema.

%-------------------- teorema 7.5
\begin{teo}
    Sean $X_1,X_2,\ldots ,X_n$ una muestra aleatoria de una distribución normal con media $\mu$ y varianza $\sigma^2$. La distribución de la variable aleatoria 
    $$Y=\sum_{i=1}^n \dfrac{(X_i-\mu)^2}{\sigma^2}$$
    es del tipo chi-cuadrada con $n$ grados de libertad.\\\\
	Demostración.-\; Dado que $X_i\sim N(\mu,\sigma)$, $i=1,2,\ldots,n$, $Z_i=\dfrac{X_i-\mu}{\sigma}$ define $n$ variables aleatorias normales estándar independientes, se tiene:
	$$Y=\sum_{i=1}^n Z_i^2.$$
	Del teorema 7.1,
	$$
	\begin{array}{rcl}
	    m_Y(t)&=&m_{Z^2_i}(t)m_{Z^2_2}(t)\ldots m_{Z^2_n}(t)\\\\
		  &=& (1-2t)^{-\frac{1}{2}}(1-2t)^{-\frac{1}{2}}\ldots (1-2t)^{-\frac{1}{2}}\\\\
	\end{array}
	$$
	dado que el cuadrado de una variable aleatoria normal estándar tiene una distribución chi-cuadrada con un grado de libertad (véase el ejemplo 5.14). De esta forma se tiene
	$$m_Y(t)=(1-2t)^{-\frac{n}{2}}$$
	que es la función generadora de momentos de una distribución chi-cuadrada con $n$ grados de libertad. De acuerdo con lo anterior, $Y\sim X_n^2$.
\end{teo}

Desde un punto de vista práctica, la varianza muestra tal como se encuentra definida por 
$$S^2=\sum\limits_{i=1}^n \dfrac{(X_i-\mu)^2}{n}$$
tiene poco uso, ya que es muy raro que se conozca el valor de la media poblacional $\mu$. De acuerdo con lo anterior, si se muestra una distribución normal con media $\mu$ y varianza $\sigma^2$, la varianza muestral se define por
\begin{tcolorbox}
    $$S^2=\sum_{i=1}^n \dfrac{\left(X_i-\overline{X}\right)^2}{n-1}$$
\end{tcolorbox}
En el capítulo 8 se verá por qué se emplea el divisor $(n-1)$. Para determinar la distribución de muestreo de $S^2$, y con base en una muestra aleatoria proveniente de una distribución normal, debe tomarse en cuenta el promedio de la muestra $\overline{X}$. Como resultado se tiene que la distribución de muestreo de $\dfrac{(n-1)S^2}{\sigma^2}$ es también una distribución chi-cuadrada con $n-1$ grados de libertad. Para ello probaremos primero un teorema útil que involucra suma de dos variables aleatorias independientes chi-cuadrada y entonces se escribirá la anterior expresión en una forma equivalente, con objeto de aprovechar este teorema.

%-------------------- teorema 7.6
\begin{teo}
    Si $X_1$ y $X_2$ son dos variables aleatorias independientes y cada una tiene una distribución chi-cuadrada con $v_1$ y $v_2$ grados de libertad respectivamente, entonces:
    $$Y=X_1+X_2$$
    también tiene una distribución chi-cuadrada con $v_1+v_2$ grados de libertad.\\\\
	Demostración.-\; Del teorema 7.1, la función generadora de momentos de $Y$ es
	$$
	\begin{array}{rcl}
	    m_Y(t) &=& m_{X_i}(t)m_{X_2}(t)\\\\
		   &=& (1-2t)^{-v_1/2} (1-2t)^{-v_2/2}\\\\
		   &=& (1-2t)^{-(v_1+v_2)/2}
	\end{array}
	$$
	que es la función generadora de momentos de una variable aleatoria chi-cuadrada con $v_1+v_2$ grados de libertad.
\end{teo}

Ahora se deducirá la distribución de muestreo de $\dfrac{(n-1)S^2}{\sigma^2}$. De  
$$S^2=\sum_{i=1}^n \dfrac{\left(X_i-\overline{X}\right)^2}{n-1}$$
se tiene que
$$(n-1)S^2=\sum_{i=1}^n \left(X_i-\overline{X}\right)^2$$
pero
$$
\begin{array}{rcl}
    \displaystyle\sum_{i=1}^n \left(X_i-\overline{X}\right)^2 &=& \displaystyle\sum_{i=1}^n \left(X_i-\mu-\overline{X}+\mu\right)^2\\\\
							      &=& \displaystyle\sum_{i=1}^n \left[(X_i-\mu)-(\overline{X}-\mu)\right]^2\\\\
							      &=& \displaystyle\sum_{i=1}^n \left[\left(X_i-\mu\right)^2-2\left(X_i-\mu\right)\left(\overline{X}-\mu\right)+\left(\overline{X}-\mu\right)^2\right]\\\\
							      &=& \displaystyle\sum_{i=1}^n \left(X_i-\mu\right)^2-2\left(\overline{X}-\mu\right) \displaystyle\sum_{i=1}^n \left(X_i-\mu\right)+n\left(\overline{X}-\mu\right)^2\\\\
							      &=&\displaystyle\sum_{i=1}^n\left(X_i-\mu\right)^2-2\left(\overline{X}-\mu\right)n\left(\overline{X}-\mu\right)+n\left(\overline{X}-\mu\right)^2\\\\
							      &=& \displaystyle\sum_{i=1}^n \left(X_i-\mu\right)^2-n\left(\overline{X}-\mu\right)^2.
\end{array}
$$

De esta forma

$$(n-1)S^2+n\left(\overline{X}-\mu\right)^2=\sum_{i=1}^n \left(X_i-\mu\right)^2.$$

Al dividir ambos miembros de la expresión anterior por la varianza poblacional $\sigma^2$ se tiene

\begin{tcolorbox}
    $$\dfrac{(n-1)S^2}{\sigma^2}+\dfrac{n\left(\overline{X}-\mu\right)^2}{\sigma^2}=\dfrac{(n-1)S^2}{\sigma^2}+\left(\dfrac{\overline{X}-\mu}{\dfrac{\sigma}{\sqrt{n}}}\right)^2=\dfrac{\displaystyle\sum_{i=1}^n \left(X_i-\mu\right)^2}{\sigma^2}$$
\end{tcolorbox}

Del teorema 7.5 se desprende que $\sum_{i=1}^n (X_i-\mu)^2/\sigma^2$ tiene una distribución chi-cuadrada con $n$ grados de libertad. De manera similar, $\left[(\overline{X}-\mu)/\sigma/\sqrt{n}\right]^2$ también posee una distribución chi-cuadrada con un grado de libertad, dado que $(\overline{X}-\mu)/(\sigma/\sqrt{n})$ es $N(0,1)$. Por lo tanto, si se supone que $(n-1)S^2/\sigma^2$ y $\left[(\overline{X}-\mu)/(\sigma/\sqrt{n})\right]^2$ son variables aleatorias independientes, entonces por el teorema 7.6, cuando se muestrea una población cuya distribución es normal con media y varianza desconocida, la distribución de $(n-1)S^2/\sigma^2$, es chi-cuadrada con $n$-grados de libertad. La función de densidad de probabilidad de $Y=(n-1)S^2/\sigma^2$ se desprende de (5.58) y está dada por:
$$
f(y;n-1)=
\left\{
    \begin{array}{lc}
	\dfrac{1}{\Gamma\left[(n-1)/2\right]2^{(n-1)/2}}y^{\left[(n-1)/2\right]-1}e^{-y/2} & y>0,\\\\
	0&\mbox{para cualquier otro valor.}
    \end{array}
\right.
$$
Nótese, que dado $Y\sim X_{n-1}^2$, $E(Y)=n-1$ y $Var(Y)=2(n-1)$. Además, ya que $Y=(n-1)S^2/\sigma^2$, $S^2=\sigma^2Y/(n-1)$. Por lo tanto
\begin{tcolorbox}
    $$\E\left(S^2\right)=E\left(\dfrac{\sigma^2 Y}{n-1}\right)=\dfrac{\sigma^2}{n-1}E(Y)=\sigma^2,$$
\end{tcolorbox}

y 

\begin{tcolorbox}
    $$\Var\left(S^2\right)=Var\left(\dfrac{\sigma^2Y}{n-1}\right)=\dfrac{\sigma^4}{(n-1)^2}Var(Y)=\dfrac{2\sigma^4}{n-1}.$$
\end{tcolorbox}


\section{La distribución t de Student}

Desde el punto de vista práctico, la necesidad de conocer $\sigma$ impide formular inferencias con respecto a $\mu$ debido a que generalmente no se conoce el valor de la desviación estándar de la población. El camino lógico será estimar $\sigma$ con una estimación $s$, que es el valor de la desviación estándar muestral $S$. Desafortunadamente, la distribución $('overline{X}-\mu)/(S/\sqrt{n})$ no es $N(0,1)$. Sin embargo es posible determinar la distribución de muestreo exacta cuando se muestra $N(\mu,\sigma)$, con $\mu$ y $\sigma^2$ desconocidos. Acá se examinará los aspectos teóricos de lo que se conoce como la distribución t de Student.
