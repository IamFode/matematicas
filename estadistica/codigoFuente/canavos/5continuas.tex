\chapter{Algunas distribuciones continuas de probabilidad}

\section{La distribución normal}

\begin{tcolorbox}
    \begin{def.}
	se dice que una variable aleatoria $X$ se encuentra normalmente distribuida si su función de densidad de probabilidad está dada por 
	$$f(x;\mu,\sigma) = \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2},\quad \begin{array}{l}-\infty<x<\infty \\ -\infty<\mu<\infty,\; \sigma>0\end{array}$$
    \end{def.}
\end{tcolorbox}

Si se obtienen las dos primeras derivadas de $f(x;\mu,\sigma)$ con respecto a $x$ y se igualan a cero, se tiene que el valor máximo de $f(x;\mu,\sigma)$ ocurre cuando $x=\mu,$ y los valores $x=\mu\pm \sigma$ son las abcisas de los dos puntos de inflexión de la curva.\\\\

\textbf{Demostrar que la definición 5.1 es una función de densidad de probabilidad.}\\\\
    Demostración.-\; El que la función sea no negativa se satisface, ya que $f(x;\mu,\sigma)>0$ para $-\infty<x<\infty,\; -\infty<\mu<\infty$ y $\sigma>0$. Para demostrar que:
    $$\int_{-\infty}^\infty f(x;\mu,\sigma)\; dx = 1.$$
    Sea
    $$I=\dfrac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^\infty e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx$$
    el valor de la integral y aplíquese la transformación lineal $y=(x-\mu)/\sigma$ de manera tal que $x=\sigma y + \mu$ y $dx = \sigma \; dy$. Esto da como resultado:
    $$I=\dfrac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-y^2/2}\; dy.$$
    Si puede demostrarse que $I^2=1$, puede deducirse que $I=1$ puesto que $f(x;\mu,\sigma)$ tiene un valor positivo. De acuerdo con lo anterior:
    $$I^2 = \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty}e^{-y^2/2}\; dy \cdot \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty}e^{-z^2/2}\; dz = \dfrac{1}{\sqrt{2\pi}} \int_{\infty}^\infty \int_{-\infty}^\infty e^{\frac{(y^2+z^2)}{2}}\; dy dz,$$
    en donde se ha escrito el producto de las dos integrales como una doble integral ya que las funciones de $z$ son contantes con respecto a $y$ como también de manera viceversa. Al cambiar de coordenadas rectangulares representadas por $x$ e $y$, a coordenadas polares $r$ y $\theta$, en donde $y=r\cos \theta$ y $z=r\sen \theta$. Esto es:
    $$y^2+z^2 = r^2\cos^2\theta + r^2\sen^2 \theta = r^2$$
    y el elemento de área $dy dz$, en coordenadas rectangulares se reemplaza por $rd rd\theta$ en coordenadas polares. Dado que los límites $(-\infty,\infty)$ tanto para $y$ como para $z$ generan el plano completo $yz$, el plano correspondiente a $r$ y a $\theta$ se genera mediante el empleo de los límites $(0,2\pi)$ para $\theta$ y $(0,\infty)$ para $r$. De esta forma se tiene:
    $$I^2 = \dfrac{1}{2\pi}\int_0^{2\pi} \int_0^x e^{-r^2/2}\; rdrd\theta = \dfrac{1}{2\pi}\int_0^{2\pi}\;d\theta \int_0^x e^{-r^2/2}\; rdr = \dfrac{\theta}{2\pi}\bigg|_0^{2\pi}\cdot [-e^{-r^2/2}]\bigg|_0^x = 1.$$\\

    \textbf{La media de una variable aleatoria distribuida normalmente} se encuentra definida por:
\begin{tcolorbox}
    $$E(X)=\dfrac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^x x e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx.$$
\end{tcolorbox}

Se pretende demostrar que $E(X)=\mu$. Supóngase que a $E(X)=\dfrac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^x x e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx$ se suma y se resta
$$\dfrac{\mu}{\sqrt{2\pi}\sigma}\int_{-\infty}^\infty e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}\; dx$$

La identidad se mantiene, pero después de reacomodar términos se tiene
$$\begin{array}{rcl}
    E(X)&=&\displaystyle\int_{-\infty}^\infty (x-\mu)e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx + \dfrac{\mu}{\sqrt{2\pi}\sigma}\int_{-\infty}^\infty e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx\\\\
	&=&\displaystyle\dfrac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^x (x-\mu)e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx + \mu\\\\
\end{array}$$

dado que el valor de la segunda integral es uno. Al afectar un cambio de variable de integración de manera tal que $y=\dfrac{x-\mu}{\sigma},\; x=\sigma y+\mu$ y $dx=\sigma dy$, se tiene:

\begin{tcolorbox}
$$E(X)=\dfrac{\sigma}{\sqrt{2\pi}}\int_{-\infty}^\infty y e^{-y^2/2}\; dy + \mu = -\dfrac{\sigma}{\sqrt{2\pi}}e^{-y^2/2}\bigg|_{-\infty}^\infty + \mu = \mu.$$
\end{tcolorbox}

El lector recordará de sus cursos de cálculo que la última integral es cero porque el integrando es una función impar y la integración se lleva a cabo sobre un intervalo simétrico alrededor de cero.\\\\

Si el valor máximo de la función de densidad de probabilidad normal ocurre cuando $x=\mu$ este es la media, la mediana y la moda de cualquier variable aleatoria distribuida aleatoriamente.\\
Para encontrar los demás momentos, se determinará la función generadora de momentos. Por definición:
$$m_{X-\mu}(t) = E\left[e^{t(X-\mu)}\right] = \dfrac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty e^{t(x-\mu)} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx = \dfrac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty e^{-\frac{1}{2\sigma^2}\left[(x-\mu)^2-2\sigma^2 t(x-\mu)\right]}\; dx.$$
de donde se completa el cuadrado en el interior del paréntesis rectangular y se tiene:
$$(x-\mu)^2-2\sigma^2 t(x-\mu) = (x-\mu)^2-2\sigma^2 t(x-\mu)+\sigma^4 t^2 - \sigma^4 t^2 = (x-\mu-\sigma^2t)^2 - \sigma^4 t^2.$$
Por lo que,
$$m_{X-\mu}(t) = \dfrac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^\infty e^{\frac{\sigma^2 t^2}{2}}e^{-\frac{x-(\mu+\sigma^2 t)^2}{2\sigma^2}}\; dx = e^{\frac{\sigma^2t^2}{2}},$$
dado que el integrando junto con el factor $\dfrac{1}{\sqrt{2\pi}\sigma}$ es una función de densidad de probabilidad normal con parámetros $\mu+\sigma^2 t$ y $\sigma.$

Al desarrollar $e^{\frac{\sigma^2 t^2}{2}}$ en serie de potencias se tiene:
$$m_{X-\mu}(t) = 1+\dfrac{(\sigma t)^2}{2}+\dfrac{(\sigma t)^4}{4\cdot 2!}+\dfrac{(\sigma t)^6}{8\cdot 3!}+\dfrac{(\sigma t)^8}{16\cdot 4!} + \ldots$$
Cuando las potencias impares de $t$ no se encuentran presentes, todos los momentos centrales de $X$ de orden impar son cero, de esta forma se asegura la simetría de la curva.\\\\

La segunda derivada de $m_{X-\mu}(t)$  evaluada en $t=0$ es \textbf{la varianza} y está dada por:
\begin{tcolorbox}
    $$Var(X)=\dfrac{d^2m_{X-\mu}(t)}{dt^2}\bigg|_{t=0}=\sigma^2 + \dfrac{12t^2 \sigma^4}{4\cdot 2!}+\dfrac{30t^4\sigma^6}{8\cdot 3!} + \ldots \bigg|_{t=0} = \sigma^2;$$
\end{tcolorbox}

De esta manera \textbf{ la desviación estándar es  $\mathbold \sigma$}. De manera similar, la cuarta derivada de $m_{X-\mu}(t)$ evaluada en $t=0$ es el cuarto momento central, el cual es:
$$\mu_4=\dfrac{d^4m_{X-\mu}(t)}{dt^4}\bigg|_{t=0}=3\sigma^4 + \dfrac{360t^2\sigma^6}{8\cdot 3!}+ \ldots \bigg|_{t=0}=3\sigma^4$$
De acuerdo con lo anterior, para cualquier distribución normal el coeficiente de asimetría es $\alpha_3(X)=0,$ mientras que la curtosis relativa es $\alpha_4(X)=\dfrac{3\sigma^4}{\sigma^4} = 3.$ Para momentos alrededor del cero, puede determinarse la función generadora de momentos centrales o viceversa. Dado que
$$m_{X-\mu}(t)=E\left[e^{t(X-\mu)}\right]=e^{-\mu t}E\left[e^{tX}\right] = e^{-ut}m_X(t),$$
para una distribución normal
$$e^{-\mu t}m_X (t) = e^{\frac{\sigma^2t^2}{2}}$$
y
$$m_X (t)=e^{\mu t + \frac{\sigma^2t^2}{2}}.$$

La probabilidad de que una variable aleatoria normalmente distribuida $X$ sea menor o igual a un valor específico, $x$ está dada por \textbf{función de distribución acumulativa}
\begin{tcolorbox}
    $$P(X\leq x)=F(x;\mu,\sigma) = \dfrac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{x} e^{\frac{-(t-\mu)^2}{2\sigma^2}}\; dt.$$
\end{tcolorbox}

Sea $Z$ una variable aleatoria definida por la siguiente relación:

$$Z=\dfrac{(X-\mu)}{\sigma}$$

en donde $\mu$ y $\sigma$ son la media y la desviación estándar de $X$, respectivamente. De acuerdo con lo anterior, $Z$ es una variable aleatoria estandarizada con media cero y desviación estándar uno. Así,
\begin{tcolorbox}
    $$P(X\leq x) = P[X\leq (x-\mu)/\sigma] = \dfrac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{(x-\mu)/\sigma} e^{\frac{-z^2}{2}}\sigma \; dz = \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^{(x-\mu)/\sigma} e^{\frac{-z^2}{2}}\; dz.$$
\end{tcolorbox}
El integrando junto con el factor $1/\sqrt{2\pi}$ es la \textbf{función de densidad de probabilidad de la variable aleatoria normal estandarizada Z}. De donde 
$$F_X(x;\mu,\sigma) = F_Z(z;0,1)$$

Para cualquier valor específico de $z$, el correspondiente valor en la tabla es la probabilidad de que la variable aleatoria normal estándar $Z$ sea menor o igual a $z$; esto es
\begin{tcolorbox}
    $$P(Z\leq z)=F_Z(z;0,1) = \dfrac{1}{\sqrt{2\pi}}\int_{-\infty}^z e^{\frac{-t^2}{2}}\; dt.$$
\end{tcolorbox}

La notación $X\sim N(\mu,\sigma)$ denotará que la variable $X$ se encuentra distribuida normalmente con media $\mu$ y desviación estándar $\sigma$.\\\\
Determinaremos la probabilidad de que un valor de $X$ se encuentre entre $a$ y $b$, si $X\sim N(\mu,\sigma)$. Por definición:
$$P(a\leq X\leq b) = \dfrac{1}{\sqrt{2\pi}\sigma} \int_a^b e^{\frac{-(x-\mu)^2}{2\sigma^2}}\; dx,$$

pero, mediante el empleo de $E(X)=\displaystyle\dfrac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^x (x-\mu)e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx + \mu$ se tiene:

\begin{tcolorbox}
    $$P(a\leq X\leq b) = P\left(\dfrac{a-\mu}{\sigma}\leq Z \leq \dfrac{b-\mu}{\sigma}\right) = \dfrac{1}{\sqrt{2\pi}}\int_{\frac{a-\mu}{\sigma}}^{\frac{b-\mu}{\sigma}} e^{\frac{-z^2}{2}}\; dz = F_Z\left(\dfrac{b-\mu}{\sigma};0,1\right)-F_Z\left(\dfrac{a-\mu}{\sigma};0.1\right)$$
\end{tcolorbox}

\begin{teo}
    Sea $X$ una variable aleatoria binomial con media $np$ y desviación estándar $\sqrt{np(1-p)}$. La distribución de la variable aleatoria tiende a la normal
    $$Y=\dfrac{X-np}{\sqrt{np(1-p)}}$$\\
    estándar conforme el número de ensayos independientes $n\to \infty$.\\\\
	Demostración.-\; La demostración que aquí se presenta se basa en el hecho de que una función generadora de momentos define, de manera única, a una distribución. Se demostrará que la función generadora de momentos de $Y$ tiende a una distribución normal conforme $n\to \infty$. $X$ es una variable aleatoria binomial:
	$$m_X(t)=[(1-p)+p e^t]^n$$
	Entonces:
	$$m_Y(t)=E(e^{tY})=E\left[e^{\frac{t(X-np)}{\sqrt{np(1-p)}}}\right] = e^{\frac{npt}{\sqrt{np(1-p)}}}\cdot E\left[e^{\frac{tX}{\sqrt{np(1-p)}}}\right]$$
	donde $E\left[e^{\frac{tX}{\sqrt{np(1-p)}}}\right]$ es la función generadora de momentos de $X$ con argumento $\frac{t}{\sqrt{np(1-p)}}$. De esta forma se tiene:
	$$m_y(t)=e^{\frac{-npt}{\sqrt{np(1-p)}}}\left[(1-p)+ pe^{\frac{t}{\sqrt{np(1-p)}}}\right]^n;$$
	pero:
	$$e^{-\frac{npt}{\sqrt{np(1-p)}}} = \left(e^{-\frac{pt}{\sqrt{np(1-p)}}}\right)^n$$
	y:
	$$m_y(t)=\left[(1-p)e^{-\frac{pt}{\sqrt{np(1-p)}}}+pe^{\frac{t}{\sqrt{np(1-p)}}-\frac{pt}{\sqrt{np(1-p)}}}\right]^n = \left[(1-p)e^{-\frac{pt}{\sqrt{np(1-p)}}}+pe^{\frac{(1-p)t}{\sqrt{np(1-p)}}}\right]^n.$$
	En la última expresión, al expander ambas funciones exponenciales en una serie de potencias, se tiene:
	$$\begin{array}{rcl}
	    (1-p)e^{-\frac{pt}{\sqrt{np(1-p)}}} &=& (1-p) - \dfrac{(1-p)pt}{\sqrt{np(1-p)}} + \dfrac{(1-p)p^2t^2}{2np(1-p)} + \mbox{términos en } (-1)^k \left(\dfrac{1}{n}\right)^{k/2}, \; k=3,4,\ldots\\\\
						&=&(1-p)-\dfrac{(1-p)pt}{\sqrt{np(1-p)}}+\dfrac{pt^2}{2n}+\mbox{términos en } (-1)^k\left(\dfrac{1}{n}\right)^{k/2},\; k=3,4,\ldots\\\\
	\end{array}$$
	y
	$$\begin{array}{rcl}
	    pe^{-\frac{(1-p)t}{\sqrt{np(1-p)}}} &=& p + \dfrac{(1-p)pt}{\sqrt{np(1-p)}} + \dfrac{(1-p)pt^2}{2np(1-p)} + \mbox{términos en } \left(\dfrac{1}{n}\right)^{k/2}, \; k=3,4,\ldots\\\\
						&=&p+\dfrac{(1-p)pt}{\sqrt{np(1-p)}}+\dfrac{(1-p)t^2}{2n}+\mbox{términos en } \left(\dfrac{1}{n}\right)^{k/2},\; k=3,4,\ldots\\\\
	\end{array}$$

	Al sustituir los resultados anteriores en $m_Y(t)$ y agrupar términos,

	$$m_Y(t)=\left[1+\dfrac{t^2}{2n}+\mbox{ términos en }\left(\dfrac{1}{n}\right)^{k/2}\right]^n,\; k=3,4,\ldots$$
	Dado que todos los términos que contiene a $(1/n)^{k/2},\; k=3,4,\ldots$, tienen exponentes mayores que uno, puede factorizarse el término $1/n$. De esta forma se tiene que:

	$$m_Y(t)=\left[1+\dfrac{1}{n} \left(\dfrac{t^2}{2}+\mbox{ términos en }\left(\dfrac{1}{n}\right)^{(k-2)/2}\right)\right]^n,\; k=3,4,\ldots$$

	Por definición:
	$$\lim_{n\to \infty}\left(1+\dfrac{u}{n}\right)^n=e^u$$
	entonces, conforme $n\to \infty$, la última expresión para $m_Y(t)$ es idéntica a esta forma, con $u$ representando a todo lo que se encuentra entre paréntesis de esta expresión. Pero conforme $n\to \infty$, todos los términos de $u$, excepto el primero, tienen un valor de cero, dado que todos tienen potencias positivas de $n$ en sus denominadores. De acuerdo con lo anterior.
	$$\lim_{n\to \infty}m_Y(t)=e^{t^2/2},$$
	que es la función generadora de momentos de la distribución normal estándar.\\\\

\end{teo}

La aproximación del teorema anterior es adecuada tanto como $np>5$ cuando $p\leq 1/2$, o cuando $n(1-p)>5$ para $p>1/2$. Esto es
$$P(a\leq X_B \leq b) = P\left(\dfrac{a-np}{\sqrt{np(1-p)}}\leq Z_N\leq \dfrac{b-np}{\sqrt{np(1-p)}}\right)$$
en donde $Z_N$ es $N(0,1)$. Como también
$$P(X_B=x)\approx P\left(\dfrac{x-np-1/2}{\sqrt{no(1-p)}}\leq Z_N\leq \dfrac{x-np+1/2}{\sqrt{np(1-p)}}\right)$$
por lo que se puede modificar la expresión de desigualdad de la siguiente manera:
$$P(a\leq X_B \leq b) = P\left(\dfrac{a-np-0.5}{\sqrt{np(1-p)}}\leq Z_N\leq \dfrac{b-np+0.5}{\sqrt{np(1-p)}}\right)$$


\section{La distribución uniforme}

\begin{tcolorbox}
    \begin{def.}
	Se dice que una variable aleatoria $X$ está distribuida uniformemente sobre el intervalo $(a,b)$ si su función de densidad de probabilidad está dada por:
	$$f(x;a,b) = \left\{ \begin{array}{ll}
		\dfrac{1}{b-a} & a\leq x\leq b \\\\
		0 & \mbox{para cualquier otro valor.} \\
	\end{array} \right.$$
    \end{def.}
\end{tcolorbox}

\textbf{La función de distribución acumulativa} se determina de manera fácil y está dada por
\begin{tcolorbox}
    $$P(X\leq x)=F_{x;a,b}=\dfrac{1}{b-a}\int_a^x dt = \left\{\begin{array}{ll}
	    0 & x<a,\\
	    \dfrac{x-a}{b-a} & a\leq x\leq b,\\
	    1 & x>b.
	\end{array}\right.$$
\end{tcolorbox}

Se sigue entonces que para cualquier subintervalo $(a_1,b_1)$ interior a $(a,b)$:
$$P(a_1\leq X\leq b_1)=F(b_1;a,b)-F(a_1;a,b)=\dfrac{b_1-a_1}{b-a}.$$\\

Este resultado ilustra que la probabilidad de que $X$ tome valores del subintervalo $\left(a_1,b_1\right)$, es $\dfrac{1}{b-a}$ por la longitud del subintervalo y de esta forma, igual a la probabilidad de que $X$ tome un valor en cualquier otro subintervalo de la misma longitud.\\\\

El \textbf{valor esperado} de una variable aleatoria distribuida de manera uniforme es 

\begin{tcolorbox}
    $$E(X)=\dfrac{1}{b-a}\int_a^b x\;dx = \dfrac{a+b}{2}.$$
\end{tcolorbox}

Para obtener los momentos superiores de $X$, es más fácil trabajar con la variable aleatoria $Y=X-\dfrac{a+b}{2}$, que desplaza la media a cero, dado que $E(Y)=E(X)-\dfrac{a+b}{2}$. De esta forma:
$$f(y;\theta)=\dfrac{1}{\theta},\qquad -\dfrac{\theta}{2}\leq y\leq \dfrac{\theta}{2},$$
en donde $\theta=b-a.$ De acuerdo con lo anterior, el r-ésimo momento central de $Y$ es igual al r-ésimo momento central alrededor del cero, esto es:
$$\mu_r(Y)=\mu_r'(Y)=\theta^{-1}\int_{-\theta/2}^{\theta/2}y^r \; dy = \left(\dfrac{1}{\theta}\right)\dfrac{y^{r+1}}{r+1}\bigg|_{-\theta/2}^{\theta/2}=\left\{\begin{array}{ll}0&\mbox{si r es impar}\\ \theta^r/[(r+1)2^r]&\mbox{si r es par}\end{array}\right.$$
Dado que ni la varianza ni los factores de forma se ven afectados por el cambio de localización, la varianza, el coeficiente de asimetria y la curtosis relativa de la variable aleatoria distribuida uniformente se encuentran a partir de $\left\{\begin{array}{ll}0&\mbox{si r es impar}\\ \theta^r/[(r+1)2^r]&\mbox{si r es par}\end{array}\right.$ y están determinadas por:
\begin{tcolorbox}
    $$Var(X)=\dfrac{(b-a)^2}{12}$$
\end{tcolorbox}

\begin{tcolorbox}
    $$\alpha_3(X)=0$$
\end{tcolorbox}

\begin{tcolorbox}
    $$\alpha_4(X)=\dfrac{\frac{(b-a)^4}{80}}{\left[\frac{(b-a)^2}{12}\right]^2}=\dfrac{9}{5}.$$
\end{tcolorbox}

Puede emplearse $f(y;\theta)=\dfrac{1}{\theta},\qquad -\dfrac{\theta}{2}\leq y\leq \dfrac{\theta}{2},$ para determinar la \textbf{desviación media} de la siguiente manera:

\begin{tcolorbox}
    $$E|Y|=\theta^{-1}\int_{-\theta/2}^{\theta/2}=2\theta^{-1}\int_{0}^{\theta/2} y \; dy = \dfrac{\theta}{4}=\dfrac{b-a}{4}.$$
\end{tcolorbox}

No tiene moda y su \textbf{mediana} es igual a la media. Los valores cuantiles $x_q$, correspondientes a la proporción acumulativa $q$, son de manera tal que:
\begin{tcolorbox}
    $$F(x_q,a,b)=q$$
\end{tcolorbox}

Los que por $\left\{\begin{array}{ll}
	    0 & x<a,\\
	    \dfrac{x-a}{b-a} & a\leq x\leq b,\\
	    1 & x>b.
	\end{array}\right.$
son:

$$x_q=a+(b-a)q.$$

\section{La distribución beta}

\begin{tcolorbox}
    \begin{def.}
	Se dice que una variable aleatoria $X$ posee una distribución beta si su función de densidad está dada por:
	$$f(x;\alpha,\beta)=\left\{\begin{array}{ll}
		\dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1} & 0<x<1,\quad \alpha,\beta >0,\\\\
		0&\mbox{para cualquier otro valor}
	     \end{array}\right.$$
	     Recordemos que $\Gamma(n)=\displaystyle\int_{0}^{\infty} u^{n-1}e^{-u} du,\; n>0$.
    \end{def.}
\end{tcolorbox}

Si tanto $\alpha$ como $\beta$ son menores que uno, la distribución beta tiene un perfil en forma de $U$. Si $\alpha<1$ y $\beta\geq 1$, la distribución tiene un perfil de $J$ transpuesta, y si $\beta<1$ y $\alpha\geq 1$, el perfil es una $J$. Finalmente cuando $\alpha=\beta$ la distribución es simétrica. Nótese que si en la definición se reemplaza por $x-1$, se obtiene la siguiente relación de simetría:
$$f(1-x;\beta,\alpha) = f(x;\alpha,\beta)$$

El nombre de esta distribución proviene de su asociación con la función beta que se encuentra definida por
$$B(\alpha,\beta) = \int_{0}^{1}x^{\alpha-1}(1-x)^{\beta -1}\; dx$$
Puede demostrarse que las funciones beta y gama se encuentran relacionadas por la expresión
$$B(\alpha,\beta)=\dfrac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}.$$

Mediante el empleo de estas últimas fórmulas, es obvio que la definición es una función de densidad de probabilidad. Esto es:
$$\dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\int_0^1x^{\alpha-1}(1-x)^{\beta-1}\; dx = \dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} B(\alpha,\beta) = 1,$$

y puesto que $f(x;\alpha,\beta)$ es no negativa, entonces la definición es una función de densidad de probabilidad.\\\\

La \textbf{función de distribución acumulativa} se encuentra definida por:

\begin{tcolorbox}
	\begin{def.}
	$$F(x;\alpha,\beta)=\left\{\begin{array}{ll}
		0 & x\leq 0,\\\\
		\dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\displaystyle\int_0^x t^{\alpha-1}(1-t)^{\beta-1} \; dt & 0<x<1,\\\\
		1&x\geq 1.
	     \end{array}\right.$$
	\end{def.}
\end{tcolorbox}

La integral que aparece es la función beta incompleta:
$$B_x(\alpha,\beta)=\int_0^x t^{\alpha-1}(1-t)^{\beta-1}\; dt.$$

De esta forma, la función de distribución beta puede expresarse como un cociente de funciones beta incompletas,
\begin{tcolorbox}
    $$ F(x;\alpha,\beta)=\dfrac{B_x(\alpha,\beta)}{B(\alpha,\beta)}=\dfrac{\displaystyle\int_0^x t^{\alpha-1}(1-t)^{\beta-1}\; dt}{\displaystyle\int_{0}^{1}x^{\alpha-1}(1-x)^{\beta -1}\; dx}=I_x(\alpha,\beta),\;\; 0<x<1,$$
\end{tcolorbox}

Con el fin de encontrar los valores cuantiles correspondientes a puntos de alto porcentaje, considérese lo siguiente:
$$P(X\leq x)=P(1-X\geq 1-x)=1-P(1-X\leq 1-x);$$

entonces, por la relación de simetría $f(1-x;\beta,\alpha)=f(x;\alpha,\beta)$:

$$F(x;\alpha,\beta)=1-F(1-x;\beta,\alpha)$$
o
$$I_x(\alpha,\beta)=1-I_{1-x}(\beta,\alpha).$$\\

Obtenemos los momentos de la variable aleatoria beta como sigue.

$$\begin{array}{rcl}
    \mu_r'&=&\dfrac{\Gamma (\alpha+\beta)}{\Gamma (\alpha)\Gamma (\beta)}\displaystyle\int_0^1 x^{\alpha+r-1}(1-x)^{\beta - 1}\; dx\\\\
    &=&\dfrac{\Gamma (\alpha+\beta)}{\Gamma (\alpha)\Gamma (\beta)}B(\alpha+r,\beta)\\\\
    &=&\dfrac{\Gamma (\alpha+\beta)}{\Gamma (\alpha)\Gamma (\beta)}\cdot \dfrac{\Gamma (\alpha + r)\Gamma (\beta)}{\Gamma (\alpha + \beta +r)}\\\\
    &=&\dfrac{\Gamma (\alpha+\beta)\Gamma (\alpha + r)}{\Gamma (\alpha)\Gamma (\alpha + \beta + r)}\\\\
\end{array}$$

Como resultado,

\begin{tcolorbox}
    $$E(X)=\dfrac{\Gamma(\alpha+\beta)\Gamma(\alpha + 1)}{\Gamma (\alpha)\Gamma (\alpha + \beta + 1)}= \dfrac{\alpha}{\alpha+\beta}.$$
\end{tcolorbox}

Y

\begin{tcolorbox}
    $$Var(x)=\dfrac{\alpha(\alpha+1)}{(\alpha+\beta)(\alpha+\beta+1)}-\dfrac{\alpha^2}{(\alpha+\beta)^2}=\dfrac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta + 1)}.$$
\end{tcolorbox}

Al seguir este procedimiento y después de efectuar el álgebra necesaria, el \textbf{el coeficiente de asimetría} y la \textbf{curtosis relativa} para la distribución beta están dadas por:

\begin{tcolorbox}
    $$\alpha_3(X)=\dfrac{2(\beta + \alpha)\sqrt{\alpha+\beta+1}}{\sqrt{\alpha \beta}(\alpha+\beta+2)}$$
\end{tcolorbox}

Y

\begin{tcolorbox}
    $$\alpha_4(X)=\dfrac{3(\alpha+\beta+1)\left[2(\alpha+\beta)^2\alpha\beta (\alpha+\beta - 6)\right]}{\alpha\beta (\alpha+\beta + 2)(\alpha +\beta + 3)}$$
\end{tcolorbox}

Puede demostrarse que si la suma de los parámetros que determinan el perfil de la distribución beta es, de manera relativa, grande, la función de distribución acumulativa beta se puede aproximar de manera adecuada por la diferencia de dos funciones de distribución normal estándar. Esto es:

$$F(x;\alpha, \beta) \approx F_N(z_u;0,1)-F_N(z_l; 0,1),$$

En donde:

$$z_u=\dfrac{[\beta] - 0.5 - (\alpha+\beta-1)(1-x)}{\left[x(\alpha+\beta - 1)(1-x)\right]^{1/2}},$$
$$z_t=-\dfrac{(\alpha+\beta-1)(1-x)+0.5}{\left[x(\alpha+\beta - 1)(1-x)\right]^{1/2}},$$

donde $[\beta]$ denota el entero más grande que no excede a $\beta$.


\section{La distribución gama}

Supóngase que una pieza metálica se encuentra sometida a cierta fuerza, de manera que se romperá después de aplicar un número específico de ciclos de fuerza. Si los ciclos ocurren de manera independiente y a una frecuencia promedio, entonces el tiempo que debe transcurrir antes de que el material se rompa es una variable aleatoria que cumple con la distribución gama.

\begin{tcolorbox}
    \begin{def.}
	Se dice que la variable aleatoria $X$ tiene una distribución gama si su función de densidad de probabilidad está dada por:
	$$f(x;\alpha,\theta)=\left\{\begin{array}{ll}
		\dfrac{1}{\Gamma(\alpha)\theta^\alpha}x^{\alpha-1}e^{-x/\theta} & x>0,\; \alpha,\theta>0\\\\
		0 & \mbox{para cualquier otro resultado.}
	\end{array}\right.$$
	en donde $\Gamma(\alpha)$ es la función gama.\\\\
    \end{def.}
\end{tcolorbox}

Demostraremos que la función dada es función de densidad de probabilidad. Para hacerlo, considére un cambio de variable de integración, tal que $u=x/\theta$, $x=\theta u$ y $dx=\theta du$; entonces:
$$\dfrac{1}{\Gamma(\alpha)\theta^\alpha}\int_0^\infty x^{\alpha-1}e^{-x/\theta}\; dx = \dfrac{1}{\Gamma(\alpha)\theta^\alpha} \int_0^\infty \left(\theta u\right)^{\alpha-1} e^{-u}\theta\; du = \dfrac{1}{\Gamma(\alpha)}\int_0^\infty u^{\alpha -1 }e^{-u}\; du = 1.$$
Dado que $\Gamma(\alpha)=\displaystyle\int_0^{\infty} u^{\alpha -1 }e^{-u}\; du.$\\\\

Con este procedimiento similar se demuestra que el r-ésimo momento alrededor del cero es:

$$\mu_r' = \dfrac{1}{\Gamma(\alpha)\theta^\alpha}\int_0^\infty x^{\alpha+r-1}e^{-x/\theta}\; dx = \dfrac{\theta^{\alpha+r}}{\Gamma(\alpha)\theta^{\alpha}}\int_0^\infty u^{\alpha+r-1}e^{-u}\; du = \dfrac{\theta^r\Gamma(\alpha+r)}{\Gamma(\alpha)}$$

Se sigue, por tanto que:

\begin{tcolorbox}
    $$E(X)=\alpha\theta.$$
\end{tcolorbox}

\begin{tcolorbox}
    $$Var(X)=\alpha \theta^2.$$
\end{tcolorbox}

Ademas el \textbf{coeficiente de asimetría} es:
\begin{tcolorbox}
    $$\alpha_3(X)=2\sqrt{\alpha}.$$
\end{tcolorbox}

Y la \textbf{curtosis relativa} está dada por:
\begin{tcolorbox}
    $$\alpha_4(X)=3\left(1+\dfrac{2}{\alpha}\right).$$
\end{tcolorbox}

Para valores grandes de $\alpha$ la distribución gama puede aproximarse, en algún grado, por una distribución normal. Esto es, la variable aleatoria
$$Z=\dfrac{X-\alpha\theta}{\theta\sqrt{\alpha}}$$
es, de manera aproximada, igual a la normal estándar para valores grandes de $\alpha$.\\\\

La \textbf{función generadora de momentos} para la variable aleatoria gama $X$ está dada por:

$$E[e^{tX}]=\dfrac{1}{\Gamma(\alpha)\theta^{\alpha}}\int_{0}^x x^{\alpha-1}e^{-\frac{1-\theta t}{\theta}}\; dx.$$

Sea $u=\frac{(1-\theta t)x}{\theta}$, $x=\dfrac{u\theta}{1-\theta t}$ y $dx=\left[\dfrac{\theta}{1-\theta t}\right]\; du$, entonces:

$$\begin{array}{rcl}
    E\left[e^{tX}\right] &=& \dfrac{1}{\Gamma(\alpha)\theta^{\theta}}\displaystyle\int_0^\infty \dfrac{u^{\alpha-1}\theta^{\alpha-1}}{(1-\theta t)^{\alpha-1}}e^{-u}\dfrac{\theta}{1-\theta t}\; du\\\\
			 &=&\dfrac{1}{\Gamma(\alpha)(1-\theta t)^{\alpha}}\displaystyle\int_{0}^{\infty}u^{\alpha-1}e^{-y}\; du\\\\
			 &=&(1-\theta t)^{-\alpha},\quad 0\leq t < \dfrac{1}{\theta}.\\\\
\end{array}$$

La \textbf{función de distribución acumulativa} se determina por la siguiente expresión:

\begin{tcolorbox}
    $$F(x;\alpha,\theta) = \dfrac{1}{\Gamma(\alpha)\theta^{\alpha}} \int_0^x t^{\alpha-1}e^{-\frac{t}{\theta}} \; dt,\quad x>0.$$
\end{tcolorbox}

Si se efectua el cambio de variable $u=\dfrac{t}{\theta}$ de manera tal que $t=\theta u$ y $dt=\theta du$, entonces la función de distribución acumulativa toma la siguiente forma:

\begin{tcolorbox}
    $$F(x;\alpha,\theta)=\dfrac{1}{\Gamma(\alpha)\theta^{\alpha}} \int_0^{x/\theta}(\theta u)^{\alpha-1}e^{-u}\; du=\dfrac{1}{\Gamma(\alpha)}\int_0^{x/\theta}u^{\alpha-1}e^{-u}\; du.$$
\end{tcolorbox}

La integral $\int_0^{x/\theta}u^{\alpha-1}e^{-u}\; du$ se conoce como la función gama incompleta y se denota generalmente por $\gamma(x/\theta;\alpha)$. De acuerdo con lo anterior la función gama de distribución acumulativa se escribe como:

\begin{tcolorbox}
    $$P(X\leq x)=F(x;\alpha,\theta)=\dfrac{\gamma(x/\theta;\alpha)}{\Gamma(\alpha)}$$
\end{tcolorbox}

Cuando el parámetro de forma $\alpha$ es igual a $1$, la distribución de Erlang (gama) se reduce a lo que se conoce como la distribución exponencial negativa.\\\\

También se tiene la equivalencia de 
$$I(u,p)=F(x;\alpha,\theta)$$
donde $u=\dfrac{u}{\theta\sqrt{\alpha}}$ y $p=\alpha-1$. Debe notarse que si el parámetro de forma $\alpha$ es un entero positivo, $I(u,p)=F(x;\alpha,\theta)$ se puede expresar, en forma cerrada:

$$F(x;\alpha,\theta)=1-\left[1+\dfrac{x}{\theta}+\dfrac{1}{2!}\left(\dfrac{x}{\theta}\right)^2+\ldots + \dfrac{1}{(\alpha-1)!}\left(\dfrac{x}{\theta}\right)^{\alpha-1}\right]e^{-x(\theta}$$

Cuando $\alpha$ es entero positivo, la distribución gama también se conoce como \textbf{distribución de Erlang}. Existe una asociación entre los modelos de probabilidad de Poisson y Erlang. Esto es, la probabilidad de que ocurra a lo más $\alpha-1$ eventos de Poisson en un tiempo $x$ a una frecuencia constante $1/\theta$ se desprende de $P(X\leq x)=F(x;\lambda)=\sum\limits_{i=0}^x \dfrac{e^{-\lambda}\lambda^i}{i!}$ y está dada por:

$$F_P(\alpha-1;x/\theta)=\left[1+\dfrac{x}{\theta}+\dfrac{1}{2!}\left(\dfrac{x}{\theta}\right)^2+\ldots + \dfrac{1}{(\alpha-1)!}\left(\dfrac{x}{\theta}\right)^{\alpha-1}\right]e^{-x/\theta}.$$

Por otro lado, si se supone que el tiempo de espera sigue el modelo de Erland, la probabilidad de que el tiempo de espera hasta que ocurra el $\alpha$-ésimo evento exceda un lapso $x$ especifico, está determinado por:

$$\begin{array}{rcl}
    P(X>x)&=&1-F_P(x;\alpha,\theta)\\\\
	  &=&1-\left\{1-\left[1+\dfrac{x}{\theta}+\dfrac{1}{2!}\left(\dfrac{x}{\theta}\right)^2+\ldots + \dfrac{1}{(\alpha-1)!}\left(\dfrac{x}{\theta}\right)^{\alpha-1}\right]e^{-x/\theta}\right\}\\\\
	  &=&\left[1+\dfrac{x}{\theta}+\dfrac{1}{2!}\left(\dfrac{x}{\theta}\right)^2+\ldots + \dfrac{1}{(\alpha-1)!}\left(\dfrac{x}{\theta}\right)^{\alpha-1}\right]e^{-x/\theta}\\\\
	  &=&F_P(\alpha-1;x/\theta)\\\\
\end{array}$$

Cuando el parámetro de forma $\alpha$ es igual a uno, la distribución de Erlang (gama) se reduce a lo que se conoce como la \textbf{distribución exponencial negativa}. Esta distribución se emplea de manera extensa para representar lapsos aleatorios de tiempo. Nótese que la variable aleatoria de una distribución exponencial negativa puede pensarse como el lapso que transcurre hasta el primer evento de Poisson. De acuerdo con lo anterior, la variable de Erlang es la suma de variables aleatorias independientes distribuidas exponencialmente.\\\\

Otro caso especial del modelo de probabilidad gama es la distribución chi-cuadrado. Si se reemplazo en en 
	$$f(x;\alpha,\theta)=\left\{\begin{array}{ll}
		\dfrac{1}{\Gamma(\alpha)\theta^\alpha}x^{\alpha-1}e^{-x/\theta} & x>0,\; \alpha,\theta>0\\\\
		0 & \mbox{para cualquier otro resultado.}
	\end{array}\right.$$

el parámetro de forma $\alpha$ con $v/2$ y el parámetro de escala $\theta$ con $2$, el resultado es la \textbf{función de densidad de probabilidad de una variable aleatoria chi-cuadrado} y se determina por:
\begin{tcolorbox}
	$$f(x;\alpha,\theta)=\left\{\begin{array}{ll}
		\dfrac{1}{\Gamma(v/2)2^{v/2}}x^{v/2-1}e^{-x/2} & x>0,\; \alpha,\theta>0\\\\
		0 & \mbox{para cualquier otro resultado.}
	\end{array}\right.$$
	La distribución chi-cuadrado se encuentra caracterizada por un solo parámetro $v$, que recibe el nombre de grados de libertad. Su notación es:
	$$X\sim \chi^2_v$$
\end{tcolorbox}

La \textbf{función de distribución acumulativa} está dada por:

\begin{tcolorbox}
    $$P(X\leq x)\dfrac{1}{\Gamma(v/2)2^{v/2}}\int_0^x t^{v/2-1}e^{-t/2}\; dt, \quad x>0.$$
\end{tcolorbox}

Los \textbf{cuantiles} $x_{1-\alpha,v}$ es

\begin{tcolorbox}
    $$P(X\leq x_{1-\alpha,v})=\int_0^{x_{1-\alpha,v}}f(x;v)\; dx=1-\alpha.$$
\end{tcolorbox}

Los momentos de distribución chi-cuadrado se obtienen a partir de $E(X)=\alpha \theta$, $Var(X)=\alpha\theta^2$, $\alpha_3(X)=2/\sqrt{\alpha}$ y $\alpha_4(X)=3\left(1+2/\alpha\right)$, dado por:

\begin{tcolorbox}
    $$E(X)=v$$
    $$Var(X)=2v$$
    $$\alpha_3(X)=\dfrac{4}{\sqrt{2v}}$$
    $$\alpha_4(X)=3\left(1+\dfrac{4}{v}\right)$$
\end{tcolorbox}

Análogamente y a partir de $E(e^{tX})=(1-\theta t)^{-\alpha}$ la \textbf{función generadora de momentos} para la distribución chi-cuadrado es:

\begin{tcolorbox}
    $$m_X(t)=(1-2t)^{-v/2},\qquad 0\leq t<\dfrac{1}{2}.$$
\end{tcolorbox}

Su varianza es dos veces el valor de su media. Presenta un sesgo positivo y un pico mayor que el de una distribución normal, pero el coeficiente de asimetria tiende a cero y a una curtosis relativa igual a tres conforme a $v$ tiende al infinito.

\section{La distribución de Weibull}




