\chapter{Algunas distribuciones continuas de probabilidad}

\section{La distribución normal}

\begin{tcolorbox}
    \begin{def.}
	se dice que una variable aleatoria $X$ se encuentra normalmente distribuida si su función de densidad de probabilidad está dada por 
	$$f(x;\mu,\sigma) = \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2},\quad \begin{array}{l}-\infty<x<\infty \\ -\infty<\mu<\infty,\; \sigma>0\end{array}$$
    \end{def.}
\end{tcolorbox}

Si se obtienen las dos primeras derivadas de $f(x;\mu,\sigma)$ con respecto a $x$ y se igualan a cero, se tiene que el valor máximo de $f(x;\mu,\sigma)$ ocurre cuando $x=\mu,$ y los valores $x=\mu\pm \sigma$ son las abcisas de los dos puntos de inflexión de la curva.\\\\

\textbf{Demostrar que la definición 5.1 es una función de densidad de probabilidad.}\\\\
    Demostración.-\; El que la función sea no negativa se satisface, ya que $f(x;\mu,\sigma)>0$ para $-\infty<x<\infty,\; -\infty<\mu<\infty$ y $\sigma>0$. Para demostrar que:
    $$\int_{-\infty}^\infty f(x;\mu,\sigma)\; dx = 1.$$
    Sea
    $$I=\dfrac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^\infty e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx$$
    el valor de la integral y aplíquese la transformación lineal $y=(x-\mu)/\sigma$ de manera tal que $x=\sigma y + \mu$ y $dx = \sigma \; dy$. Esto da como resultado:
    $$I=\dfrac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-y^2/2}\; dy.$$
    Si puede demostrarse que $I^2=1$, puede deducirse que $I=1$ puesto que $f(x;\mu,\sigma)$ tiene un valor positivo. De acuerdo con lo anterior:
    $$I^2 = \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty}e^{-y^2/2}\; dy \cdot \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty}e^{-z^2/2}\; dz = \dfrac{1}{\sqrt{2\pi}} \int_{\infty}^\infty \int_{-\infty}^\infty e^{\frac{(y^2+z^2)}{2}}\; dy dz,$$
    en donde se ha escrito el producto de las dos integrales como una doble integral ya que las funciones de $z$ son contantes con respecto a $y$ como también de manera viceversa. Al cambiar de coordenadas rectangulares representadas por $x$ e $y$, a coordenadas polares $r$ y $\theta$, en donde $y=r\cos \theta$ y $z=r\sen \theta$. Esto es:
    $$y^2+z^2 = r^2\cos^2\theta + r^2\sen^2 \theta = r^2$$
    y el elemento de área $dy dz$, en coordenadas rectangulares se reemplaza por $rd rd\theta$ en coordenadas polares. Dado que los límites $(-\infty,\infty)$ tanto para $y$ como para $z$ generan el plano completo $yz$, el plano correspondiente a $r$ y a $\theta$ se genera mediante el empleo de los límites $(0,2\pi)$ para $\theta$ y $(0,\infty)$ para $r$. De esta forma se tiene:
    $$I^2 = \dfrac{1}{2\pi}\int_0^{2\pi} \int_0^x e^{-r^2/2}\; rdrd\theta = \dfrac{1}{2\pi}\int_0^{2\pi}\;d\theta \int_0^x e^{-r^2/2}\; rdr = \dfrac{\theta}{2\pi}\bigg|_0^{2\pi}\cdot [-e^{-r^2/2}]\bigg|_0^x = 1.$$\\

    \textbf{La media de una variable aleatoria distribuida normalmente} se encuentra definida por:
\begin{tcolorbox}
    $$E(X)=\dfrac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^x x e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx.$$
\end{tcolorbox}

Se pretende demostrar que $E(X)=\mu$. Supóngase que a $E(X)=\dfrac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^x x e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx$ se suma y se resta
$$\dfrac{\mu}{\sqrt{2\pi}\sigma}\int_{-\infty}^\infty e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}\; dx$$

La identidad se mantiene, pero después de reacomodar términos se tiene
$$\begin{array}{rcl}
    E(X)&=&\displaystyle\int_{-\infty}^\infty (x-\mu)e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx + \dfrac{\mu}{\sqrt{2\pi}\sigma}\int_{-\infty}^\infty e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx\\\\
	&=&\displaystyle\dfrac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^x (x-\mu)e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx + \mu\\\\
\end{array}$$

dado que el valor de la segunda integral es uno. Al afectar un cambio de variable de integración de manera tal que $y=\dfrac{x-\mu}{\sigma},\; x=\sigma y+\mu$ y $dx=\sigma dy$, se tiene:

\begin{tcolorbox}
$$E(X)=\dfrac{\sigma}{\sqrt{2\pi}}\int_{-\infty}^\infty y e^{-y^2/2}\; dy + \mu = -\dfrac{\sigma}{\sqrt{2\pi}}e^{-y^2/2}\bigg|_{-\infty}^\infty + \mu = \mu.$$
\end{tcolorbox}

El lector recordará de sus cursos de cálculo que la última integral es cero porque el integrando es una función impar y la integración se lleva a cabo sobre un intervalo simétrico alrededor de cero.\\\\

Si el valor máximo de la función de densidad de probabilidad normal ocurre cuando $x=\mu$ este es la media, la mediana y la moda de cualquier variable aleatoria distribuida aleatoriamente.\\
Para encontrar los demás momentos, se determinará la función generadora de momentos. Por definición:
$$m_{X-\mu}(t) = E\left[e^{t(X-\mu)}\right] = \dfrac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty e^{t(x-\mu)} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx = \dfrac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty e^{-\frac{1}{2\sigma^2}\left[(x-\mu)^2-2\sigma^2 t(x-\mu)\right]}\; dx.$$
de donde se completa el cuadrado en el interior del paréntesis rectangular y se tiene:
$$(x-\mu)^2-2\sigma^2 t(x-\mu) = (x-\mu)^2-2\sigma^2 t(x-\mu)+\sigma^4 t^2 - \sigma^4 t^2 = (x-\mu-\sigma^2t)^2 - \sigma^4 t^2.$$
Por lo que,
$$m_{X-\mu}(t) = \dfrac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^\infty e^{\frac{\sigma^2 t^2}{2}}e^{-\frac{x-(\mu+\sigma^2 t)^2}{2\sigma^2}}\; dx = e^{\frac{\sigma^2t^2}{2}},$$
dado que el integrando junto con el factor $\dfrac{1}{\sqrt{2\pi}\sigma}$ es una función de densidad de probabilidad normal con parámetros $\mu+\sigma^2 t$ y $\sigma.$

Al desarrollar $e^{\frac{\sigma^2 t^2}{2}}$ en serie de potencias se tiene:
$$m_{X-\mu}(t) = 1+\dfrac{(\sigma t)^2}{2}+\dfrac{(\sigma t)^4}{4\cdot 2!}+\dfrac{(\sigma t)^6}{8\cdot 3!}+\dfrac{(\sigma t)^8}{16\cdot 4!} + \ldots$$
Cuando las potencias impares de $t$ no se encuentran presentes, todos los momentos centrales de $X$ de orden impar son cero, de esta forma se asegura la simetría de la curva.\\\\

La segunda derivada de $m_{X-\mu}(t)$  evaluada en $t=0$ es \textbf{la varianza} y está dada por:
\begin{tcolorbox}
    $$Var(X)=\dfrac{d^2m_{X-\mu}(t)}{dt^2}\bigg|_{t=0}=\sigma^2 + \dfrac{12t^2 \sigma^4}{4\cdot 2!}+\dfrac{30t^4\sigma^6}{8\cdot 3!} + \ldots \bigg|_{t=0} = \sigma^2;$$
\end{tcolorbox}

De esta manera \textbf{ la desviación estándar es  $\mathbold \sigma$}. De manera similar, la cuarta derivada de $m_{X-\mu}(t)$ evaluada en $t=0$ es el cuarto momento central, el cual es:
$$\mu_4=\dfrac{d^4m_{X-\mu}(t)}{dt^4}\bigg|_{t=0}=3\sigma^4 + \dfrac{360t^2\sigma^6}{8\cdot 3!}+ \ldots \bigg|_{t=0}=3\sigma^4$$
De acuerdo con lo anterior, para cualquier distribución normal el coeficiente de asimetría es $\alpha_3(X)=0,$ mientras que la curtosis relativa es $\alpha_4(X)=\dfrac{3\sigma^4}{\sigma^4} = 3.$ Para momentos alrededor del cero, puede determinarse la función generadora de momentos centrales o viceversa. Dado que
$$m_{X-\mu}(t)=E\left[e^{t(X-\mu)}\right]=e^{-\mu t}E\left[e^{tX}\right] = e^{-ut}m_X(t),$$
para una distribución normal
$$e^{-\mu t}m_X (t) = e^{\frac{\sigma^2t^2}{2}}$$
y
$$m_X (t)=e^{\mu t + \frac{\sigma^2t^2}{2}}.$$

La probabilidad de que una variable aleatoria normalmente distribuida $X$ sea menor o igual a un valor específico, $x$ está dada por \textbf{función de distribución acumulativa}
\begin{tcolorbox}
    $$P(X\leq x)=F(x;\mu,\sigma) = \dfrac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{x} e^{\frac{-(t-\mu)^2}{2\sigma^2}}\; dt.$$
\end{tcolorbox}

Sea $Z$ una variable aleatoria definida por la siguiente relación:

$$Z=\dfrac{(X-\mu)}{\sigma}$$

en donde $\mu$ y $\sigma$ son la media y la desviación estándar de $X$, respectivamente. De acuerdo con lo anterior, $Z$ es una variable aleatoria estandarizada con media cero y desviación estándar uno. Así,
\begin{tcolorbox}
    $$P(X\leq x) = P[X\leq (x-\mu)/\sigma] = \dfrac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{(x-\mu)/\sigma} e^{\frac{-z^2}{2}}\sigma \; dz = \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^{(x-\mu)/\sigma} e^{\frac{-z^2}{2}}\; dz.$$
\end{tcolorbox}
El integrando junto con el factor $1/\sqrt{2\pi}$ es la \textbf{función de densidad de probabilidad de la variable aleatoria normal estandarizada Z}. De donde 
$$F_X(x;\mu,\sigma) = F_Z(z;0,1)$$

Para cualquier valor específico de $z$, el correspondiente valor en la tabla es la probabilidad de que la variable aleatoria normal estándar $Z$ sea menor o igual a $z$; esto es
\begin{tcolorbox}
    $$P(Z\leq z)=F_Z(z;0,1) = \dfrac{1}{\sqrt{2\pi}}\int_{-\infty}^z e^{\frac{-t^2}{2}}\; dt.$$
\end{tcolorbox}

La notación $X\sim N(\mu,\sigma)$ denotará que la variable $X$ se encuentra distribuida normalmente con media $\mu$ y desviación estándar $\sigma$.\\\\
Determinaremos la probabilidad de que un valor de $X$ se encuentre entre $a$ y $b$, si $X\sim N(\mu,\sigma)$. Por definición:
$$P(a\leq X\leq b) = \dfrac{1}{\sqrt{2\pi}\sigma} \int_a^b e^{\frac{-(x-\mu)^2}{2\sigma^2}}\; dx,$$

pero, mediante el empleo de $E(X)=\displaystyle\dfrac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^x (x-\mu)e^{-\frac{(x-\mu)^2}{2\sigma^2}}\; dx + \mu$ se tiene:

\begin{tcolorbox}
    $$P(a\leq X\leq b) = P\left(\dfrac{a-\mu}{\sigma}\leq Z \leq \dfrac{b-\mu}{\sigma}\right) = \dfrac{1}{\sqrt{2\pi}}\int_{\frac{a-\mu}{\sigma}}^{\frac{b-\mu}{\sigma}} e^{\frac{-z^2}{2}}\; dz = F_Z\left(\dfrac{b-\mu}{\sigma};0,1\right)-F_Z\left(\dfrac{a-\mu}{\sigma};0.1\right)$$
\end{tcolorbox}

\begin{teo}
    Sea $X$ una variable aleatoria binomial con media $np$ y desviación estándar $\sqrt{np(1-p)}$. La distribución de la variable aleatoria tiende a la normal
    $$Y=\dfrac{X-np}{\sqrt{np(1-p)}}$$\\
    estándar conforme el número de ensayos independientes $n\to \infty$.\\\\
	Demostración.-\; La demostración que aquí se presenta se basa en el hecho de que una función generadora de momentos define, de manera única, a una distribución. Se demostrará que la función generadora de momentos de $Y$ tiende a una distribución normal conforme $n\to \infty$. $X$ es una variable aleatoria binomial:
	$$m_X(t)=[(1-p)+p e^t]^n$$
	Entonces:
	$$m_Y(t)=E(e^{tY})=E\left[e^{\frac{t(X-np)}{\sqrt{np(1-p)}}}\right] = e^{\frac{npt}{\sqrt{np(1-p)}}}\cdot E\left[e^{\frac{tX}{\sqrt{np(1-p)}}}\right]$$
	donde $E\left[e^{\frac{tX}{\sqrt{np(1-p)}}}\right]$ es la función generadora de momentos de $X$ con argumento $\frac{t}{\sqrt{np(1-p)}}$. De esta forma se tiene:
	$$m_y(t)=e^{\frac{-npt}{\sqrt{np(1-p)}}}\left[(1-p)+ pe^{\frac{t}{\sqrt{np(1-p)}}}\right]^n;$$
	pero:
	$$e^{-\frac{npt}{\sqrt{np(1-p)}}} = \left(e^{-\frac{pt}{\sqrt{np(1-p)}}}\right)^n$$
	y:
	$$m_y(t)=\left[(1-p)e^{-\frac{pt}{\sqrt{np(1-p)}}}+pe^{\frac{t}{\sqrt{np(1-p)}}-\frac{pt}{\sqrt{np(1-p)}}}\right]^n = \left[(1-p)e^{-\frac{pt}{\sqrt{np(1-p)}}}+pe^{\frac{(1-p)t}{\sqrt{np(1-p)}}}\right]^n.$$
	En la última expresión, al expander ambas funciones exponenciales en una serie de potencias, se tiene:
	$$\begin{array}{rcl}
	    (1-p)e^{-\frac{pt}{\sqrt{np(1-p)}}} &=& (1-p) - \dfrac{(1-p)pt}{\sqrt{np(1-p)}} + \dfrac{(1-p)p^2t^2}{2np(1-p)} + \mbox{términos en } (-1)^k \left(\dfrac{1}{n}\right)^{k/2}, \; k=3,4,\ldots\\\\
						&=&(1-p)-\dfrac{(1-p)pt}{\sqrt{np(1-p)}}+\dfrac{pt^2}{2n}+\mbox{términos en } (-1)^k\left(\dfrac{1}{n}\right)^{k/2},\; k=3,4,\ldots\\\\
	\end{array}$$
	y
	$$\begin{array}{rcl}
	    pe^{-\frac{(1-p)t}{\sqrt{np(1-p)}}} &=& p + \dfrac{(1-p)pt}{\sqrt{np(1-p)}} + \dfrac{(1-p)pt^2}{2np(1-p)} + \mbox{términos en } \left(\dfrac{1}{n}\right)^{k/2}, \; k=3,4,\ldots\\\\
						&=&p+\dfrac{(1-p)pt}{\sqrt{np(1-p)}}+\dfrac{(1-p)t^2}{2n}+\mbox{términos en } \left(\dfrac{1}{n}\right)^{k/2},\; k=3,4,\ldots\\\\
	\end{array}$$

	Al sustituir los resultados anteriores en $m_Y(t)$ y agrupar términos,

	$$m_Y(t)=\left[1+\dfrac{t^2}{2n}+\mbox{ términos en }\left(\dfrac{1}{n}\right)^{k/2}\right]^n,\; k=3,4,\ldots$$
	Dado que todos los términos que contiene a $(1/n)^{k/2},\; k=3,4,\ldots$, tienen exponentes mayores que uno, puede factorizarse el término $1/n$. De esta forma se tiene que:

	$$m_Y(t)=\left[1+\dfrac{1}{n} \left(\dfrac{t^2}{2}+\mbox{ términos en }\left(\dfrac{1}{n}\right)^{(k-2)/2}\right)\right]^n,\; k=3,4,\ldots$$

	Por definición:
	$$\lim_{n\to \infty}\left(1+\dfrac{u}{n}\right)^n=e^u$$
	entonces, conforme $n\to \infty$, la última expresión para $m_Y(t)$ es idéntica a esta forma, con $u$ representando a todo lo que se encuentra entre paréntesis de esta expresión. Pero conforme $n\to \infty$, todos los términos de $u$, excepto el primero, tienen un valor de cero, dado que todos tienen potencias positivas de $n$ en sus denominadores. De acuerdo con lo anterior.
	$$\lim_{n\to \infty}m_Y(t)=e^{t^2/2},$$
	que es la función generadora de momentos de la distribución normal estándar.\\\\

\end{teo}

La aproximación del teorema anterior es adecuada tanto como $np>5$ cuando $p\leq 1/2$, o cuando $n(1-p)>5$ para $p>1/2$. Esto es
$$P(a\leq X_B \leq b) = P\left(\dfrac{a-np}{\sqrt{np(1-p)}}\leq Z_N\leq \dfrac{b-np}{\sqrt{np(1-p)}}\right)$$
en donde $Z_N$ es $N(0,1)$. Como también
$$P(X_B=x)\approx P\left(\dfrac{x-np-1/2}{\sqrt{no(1-p)}}\leq Z_N\leq \dfrac{x-np+1/2}{\sqrt{np(1-p)}}\right)$$
por lo que se puede modificar la expresión de desigualdad de la siguiente manera:
$$P(a\leq X_B \leq b) = P\left(\dfrac{a-np-0.5}{\sqrt{np(1-p)}}\leq Z_N\leq \dfrac{b-np+0.5}{\sqrt{np(1-p)}}\right)$$


\section{La distribución uniforme}

\begin{tcolorbox}
    \begin{def.}
	Se dice que una variable aleatoria $X$ está distribuida uniformemente sobre el intervalo $(a,b)$ si su función de densidad de probabilidad está dada por:
	$$f(x;a,b) = \left\{ \begin{array}{ll}
		\dfrac{1}{b-a} & a\leq x\leq b \\\\
		0 & \mbox{para cualquier otro valor.} \\
	\end{array} \right.$$
    \end{def.}
\end{tcolorbox}

\textbf{La función de distribución acumulativa} se determina de manera fácil y está dada por
\begin{tcolorbox}
    $$P(X\leq x)=F_{x;a,b}=\dfrac{1}{b-a}\int_a^x dt = \left\{\begin{array}{ll}
	    0 & x<a,\\
	    \dfrac{x-a}{b-a} & a\leq x\leq b,\\
	    1 & x>b.
	\end{array}\right.$$
\end{tcolorbox}

Se sigue entonces que para cualquier subintervalo $(a_1,b_1)$ interior a $(a,b)$:
$$P(a_1\leq X\leq b_1)=F(b_1;a,b)-F(a_1;a,b)=\dfrac{b_1-a_1}{b-a}.$$\\

El \textbf{valor esperado} de una variable aleatoria distribuida de manera uniforme es 

\begin{tcolorbox}
    $$E(X)=\dfrac{1}{b-a}\int_a^b x\;dx = \dfrac{a+b}{2}.$$
\end{tcolorbox}


