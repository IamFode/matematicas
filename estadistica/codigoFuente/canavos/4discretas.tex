\chapter{Algunas distribuciones discretas de probabilidad}


\setcounter{section}{1}
\section{La distribución binomial}
Llámese éxito a la ocurrencia del evento y fracaso a su no ocurrencia.\\\\
Las dos suposiciones claves para la distribución binomial son:
\begin{enumerate}
    \item La probabilidad de éxito $p$ permanece constante para cada ensayo.
    \item Los $n$ ensayos son independientes entre sí.
\end{enumerate}

Para obtener la función de probabilidad de la distribución binomial, primero se determina la probabilidad de tener, en $n$ ensayos, $x$ éxitos consecutivos seguidos de $n$-$x$ fracasos consecutivos. Dado que, por hipótesis, los $n$ ensayos son independientes de la definición 2.15, se tiene:
$$p\cdot p\cdots p \cdot (1-p)\cdot (1-p)\cdots (1-p) = p^x(1-p)^{n-x}$$

%-------------------- Definición 4.1.
\begin{tcolorbox}
    \begin{def.}[Distribución binomial con función de probabilidad]
	Sea $X$ una variable aleatoria que representa el número de éxitos en $n$ ensayos y $p$ la probabilidad de éxito con cualquiera de éstos. Se dice entonces que $X$ tiene una distribución binomial con función de probabilidad.
	$$p(x;n,p)\left\{\begin{array}{ll}
	    \dfrac{n!}{(n-x)!x!} p^x(1-p)^{n-x} & x = 0,1,2,\ldots,n.\\\\
	    0\; \mbox{para cualquier otro valor.} & 0\leq p \leq 1. \; \mbox{para } n \;\mbox{entero}.\\
	\end{array}\right.$$
     \end{def.}
\end{tcolorbox}

El nombre distribución binomial proviene del hecho de que los valores de $p(x;n,p)$ para $x=1,2,\ldots,n$ son los términos sucesivos de la expansión binomial de $[(1-p)+p]^n;$ esto es,
$$\begin{array}{rcl}
    [(1-p)+p]^n & = & (1-p)^n + n(1-p)^{n-1} p + \dfrac{n(n-1)}{2!} (1-p)^{n-2}p^2 + \cdots + p^n\\\\
		& = & \sum\limits_{x=0}^n \dfrac{n!}{(x-x)!x!} p^x(1-p)^{n-x}\\\\
		& = & \sum\limits_{x=0}^n p(x;n,p)\\\\
\end{array}$$

Pero dado que $[(1-p)+p]^n = 1$ y $p(x;n,p)\geq 0$ para $x=0,1,2,\ldots n,$ este hecho también verifica que $p(x;n,p)$ es una función de probabilidad.\\\\

La probabilidad de que una variable aleatoria $X$ sea menor o igual a un valor específico de $x$, se determina por la \textbf{función de distribución acumulativa}.

% ---------------------- función de distribución acumulativa
\begin{tcolorbox}
    $$P(X\leq x) = F(x;n,p) = \sum_{i=0}^x {n\choose i} p^i (1-p)^{n-i}$$
\end{tcolorbox}

Nótese que si $n=1$, la función de probabilidad binomial se deduce a

\begin{tcolorbox}
    $$p(x;n,p)=\left\{\begin{array}{ll}
	p^x(1-p)^{1-x} & x = 0,1\\\\
	0 & \mbox{para cualquier otro valor}.
    \end{array}\right.$$
\end{tcolorbox}

Que es la \textbf{función de probabilidad de la distribución puntual o Bernoulli.}\\\\

Por definición 3.8, el primer momento alrededor del cero de la variable aleatoria binomial $X$ es el valor esperado de $X$.
$$\begin{array}{rcl}
    E(X) & = & x\sum\limits_{x=0}^n \dfrac{n!}{(n-x)!x!} p^x(1-p)^{n-x}\\\\
	 & = & x\sum\limits_{x=1}^n \dfrac{n!}{(n-x)!x!} p^x(1-p)^{n-x}\\\\
	 & = & \sum\limits_{x=1}^n \dfrac{n!}{(n-x)!(x-1)!} p^x(1-p)^{n-x}\\\\
\end{array}$$

en donde se ha escrito la suma desde uno hasta $n$, dado que cuando $x=0$ el primer término es cero y se cancela la $x$ del numerador con la $x$ en $x!$. Factorizando $n$ y $p$, se tiene: 
$$E(X)=np\sum_{x=1}^n \dfrac{(n-1)!}{(n-x)!(x-1)!}p^{x-1}(1-p)^{n-x}$$

Si $y=x-1$ y $m=n-1$, entonces:
$$E(X)=np\sum_{x=1}^n \dfrac{m!}{(m-y)!y!}p^{y}(1-p)^{n-y}$$

De donde se sabe que $p(y; m,p) = \dfrac{m!}{(m-y)!y!}p^{y}(1-p)^{n-y}$   es la función de probabilidad de una variable aleatoria binomial $Y$ con parámetros $m=n-1$ y $p$; de ésta manera $\sum_{y=0}^m p(y;m,p) = 1$ y \textbf{la media de una variable  aleatoria binomial es}:

\begin{tcolorbox}
    $$E(X)=\mu = np.$$
\end{tcolorbox}

Para obtener la varianza, se necesita el segundo momento alrededor de cero, $\mu_2^{'}$, o:
$$E(X^2) = \sum\limits_{x=0}^n x^2 p(x;n,p)$$
pero, e el término $x^2/x!$ se cancelará una sola $x$ en el numerador, y la que resta evitará que la suma se manipule de la misma forma en que se determinó la media. La alternativa es escribir $x^2$ como:
$$x^2= x(x-1)+x;$$
de esta manera se tiene:
$$E(X^2) = E[X(X-1)]+E(X).$$
Dado que $E(X)$ ya se ha determinado, puede usarse el mismo procedimiento para evaluar $E(X(X-1))$:
$$\begin{array}{rcl}
    E[X(X-1)]&=&\sum\limits_{x=0}^n x(x-1)\dfrac{n!}{(n-x)!x!}p^x (1-p)^{n-x}\\\\
	     &=&\sum\limits_{x=2}^n x(x-1)\dfrac{n!}{(n-x)!x!}p^x(1-p)^{n-x}\\\\
	     &=&\sum\limits_{x=2}^n \dfrac{n!}{(n-x)(x-2)!}p^x(1-p)^{n-x}\\\\
	     &=&n(n-1)p^2\sum\limits_{x=2}^n \dfrac{(n-2)!}{(n-x)!(x-2)!}p^{x-2}(1-p)^{n-x}\\\\
\end{array}$$

Sea $y=x-2$ y $m=n-2$, entonces:
$$\begin{array}{rcl}
    E[X(X-1)]&=&n(n-1)p^2\sum\limits_{y=0}^m \dfrac{m!}{(m-y)y!}p^y(1-p)^{m-y}\\\\
	     &=&n(n-1)p^2\sum\limits_{y=0}^m p(y;m,p)\\\\
	     &=&n(n-1)p^2\\\\
\end{array}$$

Así,
$$E(X^2) = \mu_2^{'} = n(n-1)p^2+np$$
De esta manera, la \textbf{varianza de una variable aleatoria binomial} es:

\begin{tcolorbox}
    $$Var(X) = \mu_2^{'}-\mu^2 = n(n-1)p^2 + np - n^2p^2 = np[(n-1)p+1-np] = np(1-p).$$
\end{tcolorbox}

\vspace{1cm}

Para obtener el tercer momento alrededor del cero, se determina $E[X(X-1)(X-2)]$ dado que:
$$E[X(X-1)(X-2)]=\mu_3^{'} - 3\mu_2^{'} + 2\mu$$

$$\begin{array}{rcl}
    E[X(X-1)(X-2)]&=&\sum\limits_{x=0}^n x(x-1)(x-2)\dfrac{n!}{(n-x)!x!} p^x(1-p)^{n-x}\\\\
		  &=&\sum\limits_{x=3}^n \dfrac{n!}{(n-x)!(x-3)!}p^x(1-p)^{n-x}\\\\
		  &=&n(n-1)(n-2)p^3 \sum_{x=3}^n \dfrac{(n-3)!}{(n-x)!(x-3)!}p^{x-3}(1-p)^{n-x}\\\\
		  &=&n(n-1)(n-2)p^3 \sum_{x=3}^n \dfrac{m!}{(m-y)!y!}p^{y}(1-p)^{m-y}\\\\
		  &=&n(n-1)(n-2)p^3\\\\
\end{array}$$

Así,
$$\begin{array}{rcl}
    \mu_3^{'}-3\mu_2^{'}+2\mu&=&n(n-1)(n-2)p^3\\
    \mu_3^{'}&=&n(n-1)(n-2)p^3+3[n(n-1)p^2+np]-2np\\
	     &=&n(n-1)(n-2)p^3+3n(n-1)p^2+np\\
\end{array}$$

El tercero momento central $\mu_3$ puede ser determinado por $\mu_3 = \mu_3^{'}-3\mu\mu_2^{'} + 2\mu^3$.
$$\mu_3 = n(n-1)(n-2)p^3 +3n(n-1)p^2 + np-3np[n(n-1)p^2+np]+2n^3p^3$$
Lo que se traduce como:
$$\mu_3=np(1-p)(1-2p)$$
Por tanto de $\alpha_3 = \dfrac{\mu_3}{(\mu_2)^{3/2}}$ el tercer momento estandarizado de la distribución binomial es:
$$\begin{array}{rcl}
    \alpha_3&=&\dfrac{np(1-p)(1-2p)}{[np(1-p)]^{3/2}}\\\\
	    &=&\dfrac{np(1-p)(1-2p)}{np(1-p)[np(1-p)]^{1/2}}\\\\
	    &=&\dfrac{1-2p}{[np(1-p)]^{1/2}}\\\\
\end{array}$$


De manera similar, para el cuarto momento alrededor del cero se evalúa $E[X(X-1)(X-2)(X-3)]$ dado que:
$$E[X(X-1)(X-2)(X-3)] = \mu_4^{'}-6\mu_3^{'}+11\mu_2^{'}-6\mu.$$

es decir,

$$\begin{array}{rcl}
		       E[X(X-1)(X-2)(X-3)]&=&\sum\limits_{x=4}^n x(x-1)(x-2)(x-3)\cdot \dfrac{n!}{(n-x)!x!}p^x(1-p)^{n-x}\\\\
					  &=&n(n-1)(n-2)(n-3)p^4\sum\limits_{x=4}^n \dfrac{(n-4)!}{(n-x)!(x-4)!}p^{x-4}(1-p)^{n-x}\\\\
					  &=&n(n-1)(n-2)(n-3)p^4\sum\limits_{y=0}^m\dfrac{m!}{(m-y)!y!}p^{y}(1-p)^{m-y}\\\\
		       &=&n(n-1)(n-2)(n-3)p^4\\\\
\end{array}$$

Sustituir en $E[X(X-1)(X-2)(X-3)] = \mu_4^{'}-6\mu_3^{'}+11\mu_2^{'}-6\mu.$ y para resolver $\mu_4^{'}$ se tiene:
$$\mu_4^{'} = n(n-1)(n-2)(n-3)p^4 + 6[n(n-1)(n-2)p^2+3n(n-1)p^2+np]-11[n(n-1)p^2+np]+6np$$

Luego de acuerdo a $\mu_4=\mu_4^{'}-4\mu \mu_3^{'}+6\mu^2\mu_2^{'}-3\mu^4$ el cuarto momento central es:
$$\mu_4 = np(1-p)\lbrace 3np(1-p)+[1-6p(1-p)]\lbrace.$$

Y de de acuerdo con $\alpha_4=\mu_4/\mu_2^2$
$$\alpha_4=\dfrac{np(1-p)\lbrace 3np(1-p)+[1-6p(1-p)]\lbrace}{n^2p^2(1-p)^2} = 3 + \dfrac{[1-6p(1-p)]}{np(1-p)}$$
La varianza de una variable aleatoria binomial siempre es menor que el valor de su media.\\\\

De acuerdo con la definición 3.14 la función generadora de momentos para la distribución binomial es:
$$\begin{array}{rcl}
    m_X(t)=E(e^{tX})&=&\sum\limits_{x=0}^n e^{tx}\dfrac{n!}{(n-x)x!}p^x(1-p)^{n-x}\\\\
		    &=&\sum\limits_{x=0}^n \dfrac{n!}{(n-x)x!}(e^t p)(1-p)^{n-x}\\\\
		    &=&(1-p)^n + n(1-p)^{n-1}(e^t p) + \dfrac{n(n-1)}{2!}(1-p)^{n-2} (e^t p)^2+\ldots+(e^t p)^n\\\\
		    &=&[(1-p)+e^tp]^n\\\\
\end{array}$$

Al tomar las dos primeras derivadas de $[(1-p)+e^tp]^n$ con respecto de $t$, se obtiene:
$$\dfrac{dm_X(t)}{dt} = ne^t p [(1-p)+e^t p]^{n-1}$$
y
$$\dfrac{d^2m_X (t)}{dt^2}=n(n-1)(e^t p)^2[(1-p)+e^tp]^{n-2}+ne^t p[(1-p)+e^t p]^{n-1}$$
Si $t=0$, obtiene el primero y segundo momento al rededor del cero,
$$\dfrac{dm_X(t)}{dt}\bigg|_{t=0} = np[(1-p)+p]^{n-1} = np$$
y
$$\dfrac{d^2m_X(t)}{dt^2}\bigg|_{t=0} = n(n-1)p^2[(1-p)+p]^{n-2} + np[(1-p)+p]^{n-1}=n(n-1)p^2 + np.$$


\section{La distribución de Poisson}
La distribución es el principal modelo de probabilidad empleado para analizar problemas de líneas de espera. Además, ofrece una aproximación excelente a la función de probabilidad binomial cuando $p$ es pequeño y $n$ es grande.\\\\

\begin{tcolorbox}
    \begin{def.}
	Sea $X$ una variable aleatoria que representa el número de eventos aleatorios independientes que ocurren a una rapidez constante sobre el tiempo o el espacio. Se dice entonces que la variable aleatoria $X$ tiene una distribución de Poisson con función de probabilidad.
	$$p(x;\lambda)=\left\{\begin{array}{ll}
		\dfrac{e^{-\lambda}\lambda^x}{x!} & x=0,1,2,\ldots; \quad \lambda > 0\\
		0&\mbox{para cualquier otro valor}\\
	\end{array}\right.$$\\
	El parámetro de la distribución de Poisson es $\lambda$, el número promedio de ocurrencias del evento aleatorio por unidad de tiempo.
    \end{def.}
\end{tcolorbox}
Verificamos que es una función de probabilidad. Puesto que $p(x;\lambda)>0$ para $x=0,1,2,\ldots$,
$$\sum_{x=0}^\infty p(x;\lambda) = \sum_{x=0}^\infty \dfrac{e^{-\lambda}\lambda^x}{x!} = e^{-\lambda}\sum_{x=0}^\infty \dfrac{\lambda^x}{x!}=e^{-\lambda}\left(1+\lambda + \dfrac{\lambda^2}{2!}+\ldots\right) = e^{-\lambda}e^{\lambda}=1$$\\

Nótese que la variable aleatoria de Poisson tiene un valor entero, y que pueden usarse los valores de las probabilidades acumuladas para determinar las probabilidades individuales mediante el empleo de la relación:
$$p(x;\lambda) = F(x;\lambda)-F(x-1;\lambda)$$\\

\textbf{Deducción de la función de probabilidad de Poisson.-}\\\\

Sea $p(x;t)$ la probabilidad de tener, de manera exacta, $X$ ocurrencias en un intervalo $t$, y supóngase lo siguiente:

\begin{enumerate}[\bfseries 1.]
    \item En este intervalo, los eventos ocurren de manera independiente.
    \item La probabilidad de una sola ocurrencia, en un intervalo muy pequeño $dt$ es $vdt$, en donde $v$ es la frecuencia constante de ocurrencia y $(v>0)$.
    \item El intervalo $dt$ en tan pequeño, que la probabilidad de tener más de una ocurrencia en $dt$ es despreciable.
\end{enumerate}

El evento que en el tiempo $t+dt$ ha ocurrido exactamente $x$ veces, puede llevarse a cabo de dos maneras diferentes y excluyentes:

\begin{enumerate}[\bfseries 1.]
    \item Existen $x$ ocurrencias por tiempo $t$, con probabilidad $p(x;t)$ y ninguna en $dt$, con probabilidad $(1-vdt)$. Dada la suposición de independencia, la probabilidad conjunta es $p(x;t)(1-vdt)$.
    \item Existen $x-1$ ocurrencias por tiempo $t$, con probabilidad $p(x-1;t)$ y una durante $dt$, con probabilidad $vdt$. Otra vez, dada la suposición de independencia, la probabilidad conjunta es: $p(x-1;t)vdt.$ 
\end{enumerate}
Esto es:
$$p(x;t+dt)=p(x;t)(1-vdt)+p(x-1;t)vdt.$$
Después de multiplicar, transportar $p(x;t)$ al primer miembro, y dividir por $dt$ se tiene:
$$\dfrac{p(x;t+dt)-p(x;t)}{dt}=v[p(x-1;t)]-p(x;t).$$
Si se toma el límite conforme $dt\to 0$, por definición se tiene:
$$\dfrac{dp(x;t)}{dt}=v[p(x-1;t)-p(x;t)]$$
 Si se toma el límite conforme $dt\to 0,$ por definición se tiene:
 $$\dfrac{dp(x;t)}{dt}=v\left[p(x-1;t)-p(x;t)\right]$$
 que es una ecuación diferencial lineal con respecto a $t$ y una ecuación de diferencias finitas de primer orden, con respecto a $x$. Si $x=0$, la ecuación se convierte en 
 $$\dfrac{dp(0;t)}{dt}=v\left[p(-1;t)-p(0;t)\right] = -vp(0;t)$$
 dado que $p()i-1;t$ tiene que ser cero. La solución general de la ecuación diferencial lineal 
 $$\dfrac{dp(0;t)}{dt}=-vp(0;t)$$
 se obtiene mediante separación de variables e integración en ambos miembros, lo que da como resultado:
 $$ln\left[p(0;t)\right]=ln(c)-vt,$$
 o
 $$p(0;t)=ce^{-vt}$$
 Dado que la probabilidad de tener cero ocurrencias en un intervalo $t=0$, debe ser $1$, $c=1$, y
 $$p(0;t)=e^{-vt}.$$
 Si $x=1,$ la ecuación  $\dfrac{dp(x;t)}{dt}=v[p(x-1;t)-p(x;t)]$ se convierte en
$$\dfrac{dp(1;t)}{dt}=v[p(0;t)-p(1;t)]$$
o
$$\dfrac{dp(1;t)}{dt}+vp(1;t) = ve^{-vt}$$

Esta ecuación es una ecuación diferencial no homogénea con la condición inicial de que $p(1;0)=0$ dado que la probabilidad de tener exactamente una ocurrencia en $t=0$ debe ser cero.  La solución es
$$p(1;t)=(vt) e^{-vt}$$
De manera similar, para $x=2$ y $p(2;0)=0$, $\dfrac{dp(x;t)}{dt}=v[p(x-1;t)-p(x;t)]$ se reduce a
$$\dfrac{dp(2;t)}{dt} + vp(2;t) = v^2te^{-vt}$$
cuya solución es 
$$p(2;t)=\dfrac{(vt)^2e^{-vt}}{2!}.$$
Al continuar este proceso puede deducirse que la probabilidad de tener exactamente $x$ ocurrencias en $t$ es 
$$p(x;t)=\dfrac{(xt)^x e^{-vt}}{x!},\qquad x=0,1,2,\ldots$$

siempre que $p(x;0)=0$. Si se sustituye $\lambda =vt$ en esta último ecuación, el resultado es la función de probabilidad de Poisson.\\

\begin{tcolorbox}
    La probabilidad de que una variable aleatoria de Poisson $X$ sea menor o igual a un valor de $x$ se determina por la función de distribución acumulativa.
    $$P(X\leq x)=F(x;\lambda)=\sum_{i=0}^x \dfrac{e^{-\lambda} \lambda^i}{i!}$$
    se podría usar también la relación
    $$p(x;\lambda) = F(x;\lambda)-F(x-1;\lambda).$$
\end{tcolorbox}

% ------------------ teorema 1
\begin{teo}
    Sea $X$ una variable aleatoria con distribución binomial y función de probabilidad:
    $$p(x;n,p)=\dfrac{n!}{(n-x)!x!}p^x(1-p)^{n-x}\qquad x=0,1,2,\ldots,n.$$
    Si para $n=1,2,\ldots$ la relación $p=\lambda/n$ es cierta para alguna constante $\lambda>0,$ entonces
    $$\lim_{n\to\infty, \; p\to 0} p(x;n,p)=\dfrac{e^{-\lambda} \lambda^x}{x!},\qquad x=0,1,2,\ldots$$
    Demostración.-\; Al multiplicar numerador y denominador por $n^x$ y sustituir $n!/(n-x)!=n(n-1)(n-2)\ldots(n-x+1)$, la función de probabilidad binomial es:
    $$\begin{array}{rcl}
	p(x;n,p) &=& \dfrac{n(n-1)(n-2)\ldots(n-x+1)}{n^x x!}(np)^x(1-p)^{n-x}\\\\
		 &=& \dfrac{n(n-1)(n-2)\ldots(n-x+1)}{n^x x!}\dfrac{\lambda^x}{x!}(1-p)^{n-x}\\\\
		 &=& 1\left(1-\dfrac{1}{n}\right)\left(1-\dfrac{2}{n}\right)\ldots \left(1-\dfrac{x-1}{n}\right)\dfrac{\lambda^x}{x!}(1-p)^{n-x}\\\\
		 &=& \dfrac{\left(1-\dfrac{1}{n}\right)\left(1-\dfrac{2}{n}\right)\ldots \left(1-\dfrac{x-1}{n}\right)}{(1-p)^x}\dfrac{\lambda^x}{x!}(1-p)^{n}\\\\
    \end{array}$$
    Dado que:
    $$(1-p)^n=[(1-p)^{-1/p}]^{-np} = [(1-p)^{-1/p}]^{-\lambda}$$
    y por definición 
    $$\lim_{z\to 0}(1+z)^{1/z}=e,$$
    mediante el cambio de variable $z=-p,$ se tiene
    $$\lim_{p\to 0}[(1-p)^{-1/p}]^{-\lambda=e^{-\lambda}}$$
    Además,
    $$\lim_{n\to \infty}\left(1-\dfrac{1}{n}\right)\left(1-\dfrac{2}{n}\right)\ldots \left(1-\dfrac{x-1}{n}\right)=1$$
    Al sustituir se tiene,
    $$\lim_{x\to \infty.\; p\to 0}p(x;n,p)=\dfrac{e^{-\lambda}\lambda^{x}}{x!},\qquad x=0,1,2,\ldots .$$\\
\end{teo}

Los momentos de la variable de Poisson se determinan mediante los mismo procedimientos utilizados para obtener los momentos de la variable aleatoria binomial. \textbf{\boldmath Si $X$ es una variable aleatoria de Poisson, su valor esperado es:}

\begin{tcolorbox}
    $$E(X)=\sum_{x=0}^\infty x\dfrac{e^{-\lambda}\lambda^x}{x!}=e^{-\lambda}\sum_{x=1}^\infty\dfrac{\lambda^x}{(x-1)!}=\lambda e^{-\lambda}\sum_{x=1}^\infty \dfrac{\lambda^{x-1}}{(x-1)!}=\lambda e^{-\lambda}\sum_{y=0}^\infty \dfrac{\lambda^y}{y!}=\lambda.$$
\end{tcolorbox}

Para la varianza $X$:

$$E[X(X-1)]=\sum_{x=0}^\infty x(x-1)\dfrac{e^{-\lambda}\lambda^x}{x!}=\lambda^2e^{-\lambda}\sum_{y=2}\dfrac{\lambda^{x-2}}{(x-2)!}=\lambda^2e^{-\lambda}\sum_{y=0}^\infty \dfrac{\lambda^y}{y!}=\lambda^2$$

Luego por el hecho de que $E(X^2)=E[X(X-1)]+E(X)$ tenemos,

$$E(X^2)=\mu^{'}_2 = \lambda^2 + \lambda,$$

Y \textbf{\boldmath la varianza de $X$ de una variable aleatoria de Poisson es:}

\begin{tcolorbox}
    $$Var(X)=\mu_2^{'}-\mu^2=\lambda^2+\lambda - \lambda^2 = \lambda.$$
\end{tcolorbox}

De esta manera, una característica distintiva de la variable aleatoria de Poisson  es que su media es igual a su varianza.\\\\

Para el tercer momento central, se tiene:

$$E[X(X-1)(X-2)]=\lambda^3.$$

De donde $$\mu_3^{'}=\lambda^3+3\lambda^2 + \lambda,$$

Y \textbf{el tercer momento central} es:

\begin{tcolorbox}
$$\mu_3=\lambda$$
\end{tcolorbox}

Como resultado, \textbf{el coeficiente de asimetría} se determina por:

\begin{tcolorbox}
    $$\alpha_3=\dfrac{\mu_3}{\mu_2^{3/2}}=\dfrac{1}{\sqrt{\lambda}}.$$
\end{tcolorbox}

Par el cuarto momento central puede emplearse el mismo procedimiento para demostrar que:

$$E[X(X-1)(X-2)(X-3)]=\lambda^4,$$

De donde 
$$\mu_4^{'}=\lambda^4+6\lambda^3 + 7\lambda^2+\lambda.$$

Así el \textbf{cuarto momento central} es:

$$\mu_4 = 3\lambda^2 + \lambda,$$

Y el \textbf{cuarto momento estandarizado para la distribución de Poisson} lo establece:

$$\lambda_4 = \dfrac{\mu_4}{\mu_2^2}=3+\dfrac{1}{\lambda}$$

\textbf{La función generadora de momentos para la distribución de Poisson} se determina por:

\begin{tcolorbox}
    $$m_x(t)=\sum_{x=0}^\infty e^{tx}\dfrac{e^{-\lambda}\lambda^x}{x!}=e^{-\lambda}\sum_{x=0}^\infty \dfrac{\left(\lambda e^t\right)^x}{x!}=e^{-\lambda}e^{\lambda e^t}=e^{[\lambda(e^t-1)]}.$$
\end{tcolorbox}

\begin{tcolorbox}
    En conclusión, la distribución de Poisson es leptocúrtica con un sesgo positivo y se emplea para modelar el número de eventos aleatorios independientes que ocurren a una rapidez constantes ya sea sobre el tiempo o el espacio.
\end{tcolorbox}


\section{La distribución hipergeométrica}

% -------------------- definición 4.3
\begin{def.}
    Sea $N$ el número total de objetos en una población finita, de manera tal que $k$ de estos es de un tipo $N-k$ de otros. Si se selecciona una muestra aleatoria de la población constituida por $n$ objetos de la probabilidad de que $x$ sea de un tipo exactamente y $n-x$ sea de otro, está dada por la función de probabilidad hipergeométrica:
    \begin{tcolorbox}
	$$p(x;N,n,k)=\left\{\begin{array}{ll}
		\dfrac{{k \choose x}{N-k\choose n-x}}{{N\choose n}}, & x=0,1,2,\ldots,n;\quad x\leq k, \quad n-x\leq N-k;\quad N,n,k, \mbox{ enteros positivos,}\\\\
		0 & \mbox{ para cualquier otro valor.}
	\end{array}\right.$$
    \end{tcolorbox}
\end{def.}

La \textbf{distribución acumulada} estará dada por 

\begin{tcolorbox}
    $$P(X\leqx) = F(x;N,n,k)=\sum_{i=0}^x \dfrac{{k \choose x}{N-k\choose n-x}}{{N\choose n}}.$$
\end{tcolorbox}


