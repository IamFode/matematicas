\chapter{Algunas distribuciones discretas de probabilidad}


\setcounter{section}{1}
\section{La distribución binomial}
Llámese éxito a la ocurrencia del evento y fracaso a su no ocurrencia.\\\\
Las dos suposiciones claves para la distribución binomial son:
\begin{enumerate}
    \item La probabilidad de éxito $p$ permanece constante para cada ensayo.
    \item Los $n$ ensayos son independientes entre sí.
\end{enumerate}

Para obtener la función de probabilidad de la distribución binomial, primero se determina la probabilidad de tener, en $n$ ensayos, $x$ éxitos consecutivos seguidos de $n$-$x$ fracasos consecutivos. Dado que, por hipótesis, los $n$ ensayos son independientes de la definición 2.15, se tiene:
$$p\cdot p\cdots p \cdot (1-p)\cdot (1-p)\cdots (1-p) = p^x(1-p)^{n-x}$$

%-------------------- Definición 4.1.
\begin{tcolorbox}[colback = white]
    \begin{def.}[Distribución binomial con función de probabilidad]
	Sea $X$ una variable aleatoria que representa el número de éxitos en $n$ ensayos y $p$ la probabilidad de éxito con cualquiera de éstos. Se dice entonces que $X$ tiene una distribución binomial con función de probabilidad.
	$$p(x;n,p)\left\{\begin{array}{ll}
	    \dfrac{n!}{(n-x)!x!} p^x(1-p)^{n-x} & x = 0,1,2,\ldots,n.\\\\
	    0\; \mbox{para cualquier otro valor.} & 0\leq p \leq 1. \; \mbox{para } n \;\mbox{entero}.\\
	\end{array}\right.$$
     \end{def.}
\end{tcolorbox}

El nombre distribución binomial proviene del hecho de que los valores de $p(x;n,p)$ para $x=1,2,\ldots,n$ son los términos sucesivos de la expansión binomial de $[(1-p)+p]^n;$ esto es,
$$\begin{array}{rcl}
    [(1-p)+p]^n & = & (1-p)^n + n(1-p)^{n-1} p + \dfrac{n(n-1)}{2!} (1-p)^{n-2}p^2 + \cdots + p^n\\\\
		& = & \sum\limits_{x=0}^n \dfrac{n!}{(x-x)!x!} p^x(1-p)^{n-x}\\\\
		& = & \sum\limits_{x=0}^n p(x;n,p)\\\\
\end{array}$$

Pero dado que $[(1-p)+p]^n = 1$ y $p(x;n,p)\geq 0$ para $x=0,1,2,\ldots n,$ este hecho también verifica que $p(x;n,p)$ es una función de probabilidad.\\\\

La probabilidad de que una variable aleatoria $X$ sea menor o igual a un valor específico de $x$, se determina por la \textbf{función de distribución acumulativa}.

% ---------------------- función de distribución acumulativa
\begin{tcolorbox}[colback=white]
    $$P(X\leq x) = F(x;n,p) = \sum_{i=0}^x {n\choose i} p^i (1-p)^{n-i}$$
\end{tcolorbox}

Nótese que si $n=1$, la función de probabilidad binomial se deduce a

\begin{tcolorbox}[colback=white]
    $$p(x;n,p)=\left\{\begin{array}{ll}
	p^x(1-p)^{1-x} & x = 0,1\\\\
	0 & \mbox{para cualquier otro valor}.
    \end{array}\right.$$
\end{tcolorbox}

Que es la \textbf{función de probabilidad de la distribución puntual o Bernoulli.}\\\\

Por definición 3.8, el primer momento alrededor del cero de la variable aleatoria binomial $X$ es el valor esperado de $X$.
$$\begin{array}{rcl}
    E(X) & = & x\sum\limits_{x=0}^n \dfrac{n!}{(n-x)!x!} p^x(1-p)^{n-x}\\\\
	 & = & x\sum\limits_{x=1}^n \dfrac{n!}{(n-x)!x!} p^x(1-p)^{n-x}\\\\
	 & = & \sum\limits_{x=1}^n \dfrac{n!}{(n-x)!(x-1)!} p^x(1-p)^{n-x}\\\\
\end{array}$$

en donde se ha escrito la suma desde uno hasta $n$, dado que cuando $x=0$ el primer término es cero y se cancela la $x$ del numerador con la $x$ en $x!$. Factorizando $n$ y $p$, se tiene: 
$$E(X)=np\sum_{x=1}^n \dfrac{(n-1)!}{(n-x)!(x-1)!}p^{x-1}(1-p)^{n-x}$$

Si $y=x-1$ y $m=n-1$, entonces:
$$E(X)=np\sum_{x=1}^n \dfrac{m!}{(m-y)!y!}p^{y}(1-p)^{n-y}$$

De donde se sabe que $p(y; m,p) = \dfrac{m!}{(m-y)!y!}p^{y}(1-p)^{n-y}$   es la función de probabilidad de una variable aleatoria binomial $Y$ con parámetros $m=n-1$ y $p$; de ésta manera $\sum_{y=0}^m p(y;m,p) = 1$ y \textbf{la media de una variable  aleatoria binomial es}:

\begin{tcolorbox}[colback=white]
    $$E(X)=\mu = np.$$
\end{tcolorbox}

Para obtener la varianza, se necesita el segundo momento alrededor de cero, $\mu_2^{'}$, o:
$$E(X^2) = \sum\limits_{x=0}^n x^2 p(x;n,p)$$
pero, e el término $x^2/x!$ se cancelará una sola $x$ en el numerador, y la que resta evitará que la suma se manipule de la misma forma en que se determinó la media. La alternativa es escribir $x^2$ como:
$$x^2= x(x-1)+x;$$
de esta manera se tiene:
$$E(X^2) = E[X(X-1)]+E(X).$$
Dado que $E(X)$ ya se ha determinado, puede usarse el mismo procedimiento para evaluar $E(X(X-1))$:
$$\begin{array}{rcl}
    E[X(X-1)]&=&\sum\limits_{x=0}^n x(x-1)\dfrac{n!}{(n-x)!x!}p^x (1-p)^{n-x}\\\\
	     &=&\sum\limits_{x=2}^n x(x-1)\dfrac{n!}{(n-x)!x!}p^x(1-p)^{n-x}\\\\
	     &=&\sum\limits_{x=2}^n \dfrac{n!}{(n-x)(x-2)!}p^x(1-p)^{n-x}\\\\
	     &=&n(n-1)p^2\sum\limits_{x=2}^n \dfrac{(n-2)!}{(n-x)!(x-2)!}p^{x-2}(1-p)^{n-x}\\\\
\end{array}$$

Sea $y=x-2$ y $m=n-2$, entonces:
$$\begin{array}{rcl}
    E[X(X-1)]&=&n(n-1)p^2\sum\limits_{y=0}^m \dfrac{m!}{(m-y)y!}p^y(1-p)^{m-y}\\\\
	     &=&n(n-1)p^2\sum\limits_{y=0}^m p(y;m,p)\\\\
	     &=&n(n-1)p^2\\\\
\end{array}$$

Así,
$$E(X^2) = \mu_2^{'} = n(n-1)p^2+np$$
De esta manera, la \textbf{varianza de una variable aleatoria binomial} es:

\begin{tcolorbox}[colback=white]
    $$Var(X) = \mu_2^{'}-\mu^2 = n(n-1)p^2 + np - n^2p^2 = np[(n-1)p+1-np] = np(1-p).$$
\end{tcolorbox}

\vspace{1cm}

Para obtener el tercer momento alrededor del cero, se determina $E[X(X-1)(X-2)]$ dado que:
$$E[X(X-1)(X-2)]=\mu_3^{'} - 3\mu_2^{'} + 2\mu$$

$$\begin{array}{rcl}
    E[X(X-1)(X-2)]&=&\sum\limits_{x=0}^n x(x-1)(x-2)\dfrac{n!}{(n-x)!x!} p^x(1-p)^{n-x}\\\\
		  &=&\sum\limits_{x=3}^n \dfrac{n!}{(n-x)!(x-3)!}p^x(1-p)^{n-x}\\\\
		  &=&n(n-1)(n-2)p^3 \sum_{x=3}^n \dfrac{(n-3)!}{(n-x)!(x-3)!}p^{x-3}(1-p)^{n-x}\\\\
		  &=&n(n-1)(n-2)p^3 \sum_{x=3}^n \dfrac{m!}{(m-y)!y!}p^{y}(1-p)^{m-y}\\\\
		  &=&n(n-1)(n-2)p^3\\\\
\end{array}$$

Así,
$$\begin{array}{rcl}
    \mu_3^{'}-3\mu_2^{'}+2\mu&=&n(n-1)(n-2)p^3\\
    \mu_3^{'}&=&n(n-1)(n-2)p^3+3[n(n-1)p^2+np]-2np\\
	     &=&n(n-1)(n-2)p^3+3n(n-1)p^2+np\\
\end{array}$$

El tercero momento central $\mu_3$ puede ser determinado por $\mu_3 = \mu_3^{'}-3\mu\mu_2^{'} + 2\mu^3$.
$$\mu_3 = n(n-1)(n-2)p^3 +3n(n-1)p^2 + np-3np[n(n-1)p^2+np]+2n^3p^3$$
Lo que se traduce como:
$$\mu_3=np(1-p)(1-2p)$$
Por tanto de $\alpha_3 = \dfrac{\mu_3}{(\mu_2)^{3/2}}$ el tercer momento estandarizado de la distribución binomial es:
$$\begin{array}{rcl}
    \alpha_3&=&\dfrac{np(1-p)(1-2p)}{[np(1-p)]^{3/2}}\\\\
	    &=&\dfrac{np(1-p)(1-2p)}{np(1-p)[np(1-p)]^{1/2}}\\\\
	    &=&\dfrac{1-2p}{[np(1-p)]^{1/2}}\\\\
\end{array}$$


De manera similar, para el cuarto momento alrededor del cero se evalúa $E[X(X-1)(X-2)(X-3)]$ dado que:
$$E[X(X-1)(X-2)(X-3)] = \mu_4^{'}-6\mu_3^{'}+11\mu_2^{'}-6\mu.$$

es decir,

$$\begin{array}{rcl}
		       E[X(X-1)(X-2)(X-3)]&=&\sum\limits_{x=4}^n x(x-1)(x-2)(x-3)\cdot \dfrac{n!}{(n-x)!x!}p^x(1-p)^{n-x}\\\\
					  &=&n(n-1)(n-2)(n-3)p^4\sum\limits_{x=4}^n \dfrac{(n-4)!}{(n-x)!(x-4)!}p^{x-4}(1-p)^{n-x}\\\\
					  &=&n(n-1)(n-2)(n-3)p^4\sum\limits_{y=0}^m\dfrac{m!}{(m-y)!y!}p^{y}(1-p)^{m-y}\\\\
		       &=&n(n-1)(n-2)(n-3)p^4\\\\
\end{array}$$

Sustituir en $E[X(X-1)(X-2)(X-3)] = \mu_4^{'}-6\mu_3^{'}+11\mu_2^{'}-6\mu.$ y para resolver $\mu_4^{'}$ se tiene:
$$\mu_4^{'} = n(n-1)(n-2)(n-3)p^4 + 6[n(n-1)(n-2)p^2+3n(n-1)p^2+np]-11[n(n-1)p^2+np]+6np$$

Luego de acuerdo a $\mu_4=\mu_4^{'}-4\mu \mu_3^{'}+6\mu^2\mu_2^{'}-3\mu^4$ el cuarto momento central es:
$$\mu_4 = np(1-p)\lbrace 3np(1-p)+[1-6p(1-p)]\lbrace.$$

Y de de acuerdo con $\alpha_4=\mu_4/\mu_2^2$
$$\alpha_4=\dfrac{np(1-p)\lbrace 3np(1-p)+[1-6p(1-p)]\lbrace}{n^2p^2(1-p)^2} = 3 + \dfrac{[1-6p(1-p)]}{np(1-p)}$$
La varianza de una variable aleatoria binomial siempre es menor que el valor de su media.\\\\

De acuerdo con la definición 3.14 la función generadora de momentos para la distribución binomial es:
$$\begin{array}{rcl}
    m_X(t)=E(e^{tX})&=&\sum\limits_{x=0}^n e^{tx}\dfrac{n!}{(n-x)x!}p^x(1-p)^{n-x}\\\\
		    &=&\sum\limits_{x=0}^n \dfrac{n!}{(n-x)x!}(e^t p)(1-p)^{n-x}\\\\
		    &=&(1-p)^n + n(1-p)^{n-1}(e^t p) + \dfrac{n(n-1)}{2!}(1-p)^{n-2} (e^t p)^2+\ldots+(e^t p)^n\\\\
		    &=&[(1-p)+e^tp]^n\\\\
\end{array}$$

Al tomar las dos primeras derivadas de $[(1-p)+e^tp]^n$ con respecto de $t$, se obtiene:
$$\dfrac{dm_X(t)}{dt} = ne^t p [(1-p)+e^t p]^{n-1}$$
y
$$\dfrac{d^2m_X (t)}{dt^2}=n(n-1)(e^t p)^2[(1-p)+e^tp]^{n-2}+ne^t p[(1-p)+e^t p]^{n-1}$$
Si $t=0$, obtiene el primero y segundo momento al rededor del cero,
$$\dfrac{dm_X(t)}{dt}\bigg|_{t=0} = np[(1-p)+p]^{n-1} = np$$
y
$$\dfrac{d^2m_X(t)}{dt^2}\bigg|_{t=0} = n(n-1)p^2[(1-p)+p]^{n-2} + np[(1-p)+p]^{n-1}=n(n-1)p^2 + np.$$

\section{La distribución de Poisson}
La distribución es el principal modelo de probabilidad empleado para analizar problemas de líneas de espera. Además, ofrece una aproximación excelente a la función de probabilidad binomial cuando $p$ es pequeño y $n$ es grande.\\\\

\begin{tcolorbox}[colback=white]
    \begin{def.}
	Sea $X$ una variable aleatoria que representa el número de eventos aleatorios independientes que ocurren a una rapidez constante sobre el tiempo o el espacio. Se dice entonces que la variable aleatoria $X$ tiene una distribución de Poisson con función de probabilidad.
	$$p(x;\lambda)=\left\{\begin{array}{ll}
		\dfrac{e^{-\lambda}\lambda^x}{x!} & x=0,1,2,\ldots; \quad \lambda > 0\\
		0&\mbox{para cualquier otro valor}\\
	\end{array}\right.$$\\
	El parámetro de la distribución de Poisson es $\lambda$, el número promedio de ocurrencias del evento aleatorio por unidad de tiempo.
    \end{def.}
\end{tcolorbox}
Verificamos que es una función de probabilidad. Puesto que $p(x;\lambda)>0$ para $x0,1,2,\ldots$,
$$\sum_{x=0}^\infty p(x;\lambda) = \sum_{x=0}^\infty \dfrac{e^{-\lambda}\lambda^x}{x!} = e^{-\lambda}\sum_{x=0}^\infty \dfrac{\lambda^x}{x!}=e^{-\lambda}\left(1+\lambda + \dfrac{\lambda^2}{2!}+\ldots\right) = e^{-\lambda}e^{\lambda}=1$$\\

\textbf{Deducción de la función de probabilidad de Poisson.-}\\\\

Sea $p(x;t)$ la probabilidad de tener, de manera exacta, $X$ ocurrencias en un intervalo $t$, y supóngase lo siguiente 

