\chapter{Algunas distribuciones discretas de probabilidad}


\setcounter{section}{1}
\section{La distribución binomial}
Llámese éxito a la ocurrencia del evento y fracaso a su no ocurrencia.\\\\
Las dos suposiciones claves para la distribución binomial son:
\begin{enumerate}
    \item La probabilidad de éxito $p$ permanece constante para cada ensayo.
    \item Los $n$ ensayos son independientes entre sí.
\end{enumerate}

Para obtener la función de probabilidad de la distribución binomial, primero se determina la probabilidad de tener, en $n$ ensayos, $x$ éxitos consecutivos seguidos de $n$-$x$ fracasos consecutivos. Dado que, por hipótesis, los $n$ ensayos son independientes de la definición 2.15, se tiene:
$$p\cdot p\cdots p \cdot (1-p)\cdot (1-p)\cdots (1-p) = p^x(1-p)^{n-x}$$

%-------------------- Definición 4.1.
\begin{tcolorbox}[colback = white]
    \begin{def.}[Distribución binomial con función de probabilidad]
	Sea $X$ una variable aleatoria que representa el número de éxitos en $n$ ensayos y $p$ la probabilidad de éxito con cualquiera de éstos. Se dice entonces que $X$ tiene una distribución binomial con función de probabilidad.
	$$p(x;n,p)\left\{\begin{array}{ll}
	    \dfrac{n!}{(n-x)!x!} p^x(1-p)^{n-x} & x = 0,1,2,\ldots,n.\\\\
	    0\; \mbox{para cualquier otro valor.} & 0\leq p \leq 1. \; \mbox{para } n \;\mbox{entero}.\\
	\end{array}\right.$$
     \end{def.}
\end{tcolorbox}

El nombre distribución binomial proviene del hecho de que los valores de $p(x;n,p)$ para $x=1,2,\ldots,n$ son los términos sucesivos de la expansión binomial de $[(1-p)+p]^n;$ esto es,
$$\begin{array}{rcl}
    [(1-p)+p]^n & = & (1-p)^n + n(1-p)^{n-1} p + \dfrac{n(n-1)}{2!} (1-p)^{n-2}p^2 + \cdots + p^n\\\\
		& = & \sum\limits_{x=0}^n \dfrac{n!}{(x-x)!x!} p^x(1-p)^{n-x}\\\\
		& = & \sum\limits_{x=0}^n p(x;n,p)\\\\
\end{array}$$

Pero dado que $[(1-p)+p]^n = 1$ y $p(x;n,p)\geq 0$ para $x=0,1,2,\ldots n,$ este hecho también verifica que $p(x;n,p)$ es una función de probabilidad.\\\\

La probabilidad de que una variable aleatoria $X$ sea menor o igual a un valor específico de $x$, se determina por la \textbf{función de distribución acumulativa}.

% ---------------------- función de distribución acumulativa
\begin{tcolorbox}[colback=white]
    $$P(X\leq x) = F(x;n,p) = \sum_{i=0}^x {n\choose i} p^i (1-p)^{n-i}$$
\end{tcolorbox}

Nótese que si $n=1$, la función de probabilidad binomial se deduce a

\begin{tcolorbox}[colback=white]
    $$p(x;n,p)=\left\{\begin{array}{ll}
	p^x(1-p)^{1-x} & x = 0,1\\\\
	0 & \mbox{para cualquier otro valor}.
    \end{array}\right.$$
\end{tcolorbox}

Que es la \textbf{función de probabilidad de la distribución puntual o Bernoulli.}\\\\

Por definición 3.8, el primer momento alrededor del cero de la variable aleatoria binomial $X$ es el valor esperado de $X$.
$$\begin{array}{rcl}
    E(X) & = & x\sum\limits_{x=0}^n \dfrac{n!}{(n-x)!x!} p^x(1-p)^{n-x}\\\\
	 & = & x\sum\limits_{x=1}^n \dfrac{n!}{(n-x)!x!} p^x(1-p)^{n-x}\\\\
	 & = & \sum\limits_{x=1}^n \dfrac{n!}{(n-x)!(x-1)!} p^x(1-p)^{n-x}\\\\
\end{array}$$

en donde se ha escrito la suma desde uno hasta $n$, dado que cuando $x=0$ el primer término es cero y se cancela la $x$ del numerador con la $x$ en $x!$. Factorizando $n$ y $p$, se tiene: 
$$E(X)=np\sum_{x=1}^n \dfrac{(n-1)!}{(n-x)!(x-1)!}p^{x-1}(1-p)^{n-x}$$

Si $y=x-1$ y $m=n-1$, entonces:
$$E(X)=np\sum_{x=1}^n \dfrac{m!}{(m-y)!y!}p^{y}(1-p)^{n-y}$$

De donde se sabe que $p(y; m,p) = \dfrac{m!}{(m-y)!y!}p^{y}(1-p)^{n-y}$   es la función de probabilidad de una variable aleatoria binomial $Y$ con parámetros $m=n-1$ y $p$; de ésta manera $\sum_{y=0}^m p(y;m,p) = 1$ y \textbf{la media de una variable  aleatoria binomial es}:

\begin{tcolorbox}[colback=white]
    $$E(X)=\mu = np.$$
\end{tcolorbox}

Para obtener la varianza, se necesita el segundo momento alrededor de cero, $\mu_2^{'}$, o:
$$E(X^2) = \sum\limits_{x=0}^n x^2 p(x;n,p)$$
pero, e el término $x^2/x!$ se cancelará una sola $x$ en el numerador, y la que resta evitará que la suma se manipule de la misma forma en que se determinó la media. La alternativa es escribir $x^2$ como:
$$x^2= x(x-1)+x;$$
de esta manera se tiene:
$$E(X^2) = E[X(X-1)]+E(X).$$
Dado que $E(X)$ ya se ha determinado, puede usarse el mismo procedimiento para evaluar $E(X(X-1))$:
$$\begin{array}{rcl}
    E[X(X-1)]&=&\sum\limits_{x=0}^n x(x-1)\dfrac{n!}{(n-x)!x!}p^x (1-p)^{n-x}\\\\
	     &=&\sum\limits_{x=2}^n x(x-1)\dfrac{n!}{(n-x)!x!}p^x(1-p)^{n-x}\\\\
	     &=&\sum\limits_{x=2}^n \dfrac{n!}{(n-x)(x-2)!}p^x(1-p)^{n-x}\\\\
	     &=&n(n-1)p^2\sum\limits_{x=2}^n \dfrac{(n-2)!}{(n-x)!(x-2)!}p^{x-2}(1-p)^{n-x}\\\\
\end{array}$$

Sea $y=x-2$ y $m=n-2$, entonces:
$$\begin{array}{rcl}
    E[X(X-1)]&=&n(n-1)p^2\sum\limits_{y=0}^m \dfrac{m!}{(m-y)y!}p^y(1-p)^{m-y}\\\\
	     &=&n(n-1)p^2\sum\limits_{y=0}^m p(y;m,p)\\\\
	     &=&n(n-1)p^2\\\\
\end{array}$$

Así,
$$E(X^2) = \mu_2^{'} = n(n-1)p^2+np$$
De esta manera, la \textbf{varianza de una variable aleatoria binomial} es:

\begin{tcolorbox}[colback=white]
    $$Var(X) = \mu_2^{'}-\mu^2 = n(n-1)p^2 + np - n^2p^2 = np[(n-1)p+1-np] = np(1-p).$$
\end{tcolorbox}

\vspace{1cm}

Para obtener el tercer momento alrededor del cero, se determina $E[X(X-1)(X-2)]$ dado que:
$$E[X(X-1)(X-2)]=\mu_3^{'} - 3\mu_2^{'} + 2\mu$$

$$\begin{array}{rcl}
    E[X(X-1)(X-2)]&=&\sum\limits_{x=0}^n x(x-1)(x-2)\dfrac{n!}{(n-x)!x!} p^x(1-p)^{n-x}\\\\
		  &=&\sum\limits_{x=3}^n \dfrac{n!}{(n-x)!(x-3)!}p^x(1-p)^{n-x}\\\\
		  &=&n(n-1)(n-2)p^3 \sum_{x=3}^n \dfrac{(n-3)!}{(n-x)!(x-3)!}p^{x-3}(1-p)^{n-x}\\\\
		  &=&n(n-1)(n-2)p^3 \sum_{x=3}^n \dfrac{m!}{(m-y)!y!}p^{y}(1-p)^{m-y}\\\\
		  &=&n(n-1)(n-2)p^3\\\\
\end{array}$$

Así,
$$\begin{array}{rcl}
    \mu_3^{'}-3\mu_2^{'}+2\mu&=&n(n-1)(n-2)p^3\\
    \mu_3^{'}&=&n(n-1)(n-2)p^3+3[n(n-1)p^2+np]-2np\\
	     &=&n(n-1)(n-2)p^3+3n(n-1)p^2+np\\
\end{array}$$

El tercero momento central $\mu_3$ puede ser determinado por $\mu_3 = \mu_3^{'}-3\mu\mu_2^{'} + 2\mu^3$.
$$\mu_3 = n(n-1)(n-2)p^3 +3n(n-1)p^2 + np-3np[n(n-1)p^2+np]+2n^3p^3$$
Lo que se traduce como:
$$\mu_3=np(1-p)(1-2p)$$
Por tanto de $\alpha_3 = \dfrac{\mu_3}{(\mu_2)^{3/2}}$ el tercer momento estandarizado de la distribución binomial es:
$$\begin{array}{rcl}
    \alpha_3&=&\dfrac{np(1-p)(1-2p)}{[np(1-p)]^{3/2}}\\\\
	    &=&\dfrac{np(1-p)(1-2p)}{np(1-p)[np(1-p)]^{1/2}}\\\\
	    &=&\dfrac{1-2p}{[np(1-p)]^{1/2}}\\\\
\end{array}$$


De manera similar, para el cuarto momento alrededor del cero se evalúa $E[X(X-1)(X-2)(X-3)]$ dado que:
$$E[X(X-1)(X-2)(X-3)] = \mu_4^{'}-6\mu_3^{'}+11\mu_2^{'}-6\mu.$$

es decir,

$$\begin{array}{rcl}
		       E[X(X-1)(X-2)(X-3)]&=&\sum\limits_{x=4}^n x(x-1)(x-2)(x-3)\cdot \dfrac{n!}{(n-x)!x!}p^x(1-p)^{n-x}\\\\
					  &=&n(n-1)(n-2)(n-3)p^4\sum\limits_{x=4}^n \dfrac{(n-4)!}{(n-x)!(x-4)!}p^{x-4}(1-p)^{n-x}\\\\
					  &=&n(n-1)(n-2)(n-3)p^4\sum\limits_{y=0}^m\dfrac{m!}{(m-y)!y!}p^{y}(1-p)^{m-y}\\\\
		       &=&n(n-1)(n-2)(n-3)p^4\\\\
\end{array}$$

Sustituir en 


