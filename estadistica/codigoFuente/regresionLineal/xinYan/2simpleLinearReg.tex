\chapter{Regresión lineal simple}

\section{Introducción}
Fue introducido por Francis Galton (1908). El modelo de regresión lineal simple está formado típicamente por:

$$y=\beta_0+\beta_1x+\epsilon.$$

Donde:\\

\begin{tabular}{rcl}

    $y$ &=& variable dependiente o variable de respuesta.\\
    $x$ &=& variable independiente o explicativo o predictor.\\ 
    $\beta_0$ &=& intercepto  $y$.\\
    $\beta_1$ &=& pendiente.\\ 
    $\epsilon$ &=& error aleatorio.

\end{tabular}

Una presentación más general de un modelo de regresión sería:

$$y=E(y)+\epsilon,$$

Donde: $E(y)$ es la esperanza matemática de la variable respuesta.Cuando $E(y)$ es una combinación lineal  de las variables explicativas $x_1,x_2,\ldots, x_k$ la regresión es una regresión lineal. Con $E(\epsilon_i)=0$ y $Var(\epsilon_i)=\sigma^2$. Todos los $\epsilon_i$ son independientes.\\

Ahora debemos hallar buenos estimadores para $\beta_0$ y $\beta_1$.


\section{Estimaciones por mínimos cuadrados}
El principal objetivo de los mínimos cuadrados para un modelo de regresión lineal simple es hallar los estimadores $b_0$ y $b_1$ tales que la suma de la distancia al cuadrados de la respuesta real $y_i$ y las respuesta de las pronosticadas $\hat{y}_i=\beta_0+\beta_1x_i$ alcanza el mínimo entre todas las opciones posibles de coeficientes de regresión $\beta_0$ y $\beta_1$. Es decir,
$$(b_0,b_1)=\arg \min_{(\beta_0,\beta_1)}\sum_{i=1}^n \left[\beta_0+\beta_1 x_i\right]^2.$$

Matemáticamente, las estimaciones de mínimos cuadrados de la regresión lineal simple se obtienen resolviendo el siguiente sistema:

\begin{equation}
    \dfrac{\partial}{\partial\beta_0}\sum_{i=1}^n \left[y_i-\left(\beta_0+\beta_ix_i\right)\right]^2=0
\end{equation}

\begin{equation}
    \dfrac{\partial}{\partial\beta_1}\sum_{i=1}^n \left[y_i-\left(\beta_0+\beta_ix_i\right)\right]^2=0
\end{equation}
\vspace{0.5cm}

Supongamos que $b_0$ y $b_1$ son soluciones del sistema de arriba, podemos describir la relación entre $x$ e $y$ por la regresión lineal $\hat{y}=b_0+b_1x$, el cual es llamado la \textbf{recta de regresión ajustada}. Es más conveniente resolver para $b_0$ y $b_1$ usando el modelo lineal centralizado:
$$y_i= \beta_0+\beta_1\overline{x}-\beta_1\overline{x}+\beta_1x_i+\epsilon_i \quad \Rightarrow \quad y_i=\beta_0^* + \beta_1(x_i-\overline{x})+\epsilon_i,$$
donde $\beta_0=\beta_0^*-\beta_1\overline{x}$. Necesitamos resolver para

$$\dfrac{\partial}{\partial\beta_0^*}\sum_{i=1}^n \left[y_i-\left(\beta_0^*+\beta_1(x_i+\overline{x})\right)\right]^2=0$$

$$\dfrac{\partial}{\partial\beta_1}\sum_{i=1}^n \left[y_i-\left(\beta_0^*+\beta_1(x_i+\overline{x})\right)\right]^2=0$$

Realizando la derivada parcial para $\beta_0$ y $\beta_1$ tenemos

$$\sum_{i=1}^n \left[y_i-(\beta_0^* + \beta_1(x_i-\overline{x}))\right]=0$$
$$\sum_{i=1}^n \left[y_i-(\beta_0^* + \beta_1(x_i-\overline{x}))\right](x_i-\overline{x})=0$$

Notemos que

\begin{equation}
    \sum_{i=1}^n y_i= n\beta_0^*+\sum_{i=1}^n \beta_1(x_i-\overline{x})=n\beta_0^*
\end{equation}

Por lo tanto, tenemos 

$$\beta_0^*=\dfrac{1}{n}\sum\limits_{i=1}^n y_i=\overline{y}.$$

Luego, sustituyendo $\beta_0^*$ por $\overline{y}$ en (2.3) obtenemos

$$\sum_{i=1}^n \left[y_i-(\overline{y} + \beta_1(x_i-\overline{x}))\right](x_i-\overline{x})=0$$

Después denotamos $b_0$ y $b_1$ las soluciones de los sistemas (2.1) y (2.2). Ahora, es fácil ver que
\begin{tcolorbox}
    \begin{equation}
	b_1=\dfrac{\sum\limits_{i=1}^n (y_i-\overline{y})(x_i-\overline{x})}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}=\dfrac{\dfrac{1}{n}\sum\limits_{i=1}^n (y_i-\overline{y})(x_i-\overline{x})}{\dfrac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x})^2}=\dfrac{S_{xy}}{S_{xx}}
    \end{equation}
\end{tcolorbox}

y

\begin{tcolorbox}
    \begin{equation}
	b_0=b_0^*-b_1\overline{x}=\overline{y}-b_1\overline{x}
    \end{equation}
\end{tcolorbox}
\vspace{0.5cm}

El valor ajustado de la regresión lineal simple es definida como $\hat{y}=\beta_0+\beta_1x_i.$ La diferencia entre $y_i$ y el valor ajustado $\hat{y}_i$ es $e_i=y_i-\hat{y}_i$, que se refiere al residuo de la regresión. Los residuos de regresión se pueden calcular a partir de las respuestas observadas $y_i$ y los valores ajustados $y_i$, por lo tanto, los residuos son observables. Cabe señalar que el término de error $\epsilon_i$ en el modelo de regresión no es observable. El error de regresión es la cantidad por la cual una observación difiere de su valor esperado; este último se basa en la población total de la que se eligió aleatoriamente la unidad estadística. El valor esperado, el promedio de toda la población, normalmente no es observable.\\

Un residual, por otro lado, es una estimación observable de un error no observable. El caso más simple implica una muestra aleatoria de n hombres cuyas alturas se miden. El promedio de la muestra se utiliza como una estimación del promedio de la población. Entonces, la diferencia entre la altura de cada hombre de la muestra y el promedio de la población no observable es un error, y la diferencia entre la altura de cada hombre de la muestra y el promedio de la muestra observable es un residuo. Dado que los residuales son observables, podemos usar los residuales para estimar el error del modelo no observable. La discusión detallada se proporcionará más adelante.

\section{Propiedades estadísticas de la estimación por mínimos cuadrados}

Primero discutiremos las propiedades estadísticas sin el supuesto de distribución del termino de error. Pero asumiremos que $\E(\epsilon_i)=0$, $\Var(\epsilon_i)=\sigma^2$ y $\epsilon_i$ para $i=1,2,\ldots,n$ son independientes.\\


%-------------------- Teorema 2.1
\begin{teo}
    El estimador de mínimos cuadrados $b_0$ es un estimador insesgado de $\beta_0$.\\\\

	Demostración.-\; 

	$$
	\begin{array}{rcl}
	    E(b_0) &=& E(\overline{y}-b_1\overline{x})\\\\
	    &=& E\left(\dfrac{1}{n}\displaystyle\sum_{i=1}^n y_i\right)-E\left(b_1\overline{x}\right)\\\\
		       &=& \dfrac{1}{n}\displaystyle\sum_{i=1}^n E(y_i)-\overline{x}E(b_1)\\\\
		   &=&\dfrac{1}{n}\displaystyle\sum_{i=1}^n (\beta_0+\beta_1x_i)-\beta_1\overline{x}\\\\
		   &=&\dfrac{1}{n} \displaystyle\sum_{i=1}^n \beta_0 + \beta_1 \dfrac{1}{n}\displaystyle\sum_{i=1}^nx_i-\beta_1\overline{x}\\\\
		   &=&\dfrac{1}{n} \displaystyle\sum_{i=1}^n \beta_0 + \beta_1 \dfrac{1}{n}\displaystyle\sum_{i=1}^nx_i-\beta_1\dfrac{1}{n}\displaystyle\sum_{i=1}^n x_i\\\\
		   &=& \dfrac{n\beta_0}{n}\\\\
		       &=&\beta_0.
	\end{array}
	$$
\end{teo}

%-------------------- Teorema 2.2
\begin{teo}
    El estimador de mínimos cuadrados $b_1$ es un estimador insesgado  de $\beta_1$.\\\\
	Demostración.-\;
	$$
	\begin{array}{rcll}
	    \E(b_1) &=& \E\left(\dfrac{S_{xy}}{S_{xx}}\right)&\\\\
		    &=& \dfrac{1}{S_{xx}} \E\left[\dfrac{1}{n}\displaystyle\sum_{i=1}^n(y_i-\overline{y})(x_i-\overline{x})\right]&\\\\
		    &=& \dfrac{1}{s_{xx}}\dfrac{1}{n}\E\left[\displaystyle\sum_{i=1}^n(x_i-\overline{x})y_i-\displaystyle\sum_{i=1}^n(x_i-\overline{x})\overline{y}\right]&\\\\
		    &=& \dfrac{1}{s_{xx}}\dfrac{1}{n}\E\left[\displaystyle\sum_{i=1}^n(x_i-\overline{x})y_i-\overline{y}\displaystyle\sum_{i=1}^n(x_i-\overline{x})\right]&\mbox{ya que } \overline{y} \mbox{ es constante}.\\\\
	\end{array}
	$$
	Sabemos que $\sum\limits_{i=1}^n (x_i-\overline{x})=\sum\limits_{i=1}^n x_i - n\overline{x}=\sum\limits_{i=1}^n x_i-n\left(\dfrac{1}{n}\sum\limits_{i=1}^n x_i\right)=0$, por lo que
	$$
	\begin{array}{rcl}
		   &=& \dfrac{1}{S_{xx}}\dfrac{1}{n}\displaystyle\sum_{i=1}^n (x_i-\overline{x}) \E(y_i)\\\\
		   &=& \dfrac{1}{S_{xx}}\dfrac{1}{n}\displaystyle\sum_{i=1}^n (x_i-\overline{x}) (\beta_0+\beta_1x_i)\\\\
		   &=& \dfrac{1}{S_{xx}}\dfrac{1}{n}\left[\beta_0\displaystyle\sum_{i=1}^n (x_i-\overline{x})  + \displaystyle\sum_{i=1}^n(x_i-\overline{x})\beta_1 x_i\right]\\\\
		   &=& \dfrac{1}{S_{xx}}\dfrac{1}{n}\displaystyle\sum_{i=1}^n (x_i-\overline{x})\beta_1x_i\\\\
		   &=& \dfrac{1}{S_{xx}}\dfrac{1}{n}\left[\displaystyle\sum_{i=1}^n\left(x_i-\overline{x}\right)\beta_1x_i-\displaystyle\sum_{i=1}^n\left(x_i-\overline{x}\right)\beta_1\overline{x}\right]\\\\
		   &=& \dfrac{1}{S_{xx}}\dfrac{1}{n}\displaystyle\sum_{i=1}^n (x_i-\overline{x})\beta_1(x_i-\overline{x})\\\\
		   &=& \dfrac{1}{S_{xx}}\dfrac{1}{n}\displaystyle\sum_{i=1}^n (x_i-\overline{x})^2\beta_1\\\\
		   &=& \dfrac{S_{xx}}{S_{xx}}\beta_1\\\\
		       &=&\beta_1.
	\end{array}
	$$
\end{teo}

%-------------------- Teorema 2.3
\begin{teo}
    $\Var(b_1) = \dfrac{\sigma^2}{nS_{xx}}.$\\\\

	Demostración.-\; Sea $X_1,X_2,\ldots,X_n$ IDD, con $Var(X_i)=\sigma_i^2$ para $i=1,2,\ldots,n$. Si $\displaystyle\sum_{i=1}^n a_iX_i.$ Entonces, 
	$$\Var\left(\displaystyle\sum_{i=1}^n a_iX_i\right)=\displaystyle\sum_{i=1}^n a_i^2\sigma_i^2.$$ 
	Por lo tanto,
	$$
	\begin{array}{rcll}
	    \Var(b_1) &=& \Var\left(\dfrac{S_{xy}}{S_{xx}}\right)&\\\\
		      &=& \left(\dfrac{1}{S_{xx}}\right)^2\Var\left[\dfrac{1}{n}\displaystyle\sum_{i=1}^n(y_i-\overline{y})(x_i-\overline{x})\right]&\\\\
		      &=& \dfrac{1}{S_{xx}^2}\Var\left[\dfrac{1}{n}\displaystyle\sum_{i=1}^n y_i(x_i-\overline{x})\right]&\\\\
		      &=& \dfrac{1}{S_{xx}^2}\dfrac{1}{n^2}\displaystyle\sum_{i=1}^n (x_i-\overline{x})^2\Var(y_i)&\\\\
		      &=& \dfrac{1}{S_{xx}^2}\dfrac{1}{n^2}\displaystyle\sum_{i=1}^n (x_i-\overline{x})^2\sigma^2\\\\
		      &=& \dfrac{\sigma^2}{nS_{xx}}.
	\end{array}
	$$
\end{teo}

%-------------------- Teorema 2.4
\begin{teo}[\boldmath El estimador de mínimos cuadrados $b_1$ e $\overline{y}$ no están correlacionados] Bajo el supuesto de normalidad de $y_i$ para $i = 1, 2, \ldots , n$, $b_1$ e $\overline{y}$ se distribuyen normalmente y son independientes.\\\\
    Demostración.-\; 
    $$
    \begin{array}{rcl}
	\Cov(b_1,\overline{y}) &=& \Cov\left(\dfrac{S_{xy}}{S_{xx}},\overline{y}\right)\\\\
			       &=& \dfrac{1}{S_{xx}}\Cov(S_{xy},\overline{y})\\\\
			       &=& \dfrac{1}{nS_{xx}}\Cov\left[\displaystyle\sum_{i=1}^n(x_i-\overline{x})(y_i-\overline{y}),\overline{y}\right]\\\\
			       &=& \dfrac{1}{n^2S_{xx}}\Cov\left[\displaystyle\sum_{i=1}^n (x_i-\overline{x})y_i,\displaystyle\sum_{i=1}^n y_i\right]\\\\
			       &=& \dfrac{1}{n^2S_{xx}}\displaystyle\sum_{i=1}^n (x_i-\overline{x})\Cov(y_i,y_j)\\\\
    \end{array}
    $$
    Notemos que $E(\epsilon_i)=0$ y $\epsilon_i$ son independientes. De donde por definición de covarianza, podemos escribir
    $$\Cov(y_i,y_j)=E\left\{[y_i-E(y_j)][y_j-E(y_j)]\right\}=E(\epsilon_i,\epsilon_j)=
    \left\{
	\begin{array}{rcl}
	    \sigma^2 &si& i=j\\\\
	    0 &si& i\neq j.
	\end{array}
    \right.
    $$
    Concluimos que
    $$\Cov(b_1,\overline{y})=\dfrac{1}{n^2}{S_{xx}}\sum_{i=1}^n (x_i-\overline{x})\sigma^2=0.$$
    Recuerde que la correlación cero es equivalente a la independencia entre dos variables normales. Por lo tanto, concluimos que $b_1$ e $\overline{y}$ son independientes.
\end{teo}

%-------------------- Teorema 2.5
\begin{teo}
    $\Var(b_0) = \left(\dfrac{1}{n}+\dfrac{\overline{x}^2}{nS_{xx}}\right)\sigma^2$.\\\\
    Demostración.-\; Sea, $X_1,X_2,\ldots X_n$ IDD, y  $\E(X)=\mu$ y $\Var(X)=\sigma^2$. Entonces, la media muestral $\overline{X}$ es normal con media $\mu$ y varianza $\dfrac{\sigma^2}{n}.$ Por lo tanto,
	$$
	\begin{array}{rcl}
	    \Var(b_0) &=& \Var(\overline{y}-b_1\overline{x})\\\\
		      &=& \Var(\overline{y})+(\overline{x})^2\Var(b_1)\\\\
		      &=& \dfrac{\sigma^2}{n}+ \overline{x}^2 \dfrac{\sigma^2}{nS_{xx}}\\\\
		      &=& \left(\dfrac{1}{n}+\dfrac{\overline{x}^2}{nS_{xx}}\right)\sigma^2
	\end{array}
	$$
\end{teo}

Las varianzas de $b_0$ y $b_1$ son importantes cuando queremos hacer inferencias estdísticas sobre la intersección y la pendiente de la regresión.\\

Dado que las varianzas de los estimadores de mínimos cuadrados $b_0$ y $b_1$ involucran la varianza del término de error en el modelo de regresión simple. Esta variación de error es desconocida para nosotros. Por lo tanto, necesitamos estimarlo. Ahora discutimos cómo estimar la varianza del término de error en el modelo de regresión lineal simple. Sea $y_i$ la variable de respuesta observada y $\hat{y}_i = b_0 + b_1x_i$ , el valor ajustado de la respuesta. Tanto $y_i$ como $\hat{y}_i$ están disponibles para nosotros. El verdadero error $\sigma_i$ en el modelo no es observable y nos gustaría estimarlo. La cantidad $y_i - \hat{y}_i$ es la versión empírica del error $\epsilon_i$ . Esta diferencia es un residuo de regresión que juega un papel importante en el diagnóstico del modelo de regresión. Proponemos la siguiente estimación de la varianza del error basada en $e_i$:
$$s^2 = \dfrac{1}{n-2}\sum_{i=1}^n \left(y_i-\hat{y}_i\right)^2$$
Tenga en cuenta que en el denominador es $n-2$. Esto hace que $s^2$ sea un estimador insesgado de la varianza del error $\sigma_2$. El modelo lineal tiene dos parámetros, por lo que, $n-2$ puede verse como $n-$ números de parámetros simples. En particular, en un modelo de regresión lineal múltiple con $p$ parámetros, el denominador debe ser $n - p$ para construir un estimador insesgado de la varianza del error $\sigma^2$.\\

El estimador insesgado $s^2$ para la regresión lineal simple será  demostrado en las siguientes derivaciones.

$$y_i-\hat{y}_i=y_i-b_0-b_ix_i=y_i-(\overline{y}-b_i\overline{x})-b_ix_i$$

Estamos suponiendo que $E(\epsilon)=0$; de lo que se sigue,

$$\sum_{i=1}^n\left(y_i-\hat{y}_i\right)=\sum_{i=1}^n \left(y_i-\overline{y}\right)=\sum_{i=1}^n \left(y_i-\overline{y}\right)-b_i\sum_{i=1}^n\left(x_i-\overline{x}\right)=0.$$

Demostremos este supuesto. Note que $\left(y_i-\hat{y}_i\right)x_i=\left[\left(y_i-\overline{y}\right)-b_i\left(x_i-\overline{x}\right)\right]x_i$, de donde
$$
\begin{array}[t]{rcl}
    \displaystyle\sum_{i=1}^n\left(y_i-\hat{y}_i\right)x_i &=& \displaystyle\sum_{i=1}^n\left[\left(y_i-\overline{y}\right)x_i-b_i\left(x_i-\overline{x}\right)\right]x_i\\\\
    \displaystyle\sum_{i=1}^n\left(y_i-\hat{y}_i\right) &=& \displaystyle\sum_{i=1}^n\left[\left(y_i-\overline{y}\right)x_i-b_i\left(x_i-\overline{x}\right)\right]\left(x_i-\overline{x}\right)\\\\
							&=& \displaystyle\sum_{i=1}^n\left(y_i-\overline{y}\right)\left(x_i-\overline{x}\right)-b_i\sum_{i=1}^n\left(x_i-\overline{x}\right)^2\\\\
							&=& n\left(S_{xy}-b_1S_{xx}\right)\\\\
							&=&n\left(S_{xy}-\dfrac{S_{xy}}{S_{xx}}S_{xx}\right)\\\\
							&=& 0.
\end{array}
$$

Para demostrar que $s^2$ es un estimador insesgado de la varianza del error, primero veamos que

$$\left(y_i-\hat{y}_i\right)^2=\left[\left(y_i-\overline{y}\right)-b_i\left(x_i-\overline{x}\right)\right]^2,$$

Por lo que

$$
\begin{array}{rcl}
    \displaystyle\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2 &=& \displaystyle\sum_{i=1}^n\left[\left(y_i-\overline{y}\right)-b_i\left(x_i-\overline{x}\right)\right]^2\\\\
							  &=& \displaystyle\sum_{i=1}^n\left(y_i-\overline{y}\right)^2-2b_i\displaystyle\sum_{i=1}^n\left(x_i-\overline{x}\right)\left(y_i-\overline{y}\right)+b_i^2\displaystyle\sum_{i=1}^n\left(x_i-\overline{x}\right)^2\\\\
							  &=& \displaystyle\sum_{i=1}^n\left(y_i-\overline{y}\right)^2-2nb_iS_{xy}+n b_i^2S_{xx}\\\\
							  &=& \displaystyle\sum_{i=1}^n\left(y_i-\overline{y}\right)^2-2n\dfrac{S_{xy}}{S_{xx}}S_{xy}+n\dfrac{S_{xy}^2}{S_{xx}^2}S_{xx}\\\\
							  &=& \displaystyle\sum_{i=1}^n\left(y_i-\overline{y}\right)^2-n\dfrac{S_{xy}^2}{S_{xx}}\\\\
\end{array}
$$

Después, ya que 

$$
\begin{array}{rcl}
    \left(y_i-\overline{y}\right)^2&=&\left[\beta_0+\beta_1x_i+\epsilon -\left(\beta_0+\beta_1\overline{x}+\overline{\epsilon}\right)\right]^2\\\\
				   &=&\left[\beta_1\left(x_i-\overline{x}\right)+\left(\epsilon_i-\overline{\epsilon}\right)\right]^2\\\\
				   &=& \beta_1^2\left(x_i-\overline{x}\right)^2+\left(\epsilon_i-\overline{\epsilon}\right)^2+2\beta_1\left(x_i-\overline{x}\right)\left(\epsilon_i-\overline{\epsilon}\right)\\\\
\end{array}
$$

Entonces, en vista que $E\left(\epsilon_i-\overline{\epsilon}\right)=0$ tenemos 

$$
\begin{array}{rcl}
    \E\left(y_i-\overline{y}\right)^2 &=& \beta_1^2\left(x_i-\overline{x}\right)^2+\E\left(\epsilon_i-\overline{\epsilon}\right)^2\\\\
				      &=& \beta_1^2\left(x_i-\overline{x}\right)^2 + \E\left(\epsilon_i-\overline{\epsilon}\right)^2 - \E^2\left(\epsilon_i-\overline{\epsilon}\right) + \E^2\left(\epsilon_i-\overline{\epsilon}\right)\\\\
				      &=& \beta_1^2\left(x_i-\overline{x}\right)^2 + \Var\left(\epsilon_i-\overline{\epsilon}\right)\\\\
				      &=& \beta_1^2\left(x_i-\overline{x}\right)^2 + \Var(\epsilon_i)+\Var(\overline{\epsilon})-2\Cov\left(\epsilon_i,\overline{\epsilon}\right)\\\\
				      &=& \beta_1^2\left(x_i-\overline{x}\right)^2 + \sigma^2+\dfrac{\sigma^2}{n}-2\dfrac{\sigma^2}{n}\\\\
				      &=& \beta_1^2\left(x_i-\overline{x}\right)^2+\dfrac{n-1}{n}\sigma^2,
\end{array}
$$

y

$$
\begin{array}{rcl}
    \displaystyle\sum_{i=1}^n \E\left(y_i-\overline{y}\right)^2 &=& n\beta_1^2 S_{xx} + \displaystyle\sum_{i=1}^n \dfrac{n-1}{n}\sigma^2\\\\
								&=& n\beta_1^2 S_{xx}+(n-1)\sigma^2.
\end{array}
$$

Ademàs, se tiene

$$
\begin{array}{rcl}
    \E\left(S_{xy}\right) &=& \E\left[\dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)\left(y_i-\overline{y}\right)\right]\\\\
			  &=& \dfrac{1}{n}\E\left[\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)y_i\right]\\\\
			  &=& \dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)\E\left(y_i\right)\\\\
			  &=& \dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)\left(\beta_0+\beta_1x_i\right)\\\\
			  &=& \dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)\beta_0+\dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)\beta_1x_i\\\\
			  &=& \dfrac{1}{n}\beta_{0}\displaystyle\sum_{i=1}^n x_i-\dfrac{1}{n}\beta_0\displaystyle\sum_{i=1}^n x_i+\dfrac{1}{n}\beta_1\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)x_i\\\\
			  &=& \dfrac{1}{n}\beta_{0}\left[\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)x_i-\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)\overline{x}\right]\\\\
			  &=& \dfrac{1}{n}\beta_{0}\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)^2\\\\
			  &=& \dfrac{1}{n}\beta_{0}S_{xx}\\\\
\end{array}
$$

También; sea $X_1,X_2,\ldots,X_n$ IDD, con $Var(X_i)=\sigma_i^2$ para $i=1,2,\ldots,n$. Si $\displaystyle\sum_{i=1}^n a_iX_i.$ Entonces, 
	$$\Var\left(\displaystyle\sum_{i=1}^n a_iX_i\right)=\displaystyle\sum_{i=1}^n a_i^2\sigma_i^2.$$ 
	Por lo tanto,

$$
\begin{array}{rcl}
    \Var\left(S_{xy}\right)&=&\Var\left[\dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)y_i\right]\\\\
			   &=& \dfrac{1}{n^2}\displaystyle\sum_{i=1}^n\left(x_i-\overline{x}\right)^2\Var\left(y_i\right)\\\\
			   &=& \dfrac{1}{n}S_{xx}\sigma^2.
\end{array}
$$

Así, podemos escribir

$$
\begin{array}{rcl}
    \Var\left(S_{xy}^2\right) &=& \E\left(S_{xy}^2\right)-E^2\left(S_{xy}\right)\\\\
    \E\left(S_{xy}^2\right) &=& \Var\left(S_{xy}\right)+E^2\left(S_{xy}\right)\\\\
			    &=& \dfrac{1}{n}S_{xx}\sigma^2+\beta_{1}^2 S^2_{xx}.

\end{array}
$$

y

$$
\E\left(S_{xy}^2\right)=\dfrac{1}{n}S_{xx}\left(\sigma^2+n\beta_{1}^2 S_{xx}\right)
$$

Dado que $E\left(S_{xx}\right)=S_{xx}$, entonces

$$
\E\left(\dfrac{nS_{xy}^2}{S_{XX}}\right)=\sigma^2+n\beta_{1}^2 S_{xx}.
$$

Finalmente, $\E\left(\hat{\sigma}^2\right)$ es dado por:

$$
\begin{array}{rcl}
    E\left[\displaystyle\sum_{i=1}^n \left(y_i-\hat{y}\right)^2\right] &=& \E\left[\displaystyle\sum_{i=1}^n\left(y_i-\overline{y}\right)^2\right]-\E\left[n\dfrac{S_{xy}^2}{S_{xx}}\right]\\\\
    &=&n\beta_1^2 S_{xx}+(n-1)\sigma^2-n\beta_1^2S_{xx}-\sigma^2\\\\
								      &=&(n-2)\sigma^2.\\\\
\end{array}
$$

En otras palabras, probamos que

\begin{tcolorbox}
    $$\E\left(s^2\right)=\E\left[\dfrac{1}{n-2}\sum_{i=1}^2\left(y_i-\hat{y}\right)^2\right]=\sigma^2.$$
\end{tcolorbox}

Por lo tanto, $s^2$, la estimación de la varianza del error, es un estimador insesgado de la varianza del error $\sigma^2$ en la regresión lineal simple. Otra vista de elegir $n - 2$ es que en el modelo de regresión lineal simple hay $n$ observaciones y dos restricciones sobre estas observaciones:

\begin{enumerate}
    \item $\displaystyle\sum_{i=1}^n \left(y_i-\hat{y}\right)=0$,\\\\
    \item $\displaystyle\sum_{i=1}^n \left(y_i-\hat{y}\right)x_i=0$.\\\\
\end{enumerate}

Por lo tanto, la estimación de la varianza del error tiene $n - 2$ grados de libertad, que también es el número total de observaciones - el número total de parámetros en el modelo. Veremos características similares en la regresión lineal múltiple.


\section{Estimación de máxima verosimilitud}


