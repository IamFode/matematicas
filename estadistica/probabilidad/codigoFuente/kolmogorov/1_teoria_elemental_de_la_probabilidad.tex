\chapter{Teoría elemental de la probabilidad}

%-------------------- axioma 1
\begin{tcolorbox}[colframe=white]
Sea $E$ una colección de elementos $\xi.\eta,\zeta,\ldots,$ que llamaremos sucesos elementales, y $\mathfrak{F}$ un conjunto de subconjuntos de $E$; los elementos del conjunto $\mathfrak{F}$ se llamarán eventos aleatorios.

    %----------I.
    \begin{axioma}
	$\mathfrak{F}$ es un campo de conjuntos. (Un sistema de conjuntos se denomina campo si la suma, el producto y la diferencia de dos conjuntos del sistema también pertenecen al mismo sistema).

    \end{axioma}

    %----------II.
    \begin{axioma}
	$\mathfrak{F}$ contiene el conjunto $E$.
    \end{axioma}

    %----------III.
    \begin{axioma}
	A cada conjunto $A$ en $\mathfrak{F}$ se le asigna un número real no negativo $P(A)$. Este número $P(A)$ se llama probabilidad del evento $A$.
    \end{axioma}

    %----------IV.
    \begin{axioma}
	$P(E)$ es igual a $1$.
    \end{axioma}

    %----------V.
    \begin{axioma}
	Si $A$ y $B$ no tienen ningún elemento en común, entonces $$P(A+B)=P(A)+P(B)$$
    \end{axioma}

\end{tcolorbox}

\setcounter{section}{3}
\section{Corolarios inmediatos de los axiomas; Probabilidades condicionales; teorema de Bayes}
De $A+\overline{A}=E$ y los axiomas IV y V se sigue que, 
\begin{equation}
    P(A)+P(\overline{A})=1
\end{equation}

\begin{equation}
    P(\overline{A} = 1-P(A).
\end{equation}

Ya que $\overline{E}=0$, en particular se tiene, 
\begin{equation}
    P(0)=0.
\end{equation}
\vspace{.2cm}

\begin{tcolorbox}[colframe=white]
Si $A,B,...,N$ son incompatibles, entonces por el Axioma V se sigue la fórmula (\textbf{teorema de la suma}), 
\begin{equation}
    P(A+B+\ldots,+N) = P(A) + P(B) + \ldots + P(N)
\end{equation}
\end{tcolorbox}

\begin{tcolorbox}[colframe=white]
Si $P(A)>0,$ entonces el cociente 
\begin{equation}
    P_A(B)=\dfrac{P(AB)}{P(A)}
\end{equation}
Es definida como la probabilidad condicional del evento $B$ bajo la condición $A$.
\end{tcolorbox}

Luego por (.5) se sigue que, 
\begin{equation}
    P(AB)=P(A)P_A(B).
\end{equation}

\begin{tcolorbox}[colframe=white]
Y por inducción obtenemos la fórmula general (\textbf{el teorema de la multiplicación})
\begin{equation}
    P(A_1, A_2,\ldots,A_n)=P(A_1) P_{A_1}(A_2) P_{A_1 A_2}(A_3) \ldots P_{A,A_2\ldots A_{n-1}}(A_n)
\end{equation}
\end{tcolorbox}

Los siguientes teoremas se siguen fácilmente,

\begin{equation}
    P_A(B)\geq 0,
\end{equation}

\begin{equation}
    P_A(E)=1,
\end{equation}

\begin{equation}
    P_A(B+C)=P_A(B)+P_A(C).
\end{equation}

\begin{equation}
    P_A(A)=1.
\end{equation}

Por (.6) y la fórmula análoga $$P(AB)=P(B)P_B(A)$$
obtenemos la fórmula,

\begin{tcolorbox}[colframe=white]
    \begin{equation}
	P_B(A) = \dfrac{P(A)P_A(B)}{P(B)},
    \end{equation}
    que contiene el esencia el \textbf{teorema de bayes}
\end{tcolorbox}

    \begin{teo}[Teorema de la probabilidad total] Sea $A_1+A_2+\ldots + A_n = E$ (Sea asume que los eventos $A_1,A_2,\ldots,A_n$ son mutuamente excluyentes) y sea $X$ arbitrario. Entonces 
	\begin{equation}
	    P(X)=P(A_1)P_{A_1}(X) + P(A_2)P_{A_2}(X)+\ldots + P(A_n)P_{A_n}(X);
	\end{equation}
	\vspace{.5cm}
	\textbf{Prueba.-}\; Sea $$X=A_1 X + A_2X + \ldots + A_n X;$$
Usando (.4) tenemos, $$P(X)=P(A_1X)+P(A_2X)+\ldots + P(A_nX)$$
y según (.6) tenemos al mismo tiempo
$$P(A_iX)=P(A_i)P_{A_t}(X)$$\\
    \end{teo}

    \begin{teo}[Teorema de Bayes]
	Sea $A_1+A_2+\ldots + A_n = E$ y $X$ sea arbitrario, entonces 
	\begin{equation}
	    P_X(A_i) = \dfrac{P(A_i)P_{A_i}(X)}{P(A_1)P_{A_1}(X) + P(A_2)P_{A_2}(X)+\ldots + P(A_n)P_{A_n}(X)}, \; \; para \;\; i=1,2,3,\ldots,n 
	\end{equation}

	\vspace{.5cm}

	\textbf{Prueba.-}\; De $(.12)$ se tiene, $$P_X(A_i) = \dfrac{P(A_i) P_{A_i}(X)}{P(X)}$$
	Para obtener la fórmula (.14) solo queda sustituir la probabilidad $P(X)$ por su valor derivado de (.13) aplicando el teorema de la probabilidad total.\\\\
    \end{teo}

\section{Independencia}

Pasemos a la definición de la independencia. Dado $n$ experimentos $\mathfrak{U}^{(1)},\mathfrak{U}^{(2)},\ldots,\mathfrak{U}^{(n)},$ es decir, $n$ descomposiciones 
$$E=A_1^{i} +A_2^{i} + \ldots + A_{r_i}^{i}, \qquad i=1,2,\ldots,n$$
del conjunto básico $E$. Entonces es posible asignar $r=r_1 r_2 \ldots r_n$ probabilidades (en el caso general).
$$P_{q_1 q_2 \ldots q_n} = P(A_{q_1}^{1} A_{q_2}^{2} \ldots A_{q_n}^{n}) \geq 0$$
que son completamente arbitrarios excepto por la condición única que,
\begin{equation}
    \sum_{q_1, q_2,\ldots,q_n} P_{q_1 q_2 \ldots q_n} = 1.
\end{equation}

\begin{tcolorbox}[colframe=white]
    \begin{def.} $n$ experimentos $\mathfrak{U}^{(1)}, \mathfrak{U}^{(2)},\ldots,\mathfrak{U}^{(n)}$ son llamados mutuamente independientes, si para cualquier $q_1,q_2,\ldots,q_n$ la siguientes ecuación es cierta
	\begin{equation}
	    P\left(A_{q_1}^{(1)},A_{q_2}^{(2)}\ldots A_{q_n}^{(n)}\right) = P\left(A_{q_1}^{(1)}\right)P_{q_2}\left(A_{q_2}^{(2)}\right)\ldots P\left(A_{q_n}^{(n)}\right). 
	\end{equation}
    \end{def.}
\end{tcolorbox}

%-------------------- TEOREMA 1
\begin{teo}
    Si $n$ experimentos $\mathfrak{U}^{(1)},\mathfrak{U}^{(2)},\ldots,\mathfrak{U}^{(n)},$ son llamados mutuamente independientes, entonces cualquier $m$ $(m<n)$,  $\mathfrak{U}^{(i_1)},\mathfrak{U}^{(i_2)},\ldots,\mathfrak{U}^{(i_m)},$ son también independientes.\\
    En el caso de la independencia tenemos la ecuación:
    \begin{equation}
	P\left(A_{q_1}^{(i_1)} A_{q_2}^{(1_2)} \ldots A_{q_m}^{(i_m)} \right) = P\left(A_{q_1}^{(i_1)}\right)P\left(A_{q_2}^{(i_2)}\right) \ldots P\left(A_{q_m}^{(i_m)}\right)
    \end{equation}
    (para todo $i_k$ diferente).\\\\

    Prueba.-\; Para probar esto es suficiente mostrar que de la independencia mutua de $n$ descomposiciones se sigue la independencia mutua del primer $n — 1$. Supongamos que se cumplen las ecuaciones (2). Luego
    $$P\left(A_{q_1}^{(1)} A_{q_2}^{(2)} \ldots A_{q_{n-1}}^{(n-1)} \right) = \sum\limits_{q_n} P\left(A_{q_1}^{(1)} A_{q_2}^{(2)}\ldots A_{q_n}^{(n)}\right) = P\left(A_{q_1}^{(1)} \right) P\left(A_{q_2}^{(2)}\right)\ldots P\left(A_{q_n-1}^{(n-1)} \right) \sum\limits_{q_n} P\left(A_{q_n}^{(n)}\right)$$ $$ = P\left(A_{q_1}^{(1)} \right) P\left(A_{q_2}^{(2)}\right)\ldots P\left(A_{q_n-1}^{(n-1)} \right)$$\\
\end{teo}

%--------------------definición II
\begin{tcolorbox}[colframe=white]
    \begin{def.} $n$ eventos $A_1,A_2,\ldots A_n$ son mutuamente independientes, si la descomposición
	$$E=A_k+\overline{A}k \qquad (k=1,2,\ldots , n)$$
	son independientes.
    \end{def.}
\end{tcolorbox}

%-------------------- TEOREMA 2
\begin{teo} Una condición necesaria y suficiente para la independencia de los experimentos $\mathcal{U}^{(1)}, \mathcal{U}^{(2)}, \ldots , \mathcal{U}^{(n)}$ en el caso de probabilidades positivas $P\left(A_q^{(i)}\right)$ es que la probabilidad condicional de los resultados $A_q^{(i)}$ de experimentos $\mathcal{U}^{(i)}$ bajo la hipótesis de que varias otras pruebas $\mathcal{U}^{(i_1)}, \mathcal{U}^(i_2),\ldots, \mathcal{U}^{(i_k)}$ tuvieron resultados definidos $A_{q_1}^{(i_1)},A_{q_2}^{(i_2)},\ldots, A_{q_k}^{(i_k)}$ es igual a la probabilidad absoluta $P(A_q^{(i)})$.\\\\
\end{teo}

%-------------------- TEOREMA 3
\begin{teo} Si todas las probabilidades $P(A_k)$ son positivas, entonces una condición necesaria y suficiente para la independencia mutua de los eventos $A_1,A_2, \ldots, A_n$ satisface la ecuación,
    $$P_{A_{i_1},A_{i_2},\ldots A_{i_k}}(A_i) = P(A_i)$$
    para cualquier par diferente $i_1,i_2,\ldots. i_k,i$.\\\\
\end{teo}

\section{Probabilidades condicionales como variables aleatorias, cadenas de Markov}
