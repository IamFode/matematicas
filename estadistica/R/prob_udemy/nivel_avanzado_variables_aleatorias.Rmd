---
title: 'Nivel avanzado: Variables aleatorias. Complementos'
author: "Christian Limbert Paredes Aguilera"
date: "25/1/2022"
output: html_document
---

# Momentos de orden n
Sea $X$ una variable aleatoria. Definimos el momento de orden $n$ como $m_n = E(X^n)$

El momento de orden $1$ de una variable aleatorio es su valor medio o $E(X).$

Los momentos de orden $n$ caracterizan una variable $X$. O sea, que si conocemos todos los momentos de orden $n$, podemos deducir cuál es la distribución de $X$.

## Momoentos centrales de orden n de una variable aleatoria
Sea $X$ una variable aleatoria. Definimos el momento central de orden $n$ como $\mu_n = E[(X--\mu)^n]$, donde $\mu=E(X)$ es la media o esperanza de la variables aleatoria $X$.

El momento central de orden 1 de una v.a. es siempre 0.
$$\mu_1 = E_1(X-\mu) = E(X)-E(\mu)=E(X)-E(X)=0$$

El momento central de orden 2 de una v.a. es la varianza
$$\mu_2 = E(X-\mu)^2 = Var(X)$$

Los momentos centrales de orden $n$ caracterizan también una variable $X$. O sea, que si conocemos todos los momentos centrales de orden $n$, podemos deducir cuál es la distribución de $X$.

#### Propocisión 
La relación que hay entre los momentos centrales y los momentos de una v.a. es la siguiente:
$$\mu_n = \sum_{k=0}^n (-1)^{n-k}{n\choose k} \mu^{n-k}m_k = \sum_{k=0}^n (-1)^k{n \choose k}\mu^k m_{n-k}$$
donde $\mu=E(X)$ recordemos que es la esperanza de la v.a. X.

Demostración.-\; Recordemos la definición de momentos centrales de orden $n$ y desarrollemos aplicando el binomio de Newton:
$$\mu_n = E(X-\mu) = E\left(\sum_{k=0}^n (-1)^{n-k}{n \choose k} x^k \mu^{n-k}\right)$$

La esperanza de la suma es la suma de esperanzas, obtenemos la expresión dada por la proposición:
$$\mu_n = \sum_{k=0}^n (-1)^{n-k} E(X^k)=\sum_{k=0}^n (-1)^{n-k}{n\choose k}\mu^{n-k}m_k$$

## Asimetría de una variable aleatoria
Una variable aleatoria tiene asimetría positiva si su función de densidad o de probabilidad presenta una cola a la derecha y asimetría negativa, si su función de densidad o de probabilidad presenta cola a la izquierda.

### ¿Cómo calcular la asimetría de una variable aleatoria?

#### Coeficiente de asimetría de Pearson
La simetría de una variable aleatoria $X$ se calcula a partir de sus momentos centrales de segundo y tercer orden:
$$\gamma = E \left(\dfrac{X-\mu}{\sigma} \right)^3 = \dfrac{\mu_3}{\sigma^3}$$

donde $\mu=E(X)$ y $\sigma^2 = Var(X)$

### Coeficiente de asimetría
$$\gamma_1 = \dfrac{m_3 - 3\mu\sigma^2 - \mu^3}{\sigma^3}$$

Por tanto, una variable aleatoria $X$ tendrá simetría positiva o a la derecha si   $\gamma_1>0$ y tendrá asimetría negativa o a la izquierda, si $\gamma_1<0$.

## Curtosis o apuntamiento de una variable aleatoria

### Medida de curtosis de Pearson
La manera estándard de medir la curtosis de una variable aleatoria $X$ es a partir de su momento central de cuarto orden:
$$\gamma^2 = E\left(\dfrac{X-\mu}{\sigma}\right)^4=\dfrac{\mu_4}{\sigma^4}$$


-  Diremos que una variable aleatoria no tiene exceso de curtosis o mesocúrtica si $\gamma_2=3.$

- Diremos que una variable aleatoria tiene exceso positivo de curtosis o leptocúrtica si $\gamma_2>3.$

- Diremos que una variable aleatoria tiene exceso negativo de curtosis o platicúrtica si $\gamma_2<3.$


# Función generadora de momentos
Sea $X$ una v.a. $X$ con función de probabilidad $P_X$ en el caso discreto o función de densidad $f_X$ en el caso continuo.

Sea $t\in \mathbb{R}$ un valor real cualquiera. Definimos la función generatriz de momentos $m_X(t)$ en el valor $t$ como:
$$m_X(t)=E(e^{tX})$$

## Cómo calcular los momentos a partir de la función generatriz
### Relación entre la función generatriz de momentos y los momentos

#### Proposición 
Sean $X$ una variable aleatoria con función generatriz de momentos $m_x(t)$. Entonces, el momento de orden $n$ de $X$ se puede obtener de la forma siguiente:
$$m_n = E(X^n) = \dfrac{d}{dt^n}m_X(t)\bigg|_{t=0} = m_X^{(n)}(0).$$

O sea, el momento de orden $n$ de $X$ es la derivada n-ésima de la función generatriz de momentos evaluada en $t=0$.

Demostración.- Recordemos la definición de la función generatriz de momentos: $$M_X(t)=E(e^{tX}).$$

la idea de la demostración es probar por inducción que $m_X^{(n)}(t)$

Veámoslo para $n=1$; $m^{'}_X(t) = E(e^{tX}X)$

Seguidamente, apliquemos inducción sobre $n$. Supongamos que $m_X^{(n)}(t)=E(e^{tX}X^n)$ y veamos que $m_X^{n+1}(t)=E(e^{tX}X^{n+1}): m_X^{n+1}(t) = \dfrac{d}{dt}(m_X^{(n)}X^n) = E(e^{tX}X^{n+1})$ tal como queriamos demostar.

Por último apliquemos la expresión demostrada $m_X^{(n)}=E(e^{tX}X^n)$ a $t=0$, obtenemos: $m_X^{(n)}(0) = E(X^n) = m_n$ tal como dice la proposición.

## Función característica de una variable aleatoria
### Definición

Sea $X$ una variable aleatoria $X$ con función de probabilidad $P_X$ en el caso discreto o función de densidad $f_X$ en el caso continuo.

Sea $w\in \mathbb{R}$ un valor real cualquiera.

Definimos la función característica $\phi_X(w)$ en el valor $w$ como:

$\phi_X(w) = E(e^{iwX})$, donde $i$ es el número complejo $i=\sqrt{-1}$

#### Observaciones
Si $X$ es una variable aleatoria continua, la función caracteristica $\phi_X(w)$ puede interpretarse como la transformada de Fourier de la función de densidad de $X$:
$$\phi(w) = \int_{-\infty}^\infty f_X(x)e^{iwx}\; dx$$

Por lo tanto, usando la fórmula de la antitransformada de Fourier, podemos escribir la función de densidad $f_X(x)$ como función de la función característica de $X$

$$\phi_X(x): \; f_X(x) = \dfrac{1}{2\pi}\int_{-\infty}^{\infty}\phi_X(w)e^{-iwx}\; dw$$

En el caso discreto, o sea, si $X$ es una variable discreta, la función característica $\phi_X(w)$ se escribe como función de la función de probabilidad $P_X(x_k)$ con dominio $D_X = \lbrace x_k,k\rbrace$ como: $\phi(w)\sum_k P_X(x_k)e^{iwk}.$

En los casos en que los $x_k$ sean enteros, $x_k=k$, que son la mayoría, la ecuación anterior en la transformada de Fourier de la secuencia $P_X(k)$. Dicha función es una función periódica en $w$ de periodo $2\pi$ ya que $e^{i(w+2\pi)k}=e^{iwk}.$

Por tanto, usando la fórmula de inversión, podemos escribir la función de probabilidad $P_X(k)$ como función de la función característica de $X$, $\phi(w)$:
$$P_X(k)=\dfrac{1}{2\pi}\int_0^{2\pi}\phi(w)e^{-iwk}\; dw$$

## Cómo calcular los momentos a partir de una función característica.
La relación entre la función característica y los momentos es la siguiente:

Proposición.- Sea $X$ una variable aleatoria con función característica $\phi_X(w)$. Entonces, el momento de orden $n$ de $X$ se puede obtener de la forma siguiente:
$$m_n = E(X^n) = \dfrac{1}{i^n}\dfrac{d}{dw^n}\phi(w)\bigg|_{w=0} = \dfrac{1}{i^ n} \phi_X^{(n)}(0)$$

Osea, el momento de orden $n$ de $X$ es la derivada n-ésima de la función característica evaluada en $w=0$ dividido por $i^n$ 

## Generación de variables aleatorias con el método de la transformación
Todos los métodos que vamos a describir presuponen que podemos generar números aleatorios que se distribuyen uniformemente entre $0$ y $1$. En R se puede hacer uso con la función $runif(n),$ donde $n$ es la cantidad de números aleatorios entre $0$ y $1$ a generar.

### Método de transformación 
Sea $X$ una variable aleatoria con función de distribución $F_X(x)$. Supongamos que $F_X(x)$ es estrictamente creciente o que existe $F_X^{-1}(y)$, para todo $y\in [0,1]$. Sea $Y$ la variable aleatoria definida como: $Y=F_X(X)$. Entonces la distribución de $Y$ es uniforme en el intervalo $[0,1]$.

Demostración.- Claramente, por propia definición de $Y$, tenemos que el dominio de $Y$ es $[0,1]$ ya que el conjunto recorrido de la función de distribución de cualquier variable es el intervalo $[0,1]$.

Para ver que la distribución de $Y$ es $U[0,1]$ basta comprobar que $F_Y(y)=y,$ para todo $y\in[0,1]$.

$$F_Y(y)=P(PY\leq y) = P(F_X(X)\leq y)=P(X\leq F_X^{-1}(y))=F_X(F_X^{-1}(y))=y.$$

#### Ejemplo 1
Recordemos que si $X$ es exponencial de parámetro $\lambda$, su función de distribución es: $F_X(x)=1-e^{-\lambda x}$.

Hallaremos a continuación $F_X^{-1}$:

$$y=1-e^{-\lambda x} \; \Longleftrightarrow\; 1-y=e^{-\lambda x}, \; \Longleftrightarrow \; ln(1-y)=-\lambda x, \; \Longleftrightarrow \; x=-\dfrac{1}{\lambda}ln(1-y)$$

por lo tanto, $F_X^{-1}(y)=-\dfrac{1}{\lambda}ln(1-y)$

#### Ejemplo 1 en R
Generar una muestra con $R$ de $25$ valores de una variable exponencial de parámetro $\lambda=2$ usando el método anterior:

```{r}
n = 25
lambda = 2 
muestra_y = runif(n)
muestra_x = -(1/lambda)*log(1-muestra_y)
muestra_x
```

#### Ejemplo 2 en R
Generamos una muestra de 500 valores usando el método de transformación y dibujaremos su histograma de frecuencias relativas.

Seguidamente dibujaremos la función de denisdad de la variable exponencial de parámetro $\lambda$ y compararemos los resultados:

```{r}
{n=500
lambda = 2 
muestra_y = runif(n)
muestra_x = -(1/lambda)*log(1-muestra_y)
hist(muestra_x,freq = FALSE,main = "Histograma de la muestra")
x2=seq(from=0,to=2.5,by=0.01)
lines(x2,dexp(x2,lambda),col="red")}
```


### Método de rechazo
Sea $X$ una variable aleatoria continua tal que su función de densidad verifica:

- Existen valores $a$ y $b$ tal que $f_x(x)=0$ si $x\notin [a,b]$
- Existen valores $c$ y $d$ tal que $f_X(x)\in [c,d]$, si $x\in [a,b]$

En resumen, los puntos $(x,f(x))$ pertenecen al rectángulo $[a,b]\times [c,d]$ y en caso contrario $f_X(x)=0$.

Para generar una muestra aleatoria de la variable $X$, hacemos lo siguiente: 

1. Generamos un valor aleatorio $x$ entre $a$ y $b$.
2. Generamos un valor aleatorio $y$ entre $c$ y $d$.
3. Si $y\leq f_X(x)$, aceptamos $x$ como valor de la muestra. En caso contrario, volvemos a empezar en $1$.

#### Ejemplo 1 en R
Sea 
$$f_X(x)=\left\{\begin{array}{rcl}x,&si&0\leq x \leq 1 \\ 2-x,&si&1\leq x \leq 2\\ 0, & & \mbox{en caso contrario}\end{array}\right.$$

```{r}
{a=0;b=2; c=0; d=1; n=25; i=1;
f = function(x){
  ifelse(x>=0 & x<=1, x, ifelse(x>=1 & x<=2, 2-x, 0))
}
muestra = c()
while(i<=n){
  x=runif(1,a,b)
  y=runif(1,c,d)
  if(y<=f(x)){
    muestra=c(muestra,x);
    i=i+1
  }
}
muestra}
```

#### Ejemplo 2 en R
```{r}
{a=0;b=2; c=0; d=1; n=500; i=1;
f = function(x){
  ifelse(x>=0 & x<=1, x, ifelse(x>=1 & x<=2, 2-x, 0))
}
muestra = c()
while(i<=n){
  x=runif(1,a,b)
  y=runif(1,c,d)
  if(y<=f(x)){
    muestra=c(muestra,x);
    i=i+1
  }
}
muestra
hist(muestra,freq = FALSE, main = "Histograma de la muestra")
x2 = seq(from=0,to=2,by=0.01)
lines(x2,f(x2),col="red")}
```

## Entropía
La entropía es una medida de la incertidumbre en un experimento aleatorio.

Veremos cómo la entropía cuantifica la incertidumbre por la cantidad de de información requerida para especificar el resultado de un experimento aleatorio.

Supongamos que tenemos una variable aleatoria $X$ discreta con valores enteros:
$$D_X=\lbrace 1,2,\ldots,N \rbrace$$

Sea $k\in D_X$ un valor de la variable. Estamos interesados en cuantificar la invertidumbre del suceso $A_k=\lbrace X=k \rbrace$.

O sea, cuánta menos incertidumbre tenga $A_k$, más alta será su probabilidad, y cuánta más incertidumbre, menos probabilidad de aparecer $A_k.$

Una medida que cumple las condiciones anteriores es la siguiente:
$$I(A_k) = I(\lbrace X= k\rbrace)=ln\left(\dfrac{1}{P(X=k)}\right)=-ln(P(X=k)).$$

Por ejemplo , si $P(A_k)=1,$ o sea, $A_k$ aparece seguro, entonces tiene incertidumbre nula, $I(A_k)=0,$ y si $P(A_k)=0,$ o sea, $A_k$ no aparece nunca, tiene invertidumbre máxima, $I(A_k)=\infty$.

### Definición de entropía de una variable aleatoria
Sea $X$ una variable aleatoria con función de densidad $f_X(x)$ en el caso continuo o función de probabilidad $P_X(x)$ en el caso discreto. Definimos entropía de $X$ como: $H_X=E(-ln(f_X)) = \int_{-\infty}^{\infty}-ln(f_X(x))f_X(x)\; dx,$ en el caso continuo y $H_X=E(-ln(P_X)) = \sum_{x_K \in D_X}-ln(P_X(x_k))P_X(x_k)$, en el caso discreto.

