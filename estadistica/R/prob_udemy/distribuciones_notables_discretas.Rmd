---
title: "Distribuciones notables discretas"
author: "Christian Limbert Paredes Aguilera"
date: "23/12/2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

#library
source("funciones.R")

# Distribución Bernoulli
\begin{itemize}
  \item Consideremos un experemento con dos resultados posibles éxito (E) y fracaso (F). El espacio de sucesos será $\Omega = \lbrace E,F\rbrace$
  \item Supongamos que la probabilidad de éxito es $P(E) = p,$ y naturalmente $P(E)=1-p=q$ con $0<p<1$.
  \item Consideremos la aplicación $$X:\Omega = \lbrace E,F\rbrace \longrightarrow \mathbb{R}$$
  definida por $$E(X)=1,\quad X(F)=0$$
\end{itemize}

Su función de probabilidad es 
$$P_X(x) = \left\{\begin{array}{rcl}
  1-p=q&si&x=0\\
  p&si&x=1\\
  0&si&\mbox{en cualquier otro caso}\\
\end{array}\right.$$

Su función de distribución es 

$$P_X(x) = \left\{\begin{array}{rcl}
  0&si&x<0\\
  1-p=q&si&0\leq x<1\\
  1&si&1\leq x\\
\end{array}\right.$$

\begin{itemize}
  \item Lo denotaremos por $$X\equiv Ber(p) \quad o\quad X\equiv B(1,p)$$
  \item a este tipo de experimentos (éxito/fracaso) se les denomina experimentos Bernoulli.
\end{itemize}

### Esperanza de una v.a. X Ber(p)
Su valor esperado es $$E(X) = \sum_{x=0}^1 x\cdot P(X=x) = 0\cdot(1-p)+1\cdot p = p$$

Calculemos también $E(X^2)$
$$E(X^2) = \sum_{x=o}^1 x\cdot P(X=x) = 0^2 \cdot (1-p) + 1^2\cdot p = p$$

### Varianza de una v.a. X Ber(p)
Su varianza es $$Var(X) = E(X^2) - E^2(X) = p-p^2 = p\cdot (1-p) = p\cdot q$$

Su desviación típica es $$\sqrt{Var(X)}=\sqrt{p\cdot (1-p)}$$

```{r}
#función de probabilidad
dbinom(0, size = 1, prob = 0.25)
```

```{r}
#función de probabilidad
dbinom(1, size = 1, prob = 0.25)
```

```{r}
# muestra aleatoria simple de probabilidad Bernoulli
rbinom(n = 20, size = 1, prob = 0.25)
```

```{r}
# gráfico de la función de probabilidad y la de distribución de una Ber(p=0.25)
{par(mfrow=c(1,2))
plot(x=c(0,1),y=dbinom(c(0,1),size=1,prob=0.25),
     ylim=c(0,1),xlim=c(-1,2),xlab="x",
     main="Función de probabilidad\n Ber(p=0.25)")
lines(x=c(0,0,1,1),y=c(0,0.75,0,0.25), type = "h", lty = 2,col="blue")
curve(pbinom(x,size=1,prob=0.25),
      xlim=c(-1,2),col="blue",
      main="Función de distribución\n Ber(p=0.25)")
par(mfrow=c(1,1))}
```

# Distribución binomial

Si repetimos $n$ veces de forma independiente un experimento Bernoulli de parámetro $p$.
Se conoce a priori cuantas veces se tirará la moneda.

El espacio muestral $\Omega$ estará formado por cadenas $E^{'} s$ y $F^{s}$ de longitud $n$ consideremos la v.a.
$$X(EFF\ldots EEF) \; = \; \mbox{número de éxitos en la cadena}$$

### Función de probabilidad de una binomial

$$P_X(x)\left\{\begin{array}{rcl}
  {n \choose x}\cdot p^x \cdot (1-p)^{n-x}&si&x=0,1,\ldots,n\\
  0&si&\mbox{en otro caso}\\
\end{array}\right.$$

### Función de distribución

Su función de distribución no tiene una fórmula cerrada. Hay que acumular la función de probabilidad:
$$F_X(x) = P(X\leq x) = \sum_{i=0}^x P_X(i)$$

$$P_X(x)\left\{\begin{array}{rcl}
  0&si&\\
  \sum_{i=1}^k {n \choose i} \cdot p^i \cdot (1-p)^{n-i}&si&k\leq x < k+1 \quad y \quad k=0,1,\ldots, n\\
  1&si&n\leq x\\
\end{array}\right.$$

```{r}
# Cuantas maneras distintas hay para elegir 5 jugadores en un conjunto de 6 jugadores. Todo el múndo diría 6!!!!. Efectivamente con R es
choose(6,5)
```

```{r}
# con 10 jugadores el número de equipos de 5 distintos es bastante más grande
choose(10,5)
```

```{r}
# Y por ejemplo con un equipo de fútbol profesional que tiene en plantilla 22 jugadores (quitamos el portero) se puede formar nada menos que:
choose(22,10)
```
Se denotará por 
$$X\equiv B(n,p)$$

Obviamente se tiene que una bernoulli es una binomial con $n=1$
$$B(1,p) \equiv Ber(p)$$

### Esperanza de un X B(n,p)
Su esperanza es
$$E(X) = \sum_{k=0}^n k\cdot {n \choose k}\cdot p^k \cdot q^{n-k} = n\cdot p$$

La esperanza de $X^2$ es
$$E(X^2) = \sum_{k=0}^n k^2\cdot {n\choose k}\cdot p^k \cdot q^{n-k} = n\cdot p\cdot q - (n\cdot p)^2$$

### Varianza de una X B(n,p)
Su varianza es 
$$Var(X) = E(X^2) - E^2(X) = n\cdot p\cdot q = n\cdot p \cdot (1-p)$$

Su desviación típica es 
$$\sqrt{n\cdot p \cdot q} = \sqrt{n\cdot p \cdot (1-p)}$$

## La distribución Binomila en Python y R
### Caáculos binomila con R
Veamos los cálculos con funciones de R para una v.a. X con distribución binomial B(n=10,p=0.25)

Si queremos calculo con R algún valor de la función de distribución como por ejemplo $F_X(0) = P(X\leq 0)$

```{r}
pbinom(0,size = 10, prob = 0.25)
```

si queremos por ejemplo $F_X(4) = P(X\leq 4)$

```{r}
pbinom(4, size = 10, prob = 0.25)
```

Sin embargo, si queremos calcular algún valor de la función de probabilidad como por ejemplo $P(X=0)$

```{r}
dbinom(0,size=10, prob = 0.25)
```

o por ejemplo para $P(X=4)$
```{r}
dbinom(4, size=10, prob = 0.25)
```

### Generación de muestras aleatorias con R
Generaremos una muestra eleatoria de 100 valores de una población B(20,0.5)

```{r}
# Repetir 100 veces el experimento lanzar una moneda 20 veces y contar el número de caras.
set.seed(2021)
rbinom(100, size = 20, prob = 0.5)
```
## Ejemplo completo de distribución binomial
Tenemos una urna con 100 bolas de las cuales 40 son rojas y 60 blancas. Extraemos al azar una bola, anotamos su color y la devolvemos (la reponemos) a la urna
Supongamos que repetimos este proceso $n=10$ reponiendo en cada ocación la bola extraida.
Consideremos la v.a. $X$ = Número de bolas extraidas con repocisión en $n=10$ repeticiones del mismo experimento Bernoulli.
Bajo estas condiciones tenemos que repetimos $n=10$ veces el mismo experimento Bernoulli con probabilidad de éxito (sacar bola roja) es
$$P(Roja) = P(Exito)=p=\dfrac{40}{100}=0.4$$

Así que la variable $X$ = número de bolas rojas extraídas de la urna con reposición en $n=10$ ocasiones sigue una ley binomial $B(n=10,p=0.4)$


1.¿Cual es la probabilidad de que saquemos exactamente 4 rojas?
  
  Respuesta.- Utilizando la función de probabilidad tenemos que 
  $$P_X(X=4) = {10 \choose 4} \cdot 0.4^4 \cdot (1-0.4)^10.4 = \dfrac{10!}{(10-4)!\cdot 4!}\cdot 0.4^4 \cdot 0.6^6 = 0.2508227$$
  
```{r}
dbinom(4, size = 10, prob = 0.4)
```

2. ¿Cuál es la probabilidad de que saquemos al menos 4 rojas?
  
  Respuesta.- Al menos 4 rojas es $P(X\geq 4) = 1 - P(X<4) = 1 - P(X\leq 3)$
  es decir,
  
  $$\begin{array}{rcl}
    P(X\leq 3)&=&P(X=0) + P(X=1) + P(X=2) + P(X=3)\\\\
    &=&\displaystyle {10\choose 0}\cdot 0.4^0 \cdot (1-0.4)^{10-0} + {10\choose 1}\cdot 0.4^1 \cdot (1-0.4)^{10-1}\\\\ 
    &+& \displaystyle {10\choose 2}\cdot 0.4^2 \cdot (1-0.4)^{10-2} + {10\choose 3}\cdot 0.4^3 \cdot (1-0.4)^{10-3} \\\\
    &=&0.3822806\\\\
  \end{array}$$
  
```{r}
pbinom(3,10,0.4)
```

Así, $P(X\geq 4) = 1-P(X<4) = 1 - P(X\leq 3) = 1 - 0.3822806 = 0.6177194$

```{r}
1-pbinom(3,10,0.4)
```

Aunque en estos casos el parámetro lower.tail = FALSE es sin duda nuestra mejor opción:
```{r}
pbinom(3,10,0.4,lower.tail = FALSE)
```

3.- ¿Cuál es la porbabilidad de que saquemos menos 3 rojas?

4.- ¿Cuál es el valor esperado del número de bolas rojas?

Respuesta.- Como $X$ es una $B(10,0.4)$ sabemos que 
$$E(X) = n\cdot p = 10 \cdot 0.4 = 4$$

5.- ¿Cuál es la deviación típica del número de bolas rojas?
La varianza es 
$$Var(X) = n\cdot p \cdot (1-p) = 10\cdot 0.4 \cdot 0.6 = 2.4$$
Y por lo tanto $\sqrt{Var(X)} = \sqrt{2.4} = 1.5491933$


# Distribución geométrica

\begin{itemize}
  \item Repitamos un experimento Bernoulli de parámetro $p$, de forma independiente hasta obtener el primer éxito.
  \item Sea $X$ la v.a. que cuenta el número de fracasos antes del primer éxito. Por ejemplo que hayamos tenido $x$ fracasos será una cadena de $x$ fracasos culminada con un éxito. Más concretamente
  $$P(FFF, \ldots FE) = P(F)^x \cdot P(E) = (1-p)^x \cdot p = q^x \cdot p$$
  
\end{itemize}

### Distribución geométrica
Su función de probabilidad es
$$PX_(x) = P(X=x) = \left\{\begin{array}{rcl}
  (1-p)^x\cdot p&si&x=0,1,2,\ldots\\
  0&& \mbox{en otro caso}\\
\end{array}\right.$$

La denotaremos por $Ge(p)$


### Función de distribución

Calculemos $P(X\leq 3)$.

Por la propiedad de la probabilidad del suceso complementario tenemos que
$$P(X\leq 3) = 1 -P(x>3) = 1-P(X\geq 4)$$
Efectivamente, el evento tenemos que $X\leq 3$ es que hemos fracasado más de tres veces hasta conseguir el primer éxito; es decir hemos gracasado 4 o más veces, por lo tanto
$${X>3} = {X\geq4} = {FFFF}$$

Ahora al ser los intentos sucesos independientes, tenemos que:
$$\begin{array}{rcl}
  P(X>3)&=&P({FFFF}) = P(F)\cdot P(F)\cdot P(F)\cdot P(F)\\
  &=&(1-p)\cdot (1-p)\cdot (1-p)\cdot (1-p) = (1-p)^{3+1} \\
  &=&(1-p)^4\\
\end{array}$$

calculamos 
$$F_X(3) = P(X\leq 3) = 1-P(X>3) = 1-(1-p)^{3+1}$$

por lo que podemos generalizar a cualquier entero positivo $k=0,1,2,\ldots$

$$F_X(x) = P(X\leq x) = \left\{\begin{array}{rcl}
  0&si&x<0\\\\
  1-(1-p)&si&k= 0 \leq x < 1\\\\
  1-(1-p)^2&si& k = 1\leq x < 2\\\\
  1-(1-p)^3&si& k = 2\leq x < 3\\\\
  1-(1-p)^{k+1}&si& k\leq x < k+1 \; para \; k=0,1,2,\ldots\\
\end{array}\right.$$

notemos que si $k=0,1,2,\ldots$ el límite de la función de distribución es 
$$\lim_{k\to \infty} F_X(k) = \lim_{k\to \infty} 1 - (1-p)^{k+1} = 1$$

ya que $0<1-p<1$

## La esperanza y la varianza de una distribución geométrica.

### Esperanza de una v.a. Ge(p)
$$\begin{array}{rcl}
  E(X) &=& \sum\limits_{x=0}^\infty x \cdot P_X(x)\\
  &=& \sum\limits_{x=0}^\infty x\cdot (1-p)^x \cdot p\\\
  &=& p\cdot (1-p)\cdot \sum\limits_{x=1}^\infty x\cdot (1-p)^{x-1}\\
  &=& p\cdot (1-p)\cdot \dfrac{1}{\left[1-(1-p)\right]^2}\\
  &=&(1-p)\cdot \dfrac{1}{p^2}\\
  &=&\dfrac{1-p}{p}\\
\end{array}$$ 

### Valor E(X^2) de una v.a. Ge(p)
$$\begin{array}{rcl}
  E(X^2) &=& \sum\limits_{x=0}^\infty x^2 \cdot P_X(x)\\\\
  &=& \sum\limits_{x=0}^\infty x^2 \cdot (1-p)^x \cdot p\\\\
  &=& \sum\limits_{x=1}^\infty \left[x\cdot (x-1)+x \right] (1-p)^{x} \cdot p\\\\
  &=&\sum\limits_{x=1}^\infty x\cdot (x-1)\cdot (1-p)^x\cdot p + \sum\limits_{x=1}^\infty x\cdot (1-p)^x \cdot p\\\\
  &=&(1-p)^2\cdot p \cdot \sum\limits_{x=2}^\infty x\cdot (x-1)\cdot (1-p)^{x-2}\\\\
  &+&(1-p)\cdot p\sum\limits_{x=1}^\infty x\cdot (1-p)^{x-1} \\\\
  &=&p\cdot (1-p)^2 \dfrac{2}{\left[1-(1-p)\right]^3} + (1-p)\cdot p \dfrac{1}{\left[1-(1-p)\right]^2}\\\\
  &=&p\cdot (1-p)^2 \dfrac{2}{p^3}+(1-p)\cdot p \dfrac{1}{p^2}\\\\
  &=&\dfrac{2\cdot(1-p)^2}{p^2}+\dfrac{1-p}{p}\\
\end{array}$$ 

### Varianza de una v.a. Ge(p)

$$\begin{array}{rcl}
  Var(X)&=&E(X^2) - E^2(X)\\\\
  &=&\dfrac{2\cdot(1-p)^2}{p^2}+\dfrac{1-p}{p}-\left(\dfrac{1-p}{p}\right)^2\\\\
  &=&\dfrac{2\cdot(1-p)^2+p\cdot (1-p)-(1-p)^2}{p^2}\\\\
  &=&\dfrac{(1-p)^2 + p\cdot (1-p)}{p^2}\\\\
  &=&\dfrac{1-2\cdot p p^2 + p - p^2}{p^2}\\\\
  &=&\dfrac{1-p}{p^2}\\\\
\end{array}$$

## Propiedad de falta de memoria
Sea  $X$ una v.a. discreta con dominio $D_X = \lbrace0,1,2, \ldots\rbrace$ con $P(X=0) = p$ entonces $X$ sigue una ley $Ge(p)$ si y sólo si
$$P(X> k + j \;|\; X\geq j) = P(X>k)$$
para todo $k,j = 0,1,2,3,\ldots$

Es decir, el número de veces que voy a fallar hasta el primer éxito no está condicionado por las veces que estamos probando. Por ejemplo es lo mismo lanzar 80 veces o lanzar 2 veces hasta el primer éxito.

Demostración.- Si es geométrica entonces el lado derecho de la igualdad es 
$$P(X>k) = 1 - P(x\leq k) = 1 - (1-(1-p)^{k+1}) = (1-p)^{k+1}$$

Luego 
$$\begin{array}{rcl}
  P(X>k+j | X\geq j)&=&\dfrac{P(\lbrace X>k+j\rbrace)\cap \lbrace X\geq j\rbrace}{P(X\geq j)}\\\\
  &=&\dfrac{P(X>k+j)}{P(>\geq j)}\\\\
  &=&\dfrac{1-P(X\leq k+j)}{1-P(X\leq j-1)}\\\\
  &=&\dfrac{1-(1-(1-p)^{k+j+1})}{1-(1-(1-p)^{k-1+1})}\\\\
  &=&\dfrac{(1-p)^{k+j+1}}{(1-p)^j}\\\\
  &=&(1-p)^{k+1}\\\\
\end{array}$$

Para demostrar el recíproco tomemos $j=1$ y $k\geq 0$ entonces por la propiedad de la pérdida de memoria
$$P(X>k+1 | X\geq 1) = P(X>k)$$

Como sabemos $P(X=0)=p$ tenemos $P(X\geq 1) = 1 - P(X<1) = 1-P(X=0) = 1-p$

Luego combinando las igualdades tenemos que
$$P(X<k+1 | X\geq 1) = \dfrac{P(X>k+1, X\geq 1)}{P(X\geq 1)} = \dfrac{P(X>k+1)}{P(X\geq 1)} = P(X>k)$$

Así  podemos poner que 
$$\begin{array}{rcl}
  P(X>k+1)&=&P(X\geq 1)\cdot P(X>k)\\\\
  &=&P(X<1)\cdot P(X>k)\\\\
  &=&(1-P(X=0))\cdot P(X>k)\\\\
  &=&(1-p)\cdot P(X>k)\\\\
\end{array}$$

En general tenemos que 
$$P(X>k+1) = (1-p)\cdot P(X>k)$$

del mismo modo para $j=2$
$$P(X>k+2) = (1-p)\cdot P(X>k+1)$$
Restando la primera igualdad de la última obtenemos:
$$P(X>k+1) - P(X>k+2) = (1-p)\cdot P(X>k) - (1-p)\cdot P(X>k+1)$$
de donde operando en cada lado de la igualdad obtenemos la recurrencia 
$$\left[1-P(X\leq k+1)  \right] - \left[1-P(X\leq k+2)\right] = (1-p)\cdot \left[P(X>k)-P(X>k+1)\right]$$

ahora operando 
$$\begin{array}{rcl}
  P(X\leq k+2)-P(X\leq k+1)&=&(1-p)\cdot \left[ 1-P(X\leq k)-(1-P(X\leq k+1))\right]\\\\
  P(X=k+2)&=&(1-p)\cdot \left[P(X\leq k+1)-P(X\leq k)\right]\\\\
  P(X=k+2)&=&(1-p)\cdot P(X=k+1)\\\\
\end{array}$$

De forma similar obtenemos 
$$P(X=k+1) = (1-p)\cdot P(X=k)$$
Utilizando la recurrencia anterior para calcular todas las probabilidades a partir de la $P(X=0)=p$; que vienen dadas por:

$$\begin{array}{rcl}
  P(X=0)&=&p\\
  P(X=1)&=&P(X=0+1) = (1-p)\cdot P(X=0) = (1-p)\cdot p\\
  P(X=2)&=&P(X=1+1) = (1-p)\cdot P(X=1)=(1-p)\cdot (1-p)\cdot p = (1-p)^2\cdot p\\
  P(X=k)&=&P(X=(k-1)+1) = (1-p)\cdot P(X=k+1) = (1-p)\cdot (1-p)^{k-1}\cdot p = (1-p)^k \cdot p\\
\end{array}$$

Lo que demuestra el recíproco, es decir que $X$ es Geom(p).


## La distribución geométrica con R y Python
Veamos los cálculos básicos con R para la distribución geométrica Ge(p=0.25). R implementa la geométrica que cuenta el número de fracasos.

$P(X=0) = (1-0.25)^0 \cdot 0.25 = 0.25$

```{r}
dgeom(0,prob = 0.25)
```

$P(X\leq 0) = 1-(1-0.25)^{0+1} = 1 - 0.75 = 0.25$

```{r}
pgeom(0,prob = 0.25)
```

$P(X\leq 4) = 1-(1-0.25)^{4+1} = 1-0.75^5 = 0.7626953$

```{r}
pgeom(4,prob = 0.25)
```

Una muestra de tamaño 25 de una Ge(0.25) 

```{r}
rgeom(n = 25, prob = 0.25)
```


# La distribución binomial negativa

### Función de probabilidad
$$P_X(x) = P(X=k) = \left\{\begin{array}{lcr}
  {k+n-1 \choose n-1}\cdot (1-p)^k \cdot p^n&si&k=0,1,\ldots\\\\
  0&&\mbox{en otro caso}\\
\end{array}\right.$$

## Números binomiales negativos
Dados dos enteros positivos $n$ y $k$ se define en número binomial negativo como 
$${-n\choose k} = \dfrac{(-n)(-n-1)\ldots (-n-k+1)}{k!}$$

se cumple que 
$$(t+1)^{-n} = \sum\limits_{k=0}^{\infty} {-n \choose k} t^k$$

Con R es la misma función que para los números binomiales

$${-6 \choose 4} = \dfrac{-6\cdot (-6-1)\cdot (-6-2) \cdot (-6-3) }{4!} = \dfrac{-6\cdot (-7)\cdot (-8)\cdot (-9)}{24} = \dfrac{3024}{24} = 126$$

```{r}
choose(-6,4)
```

### Esperanza de una BN(n,p)

Su esperanza es
$$E(X) = \sum\limits_{k=0}^\infty k \cdot {k+n-1 \choose n-1} \cdot (1-p)^k \cdot p^n = n\cdot \dfrac{1-p}{p}$$

La esperanza del cuadrado $X^2$ es 

$$E(X^2) = \sum\limits_{k=0}^\infty k^2 \cdot {k+n-1 \choose n-1} \cdot (1-p)^k \cdot p^n = n\cdot \dfrac{1-p}{p^2} + \left(n\cdot \dfrac{1-p}{p}\right)^2$$

### Varianza de una BN(n,p)
Por último la varianza es 
$$Var(X) = E(X^2)-E(X)^2 = n\cdot \dfrac{1-p}{p^2}+\left(n\cdot \dfrac{1-p}{p}\right)^2 - \left(n\cdot \dfrac{1-p}{p}\right)^2 = \dfrac{1-p}{p^2}$$

Y por tanto la desviación típica es
$$\sqrt{Var(X)} = \dfrac{\sqrt{n(1-p)}}{p}$$

## Ejemplo de la puerta con dos llaves
Tenemos un manojo de 10 llaves casi idénticas. De mnaera que cada vez que probamos una llave olvidamos qué llave hemos usado.
Sea $X$ la v.a. que nos da el número de intentos fallidos hasta abrir la puerta.

#### 1.  
¿Cuál es la distribución de probabilidad de $X$ la v.a. que nos da el número de fallos hasta abrir la puerta?
  Respuesta.- Sabemos que la probabilidad de éxito de cada intento es $p=dfrac{1}{10} = 0.1$ Como cada vez olvidamos qué llave hemos probado cada intento será independiente del anterior.
  Así que la variable $X$ que queremos modelar cuenta el número de fallos de repeticiones sucesivas e independientes de un experimento Ber(p=0.1) hasta conseguir 2 éxitos en un experimento.
  Por lo tanto podemos asegurar que $X$ sigue un distribución BN(n=2,p=0.1)
  
#### 2.
¿Cuál es la función de probabilidad y de distribución de $X$?
  Respuesta.- En general la función de probabilidad de un BN(n,p) es 
  $$P_X(x) = P(X=k) = \left\{ \begin{array}{ll}
  {k+n-1 \choose n-1}\cdot (1-p)^k \cdot p^n&si&k=0,1,\ldots\\\\
  0&&\mbox{en otro caso}\\
  \end{array}\right.$$
  
  En particular la función de probabilidad de una BN(n=2,p=0.1) es
  $$P_X(x) = P(X=k) = \left\{ \begin{array}{ll}
  {k+2-1 \choose 2-1}\cdot 0.9^k \cdot 0.1^2&si&k=0,1,\ldots\\\\
  0&&\mbox{en otro caso}\\
  \end{array}\right.$$
  
  La función de distribución en general es
  $$F_X(x) = P(X\leq x) = \left\{\begin{array}{lcl}
    0&si&\\
    \sum\limits_{i=1}^k {i+n-1 \choose n-1}\cdot \left(1-p\right)^{i+n-1}\cdot 0.1^n&si&x<0\\
    &si&k\leq x < k+1 \; y \; k=0,1,2,\ldots\\
    1&si&\mbox{en otro caso}\\
  \end{array}\right.$$
  
  Para $n=2$ y $p=0.1$
  
  $$F_X(x) = P(X\leq x) = \left\{\begin{array}{lcl}
    0&si&\\
    \sum\limits_{i=1}^k {i+1 \choose 1}\cdot 0.9^{i+1}\cdot 0.1^2&si&x<0\\
    &si&k\leq x < k+1 \; y \; k=0,1,2,\ldots\\
    1&si&\mbox{en otro caso}\\
  \end{array}\right.$$

#### 3.  
¿Cuál es la probabilidad de fallar exactamente 5 veces antes de abrir la puerta?
  Respuesta.-
  $$P(X=5) = {5+2-1\choose 1}\cdot 0.9^5 \cdot 0.1^2 = {6\choose 1}\cdot 0.9^5 \cdot 0.1^2 = 6\cdot 0.9^5 \cdot 0.1^2 = 0.0354294$$
  
#### 5.
¿Cuál es la probabilidad de fallar más de 4 ?
  Respuesta.-\; Nos piden que 
  $$P(X>4) = 1 - P(X\leq 4)$$
  Calculemos primero $P(X\leq 4):$
  $$\begin{array}{rcl}
    P(X\leq 4)&=&\sum\limits_{x=0}^4 P(X=x)\\\\
    &=&P(X=0) + P(X=1) + P(X=2) + P(X=3) + P(X=4)\\\\
    &=&{0+2-1\choose 1}\cdot 0.9^0 \cdot 0.1^2 + {1+2-1 \choose 1}\cdot 0.9^1 \cdot 0.1^2\\\\
    &+&{0+2-1\choose 1}\cdot 0.9^2 \cdot 0.1^2 + {1+2-1 \choose 1}\cdot 0.9^3 \cdot 0.1^2\\\\
    &+&{0+2-1\choose 1}\cdot 0.9^4 \cdot 0.1^2 \\\\
    &=&0.114265\\\\
  \end{array}$$
  
#### 5.
¿Cuál es el número esperado de fallos? ¿Y su desviación típica?
  Respuesta.- Como $X$ sigue una ley BN(n=2,p=0.1)
  $$E(X) = n\cdot \dfrac{1-p}{p} = 2 \cdot \dfrac{1-0.1}{0.1} = 18$$
  
  El número de fallos esperado es 18
  
  $$Var(X) = n\cdot \dfrac{1-p}{p^2} = 2\cdot \cdot \dfrac{1-0.1}{0.1^2} = 180$$
  
  Y su desviación típica será
  $$sv(X) = \sqrt{180}$$
  
  
## La binomial negativa en R

En el ejemplo de la puerta $X$ es una BN(size = 2, p = 0.1)
Por ejemplo $P(X=5)$ 
```{r}
dnbinom(5,size = 2, p = 0.1)
```

Calculemos $P(X\leq 4)$ 

```{r}
pnbinom(4,size = 2, p = 0.1)
```

P(X>4)
```{r}
1-pnbinom(4,2,0.1)
pnbinom(4,2,0.1,lower.tail = FALSE)
```


# La distribución de Poisson
Diremos que una v.a. discreta $X$ con $X(\Omega) = N$ tiene distribución de Poisson con parámetro $\lambda > 0$, y lo denotaremos por $Po(\lambda)$ si su función de probabilidad es:
$$P_X(x) = P(X=x) = \left\{\begin{array}{rcl}
  \frac{\lambda^x}{x!}e^{-\lambda}&si&x=0,1,\ldots\\
  0&&\mbox{en otro caso}\\
\end{array}\right.$$

Recordemos que el desarrollo en serie de taylor de la exponencial es
$$e^\lambda = \sum\limits_{x=0}^\infty \dfrac{\lambda^x}{x!}$$

Teniendo en cuenta esto es fácil comprobar que todos los valores de la función de probabilidad suman $1$.

Además recordando que dado $x\in \mathbb{R}-{0}$ se tiene que

$$\lim\limits_{n\to \infty} \left(1+\dfrac{x}{n}\right)^n = e^x$$

```{r}
# exponencial con serie de Taylor
eT <- function(l){
  sum <- 0
  for (i in 0:500){
    sum <- sum  + (l^i/factorial(i))
  }
  return(sum)
}

# Exponencial como límite
e <- function(x,n=100000000) (1+x/n)^n
exp(3)

eT(4)
e(4)
exp(4)
```

## La distribución Poisson como límite de una binomial

