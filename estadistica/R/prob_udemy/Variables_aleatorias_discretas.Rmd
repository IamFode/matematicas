---
title: "Variables aleatorias discretas"
author: "Christian Limbert Paredes Aguilera"
date: "1/12/2021"
output: pdf_document
---

## Definición de variable aleatoria
Una variable aleatoria es una aplicación que toma valores numéricos determinados por el resultado de un experimento aleatorio.

## Tipos de variables aleatorias
Variables aleatorias discretas, continua y mixtas.

## Función de probabilidad para varibales discretas
La función de probabilidad de una variable aleatoria discreta $X$ es la que denotamos por 
$$P_X(x) = P(X=x)$$
Dominio de una variable aleatoria discreta
$$D_X = \left\{ x \in \mathbb{R} \; | \, P_X(x) > 0\right\}$$
en el caso discreto lo mas habitual es que
$$X(\Omega) =D_X$$

## Propiedades de la función de probabilidad

Sea $X$ una v.a. discreta $X:\Omega:\Rightarrow \mathbb{R}$ con dominio $D_X$. Su función de probabilidad $P_X$ verifica las siguientes propiedades:

- $0\leq P_X(x) \leq 1$ para todo $x\in \mathbb{R}$

- $\sum\limits_{x\in D_X} P_X(x) = 1$

$$P_X(x) = \left\{ \begin{array}{rcl} 
  \dfrac{1}{8} & si & x=0,3\\\\ 
  \dfrac{3}{8}& si & x=1,2\\\\
  0&&en\; otro \; caso\\\\
\end{array}\right.$$

Efectivamente los valores de la función de distribución suman 1
$$\sum_{x=0}^{3} P_X(x) = \dfrac{1}{8} + \dfrac{3}{8} + \dfrac{3}{8} + \dfrac{1}{8} = 1$$

## Función de distribución de variables aleatorias 
La función de distribución de probabilidad (acumulada) de la v.a. X ya sea discreta o continua $F_X(x)$ representa la probabilidad de que $X$ toem un menor o igual que $x$ es decir
$$F_X(x) = P(X\leq x)$$
Sea $X$ una v.a. y $F_X$ su función de distribución

1. $P(X>x) = 1-P(X\leq x) = 1 - F_X(x)$

Demostración.- Tenemos que el complementario de $X$ mayor que $x$ es $\overline{\lbrace X>x\rbrace} = \lbrace X>x\rbrace^c = \lbrace X\leq x\rbrace$. Además 

$$P(X>x) = 1 -P\left(\overline{\lbrace X > x}\rbrace\right) = 1 - P(X\leq x) = 1 - F_X(x)$$

2. Sea $a$ y $b$ tales que $a<b$,
$$P(a<X\leq b) = P(X\leq b) - P(X\leq a) = F_X(b) - F_X(a)$$

Demostración.- Por otro lado, que $X$ se encuentre entre dos valores $a$ y $b$ es $\lbrace a<X\leq b \rbrace = \lbrace X\leq b \rbrace - \lbrace X\leq a \rbrace$ ahora podemos hacer

$$\begin{array}{rcl}
  P(a<X\leq b)&=&P( \lbrace X\leq \lbrace - \lbrace X \leq a \rbrace )\\
  &=&P( \lbrace X \leq b \rbrace ) - P( \lbrace X \leq a \rbrace )\\
  &=&F_X(b)-F_X(a)\\
\end{array}$$

### propiedadades de la función de distribución
Sea $F_X$ la función de distribución de un v.a. $X$ entonces:
\begin{itemize}
\item $0\leq F_X(x) \leq 1$
\item La función $F_x$ es no decreciente.
\item Si denotamos por $F_X(x_o^-) = \displaystyle\lim_{x\to x_o^-} F(x)$, entonces se cumple que 

$P(X<x_0) = F_X(x_0^-)$ y que $P(X=x_0) = F_X(x_0) - F_X(x_0^-)$
\item Se cumple que $\displaystyle\lim_{x\to\infty} F_X(x)=1\;; \quad \lim_{x\to -\infty}F_X(x)=0$
\item Toda función $F$ verificando las propiedades anteriores es función de distribución de alguna v.a. $X$.
\item $P(X>x) = 1-F_X(x)$
\item Dado $a,b\in \mathbb{R}$ con $a < b$
$$P(a<X\leq b) = F_X(b) - F_X(a)$$
\end{itemize}

### Desigualdades estrictas
\begin{itemize}
\item $P(X=x) = F_X(x)-F_X(x^-)$
\item $P(a<X<b) = F_X(b^-) - F_X(a)$
\item $P(a\leq X < b) = F_X(b^-) - F_X(a^-)$
\item $P(X<a)=F_X(a^-)$
\end{itemize}

### Más propiedades de la función de distribución

\begin{itemize}
\item Si $F_x$ es continua en $x$ se tiene que $P(X=x) = 0$ y por lo tanto $P(X\leq a) = P(X<a) + P(X=a) = P(X<a)$.

Demostración.- Si $X$ es continua entonces,
$$P(X=x) = F(a) - F(a^-) = F(a) - F(a) = 0$$
por lo tanto 
$$P(X\leq a) = P(X<a) + P(X=x) = P(X<a) + 0 = P(X<a)$$
\item Sea $X$ una v-a- discreta con dominio $D_X$ y que tiene por función de probabilidad $P_X(x)$ entonces su función de distribución $F_X(x_0)$ es 
$$F_X(x_0) = \sum_{x\leq x_0} P_X(x)$$
donde $\sum\limits_{x\leq x_0}$ indica que sumamos todos los $x\in D_X$ tales que $x\leq x_0$

Demostración.- 
$$F_X(x_0) = P(X\leq x_0) P\left(\bigcup_{x\leq x_0; x\in D_X} \lbrace x \rbrace \right) = \sum_{x\leq x_0}P(X=x)=  \sum_{x\leq x_0} P_X(x)$$
\end{itemize}

## Valor esperado
$$E(X)=\sum_{x\in X(\Omega)} x P_X(x)$$

En ocasiones se le denomina media poblacional o simplemente media y muy frecuentemente se la denota por 
$$\mu_X = E(X) \quad o \quad \mu = E(X)$$

Si ${n\to \infty}$ se tiene que $\lim\limits_{n\to \infty} \dfrac{n_x}{x} = P_X(x)$ por lo tanto $E(X) = \lim\limits_{x\to \infty}\sum\limits_{x=1}^n x\dfrac{n_x}{n}$
Entonces el valor esperado en una v.a. discreta puede entenderse como el valor promedio que tomaría una v.a. en un número grande de repeticiones.

### Esperanza de funciones de variables aleatorias discretas
Sea $X$ una v.a. discreta con función de probabilidad $P_X$ y de distribución $F_X$. Entonces el valor esperado de una función $f(x)$ es:
$$E(g(x)) = \sum_{x} g(x)P_X(x)$$

### Propiedades de los valores esperados

\begin{itemize}
\item $E(k) = k$ para cualquier constante $k$.

Demostración.- Se tiene que 
$$E(k) = \sum_{x=1}^n k\cdot P(X=k) = k\cdot P(X=k) + \ldots + k\cdot   P(X=k) = k\left[ P(X=k) + \ldots + P(X=k)\right] = k\cdot 1 = k$$

\item Si $a\leq X\leq b$ entonces $a\leq E(X) \leq b$

Demostración.- Sea $E(a) \leq E(X)\leq E(b)$ entonces por la anterior propiedad se tiene que $$a\leq E(X)\leq b$$

\item SI $X$ es una v.a. discreta que toma valores enteros no negativos entonces $E(X) = \sum\limits_{x=0}^{\infty} (1-F_X(x))$

Demostración.- Sea,
$$\begin{array}{rcl}
E(X)&=&\sum\limits_{k=0}^\infty k\cdot P(X=k)\\\\
&=&P(X=1)\\\\
&+&P(X=2)+P(X=2)\\\\
&+&P(X=3)+P(X=3)+P(X=3)\\\\
&+&P(X=4)+P(X=4)+P(X=4)+P(X=4)\\\\
&+&\ldots\\\\
\end{array}$$

Luego sumando por columnas se tiene,

$$\begin{array}{rcl}
\sum\limits_{k=1}^\infty P(X=k)&=&P(X>0)\\\\
\sum\limits_{k=2}^\infty P(X=k)&=&P(X>1)\\\\
\sum\limits_{k=3}^\infty P(X=k)&=&P(X>2)\\\\
\sum\limits_{x=0}^\infty kP(X=k)&=&\sum\limits_{k=0}^\infty P(X>k)\\\\
E(X)&=&\sum\limits_{k=0}^\infty 1-F_X(x)\\\\
\end{array}$$
\end{itemize}

### Propiedades de las series geométricas
\begin{itemize}
\item Una progresión geométrica de razon $r$ es una sucesión de la forma 
$$r^0,r^1,r^2, \ldots$$

\item La serie geométrica es la suma de todos los valores de la progresión geométrica
$$\sum_{k=0}^\infty r^k$$
\item Las sumas parciales desde el término $n_0$ al $n$ de una progesión geométrica valen
$$\sum_{k=n_0}^n r^k = \dfrac{r^{r_0}-r^n r}{1-r}$$
\item Si $|r|<1$ la serie geométrica es convergente y 
$$\sum_{k=0}^\infty r^k = \dfrac{1}{1-r}$$
\item En el caso en que se comience en $n_o$ se tiene que
$$\sum_{k=0}^\infty r^k = \dfrac{r^{n_0}}{1-r}$$
\item Si $|r|<1$ también son convergentes las derivadas, respecto de $f$, de la serie geométrica y convergen a la derivada correspondiente. Así teneos que 
$$\left(\sum_{k=0}^\infty r^k\right)^{'} = \sum_{k=1}^\infty kr^{k-1} \quad y \quad \left(\dfrac{1}{1-r}\right)^{'} = \dfrac{1}{(1-r)^2}$$
$$\left(\sum_{k=0}^\infty r^k\right)^{''} = \sum_{k=2}^\infty kr^{k-2}\quad y \quad \left(\dfrac{1}{1-r}\right)^{''} = \dfrac{2}{(1-r)^3}$$
\end{itemize}

Por lo tanto,

$$\begin{array}{rcl}
  E(X)&=&\sum\limits_{x=0}^\infty xP(X=x)\\\\
  &=&\sum\limits_{x=0}^\infty x\left(\dfrac{1}{2}\right)^{x+1}\\\\
  &=&\left(\dfrac{1}{2}\right)^2 \sum\limits_{x=0}^\infty x\left(\dfrac{1}{2}\right)^{x-1}\\\\  
  &=&\left(\dfrac{1}{2}\right)^2 \dfrac{1}{(1-\frac{1}{2})^2}\\\\
  &=&1\\\\
\end{array}$$

Luego calculamos su función de distrbución 

$$\begin{array}{rcl}
  F_X(x)&=&P(X\leq x)\\\\
  &=&\sum\limits_{k=0}^x P(X=k)\\\\
  &=&\sum\limits_{k=0}^x \left(\dfrac{1}{2}\right)^{k+1}\\\\
  &=&\dfrac{\frac{1}{2}-\frac{1}{2}^{x+1} \frac{1}{2} }{1-\frac{1}{2} }\\\\
  &=&1-\left(\dfrac{1}{2}\right)^{x+1}\\\\
\end{array}$$

## Momentos de orden m

LLamaremos momento de orden m respecto al punto $C$ a
$$E\left[(X-C)^m\right] = \sum_{x\in X(\Omega)} (X-C)^m\cdot 
  P(x)$$

\begin{itemize}
  \item Cuando $C=0$ los momentos reciben el nombre de momentos respecto al origen.
  \item Cuando $C=E(x)$ reciben el nombre de momentos centrales o respecto de la media. Luego la esperanza es el momento de orden 1 respecto al origen. Estos momentos son la versión poblacional.
\end{itemize}

## Varianza y desviación típica

### La varianza
Sea $X$ una v.a. Llamaremos varianza de $X$ a
$$Var(X) = E\left[(X-E(X))^2\right]$$

Por lo tanto la varianza es el momento central de orden 2.

de forma frecuente se utiliza la notación

$$\sigma_X^2 = Var(X)$$

A la raíz cuadrada positiva de la varianza

$$\sigma_X = \sqrt{Var(X)}$$

se la denomina desviación típica o estándar de $X$.

### Propiedades
\begin{itemize}
  \item Si $X$ es una v.a. discreta con función de probabilidad $P_X$ su varianza es
  $$\sigma_X^2 = Var(x) = E\left[(X-E(X))^2\right] = \sum_{x} \left[x-E(X)\right]^2 P_X(x)$$
 
  \item Sea $X$ una v.a. 
  $$Var(X) = E(X^2) -\left[E(X)\right]^2 = \sum_{x} x^2 P_X(X) - \left[E(X)\right]^2$$
  
 Demostración.- 
 $$\begin{array}{rcl}
  Var(X)&=&\sum\limits_{x}\left[x-E(X)\right]^2P_X(x)\\\\
  &=&\sum\limits_{x}\left[x^2 - 2xE(X) + E(X)^2\right]P_X(x)\\\\
  &=&\sum\limits_{x} x^2P_X(x) - E(X)\sum\limits_{x}2xP_X(x) + E(X)^2 \sum\limits_{x}P_X(x)\\\\
  &=&E(X^2) - 2E(X)E(X) + E(X)^2\\\\
  &=&E(X^2) - E(X)^2\\\\
 \end{array}$$
 
\end{itemize}

### Propiedades de la varianza
\begin{itemize}   
  \item $Var(X) \geq 0$
  
    Demostración.- La definición nos dice que la diferencia de la v.a. X y la esperanza del mismo está elevada al cuadrado y por tanto $Var(X)\geq 0$.
    
  \item $Var(cte) = E(cte^2) -E(cte)^2 = cte^2 - cte^2 = 0$
  
    Demostración.- Dado que la varianza de una constante es la misma constante, entonces $cte^2 - cte^2$ y por tanto queda demostrada la proposición. 
    
  \item El mínimo de $E\left[(X-C)^2\right]$ se calcanza cuando $C=E(X)$ y es $Var(X)$. Esta propiedad es una de las que hace útil a la varianza como medida de dispersión.
  
    Demostración.- Sea
    $$\begin{array}{rcl}
      E\left[(X-C)^2\right]&=&\sum\limits_{x}(x-C)^2 P_X(x)\\\\
      &=&\sum\limits_{x} \left(x^2 - 2xC + C^2\right) P_X(x)\\\\
      &=&\sum\limits_{x}x^2P_X(X) - 2\sum\limits_{x} xCP    _X(x) + \sum\limits_{x}C^2P_X(x)\\\\
      &=&\sum\limits_{x}x^2P_X(x)-2CE(X) + C^2\\\\
    \end{array}$$
    
  
  Luego derivando e igualando a cero nos queda
  $2E(x)-2C = 0$ y por lo tanto $C=E(X)$
  
  por último para minimizar necesitamos saber que la no convexidad va hacia arriba realizando su segunda derivada, de donde concluimos que efectivamente se está minimizando.
    
\end{itemize}

## Transformaciones lineales de variables aleatorios
Un cambio de variable lineal o transformación lineal de una v.a. X es otra v.a. $Y=a+bX$ donde $a,b\in \mathbb{R}$

### Esperanza de una transformación lineal
Sea $X$ una v.a. con $E(X) = \mu_X$ y $Var(X)=\sigma^2_X$ y $a,b\in \mathbb{R}$, entonces si $Y=a+bX$
\begin{itemize}
  \item $E(Y) = E(a+bX) = a+b\cdot E(X) = a+b\mu_X$
  
  Demostración.- 
  $$\begin{array}{rcl}
    E(Y)&=&E(a+bX)\\\\
    &=&\sum\limits_{x}(a+b\cdot X)P_X(x)\\\\
    &=&a\sum\limits_{x}P_X(x) + b\sum\limits_{x}x\cdot P_X(x)\\\\
    &=&a+b\cdot E(X)\\\\
    &=&a+b\mu_X\\\\
  \end{array}$$
  
  \item $Var(Y) = Var(a+bX) = b^2 Var(X)= b^2 \sigma_X^2$
  
  Demostración.-\; 
  $$\begin{array}{rcl}
    Var(Y) = Var(a+bX)&=&E\left[(a+bX)\right]-\left[E(a+bX)\right]^2\\\\
    &=&E\left(a^2 + 2abX + b^2 X^2\right) - \left[a + bE(X)\right]^2\\\\
    &=&a^2 + 2abE(X) + b^2E(X^2) - a^2 - 2abE(X) - b^2\left[E(X)\right]^2\\\\
    &=&b^2E(X^2)-b^2\left[E(X)\right]^2\\\\
    &=&b^2Var(X)\\\\
    &=&b^2\cdot \sigma_X^2\\\\
  \end{array}$$
  
  \item $\sigma_Y = \sqrt{Var(Y)} \sqrt{2^2 Var(X)} = |b|\sigma_X$
  
  Demostración.-\; ya que $\sqrt{b^2} = |b|$ y en consecuencia del anterior ejercicio se tiene demostrada la proposición mencionada.
  
\end{itemize}


