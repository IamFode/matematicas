\chapter{Muestras aleatorias y distribuciones de muestreo}

\begin{def.}
    Si las variables aleatorias $X_1$, $X_2,\ldots, X_n$ tiene la misma función (densidad) de probabilidad que la de la distribución de la población y su función (distribución) conjunta de probabilidad es igual al producto de las marginales, entonces $X_1,X_2,\ldots , X_n$ forman un conjunto de $n$ variables aleatorias independientes e idénticamente distribuidas (IID) que constituyen una muestra aleatoria de la población.
\end{def.}

En el contexto ed la definición 7.1, la función (densidad) conjunta de probabilidad de $X_1,x_2,\ldots,X_n$ es la función de verosimilitud de la muestra dada por
\begin{tcolorbox}
    $$ L(x;\theta)=\prod\limits_{i=1}^n f(x_i; \theta),$$
\end{tcolorbox}
    en donde $x=\left\{x_1,x_2,\ldots.x_n\right\}$ denota los datos muestreados. Cunado las realizaciones $x$ se conocen, $L(x;\theta)$ es una función del parámetro desconocido $\theta$. 

\begin{ejem}
    Se ilustrará el concepto de muestra aleatoria dada en el definición 7.1 midiante lo siguiente: Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de $n$ variables aleatorias IID de una población cuya distribución de probabilidad es exponencial con densidad
    $$f(x;\theta)=\dfrac{1}{\theta}e^{-x/\theta},\quad 0<x<\infty.$$
    Cuando se observa $X_1$ y se registra su realización $x_1$,
    $$f(x_1;\theta)=\dfrac{1}{\theta}e^{-x_1/\theta},\quad 0<x_1<\infty.$$
    Ahora se observa $X_2$ y se registra su realización $x_2$. Dado que $X_1$ y $X_2$ son estadísticamente independientes y tienen las mismas densidades marginales,
    $$f(x_2|x_1)=f(x_2\; \theta)=\dfrac{1}{\theta}e^{-x_2/\theta},\quad 0<x_2<\infty.$$
    La función de densidad conjunta de $X_1$ y $X_2$ es
    $$f(x_1,x_2;\theta)=f(x_1;\theta)f(x_2;\theta)=\dfrac{1}{\theta^2}e^{-(x_1+x_2)/\theta},\quad 0<x_i<\infty,\; i=1,2.$$
    Por lo tanto, se desprende que para una muestra aleatoria de tamaño $n$
    $$L(x_1,x_2,\ldots,x_n; \theta)=\dfrac{1}{\theta}e^{-(x_1+x_2+\ldots +x_n)/\theta},\quad 0<x_1<\infty,\; i=1,2,\ldots,n.$$
\end{ejem}

\setcounter{section}{2}
\section{Distribuciones de muestreo de estadísticas}


\begin{def.}
    Un parámetro es una caracterización numérica de las distribuciones de la población de manera que describe, parcial o completamente, la función de densidad de probabilidad de la característica de interés. Por ejemplo, cuando se especifica el valor del parámetro de escala exponencial $\theta$, se describe de manera completa la función de probabilidad
    $$f(x;\theta)=\dfrac{1}{\theta}e^{(-x/\theta)}.$$
    La oración describe de manera completa, sugiere que una vez que se conoce el valor de $\theta$ entonces, puede formularse cualquier proposición probabilistica de interes.
\end{def.}

Dado que los parámetros son prácticamente inherentes a todos los modelos de probabilidad, es imposible calcular las probabilidades deseadas sin un conocimiento del valor de estos. Es por esta razón que la noción de una estadística y su distribución de muestreo es muy importante en inferencia estadística.

Antes de dar la definición de una estadística, debe notarse que desde un punto de vista clásico (no bayesiano), un parámetro se considera como una cosntante fija cuyo valor se desconoce. Desde una perspectiva bayesiana un parámetro siempre es una variable aleatoria con algún tipo de distribución de probabilidad.

\begin{def.}
    Una estadística es cualquier función de las variables aleatorias que se observaron en la muestra de manera que esta función no contiene cantidades desconocidas.
\end{def.}

De manera general, denótese una estadística por $T=u(X)$. Dado que $T$ es una función de variables aleatorias, es en sí misma una variable aleatoria, y su valor específico $t=u(x)$ puede determinarse cuando se conozcan las realizaciones $x$ de $X$. Si se emplea una estadística $T$ para estimar un parámetro desconocido $\theta$, entonces $T$ recibe el nombre de \textbf{estimador} de $\theta$, y el valor específico de $t$ como un resultado de los datos muestrales recibe el nombre de \textbf{estimación} de $\theta$. Esto es, un estimador es una estadística que identifica al mecanismo funcional por medio del cual, una vez que las observaciones en la muestra se realizan, se obtiene una estimación.\\

Un parámetro es una constante pero una estadística es una variable aleatoria. Además, un valor del parámetro descrito describe de manera completa un modelo de probabilidad; ningún valor de la estadística puede desempeñar tal papel si cada uno de estos depende del valor de las observaciones de las muestras. 

%------------------- Definición 7.4
\begin{def.}
    La distribución de muestreo de una estadística $T$ es la distribución de probabilidad de $T$ que puede obtenerse como resultado de un número infinito de muestras aleatorias independientes, cada una de tamaño $n$, provenientes de la población de interés.
\end{def.}

La distribución de muestreo de una estadística hace posible este tipo de análisis de probabilidad, esencial para valorar el riesgo inherente cuando se formulan ingerencias.

%-------------------- Teorema 7.1
\begin{teo}
    Sea $X_1,X_2,\ldots, X_n$ un conjunto de $n$ variables aleatorias independientes cada una con función generadoras de momentos $m_{X_1}(t),m_{X_2}(t),\ldots , m_{X_n}(t).$
    Si 
    $$Y=a_1X_1+a_2X_2+\ldots + a_nX_n,$$
    en donde $a_1,a_2,\ldots,a_n$ son constantes, entonces:
    $$m_Y(t)=m_{X_i}(a_1t)m_{X_2}(a_2t)\cdots m_{X_n}(a_nt).$$
	Demostración.-\; Mediante el empleo de la definición y la hipótesis de independencia, se tiene
	$$\begin{array}{rcl}
	    m_Y(t)&=&E\left\{e^{\left[t\left(a_1X_1+a_2X_2+\ldots + a_nX_n\right)\right]}\right\}\\\\
		  &=&E\left\{e^{\left[t\left(a_1X_1\right)\right]}e^{\left[t\left(a_2X_2\right)\right]}\cdots e^{\left[t\left(a_nX_n\right)\right]}\right\}\\\\
		  &=&E\left\{e^{\left[t\left(a_1X_1\right)\right]}\right\}E\left\{e^{\left[t\left(a_2X_2\right)\right]}\right\}\cdots E\left\{e^{\left[t\left(a_nX_n\right)\right]}\right\}\\\\
		  &=&m_{X_1}(a_1t)m_{X_2}(a_2t)\cdots m_{X_n}(a_nt).
	\end{array}$$
\end{teo}

De esta forma, la función generadora de momentos de una combinación lineal de $n$ variables aleatorias independientes es el producto de las correspondientes funciones generadoras de momentos con argumentos iguales a las constantes de tiempo $t$.

%------------------- Teorema 7.2
\begin{teo}
    Sea $X_1,X_2,\ldots,X_n$ un conjunto de variables aleatorias independientes normalmente distribuidas con medias $E(X_i)=\mu_i$ y varianzas $Var(X_i)=\sigma^2_i$ para $i=1,2,\ldots n$. Si
    $$Y=a_1X_1+a_2X_2+\ldots a_nX_n,$$
    en donde $a_2,a_2,\ldots,a_n$ son constantes, entonces $Y$ es una variable aleatoria con distribución normal y media
    $$E(Y)=a_1\mu_1+a_2\mu_2+\ldots + a_nX_n$$
    y con varianza 
    $$Var(Y)=a_1^2\sigma_1^2+a_2^2\sigma_2^2 + \ldots + a_n^2\sigma_n^2.$$\\
	Demostración.-\; Dado que $X_i$ se encuentra normalmente distribuida, su función generadora de momentos es
	$$m_{X_i}(t)=e^{\left(\mu_i t + \frac{\sigma_i^2t^2}{2}\right)}.$$
	De acuerdo con el teorema 7.1, la función generadora de momentos de $Y$ es
	$$\begin{array}{rcl}
	    m_Y(t)&=&m_{X_1}(a_1t)m_{X_2}(a_2t)\cdots m_{X_n}(a_nt)\\\\
		  &=&e^{\left(\mu_1 a_1t + \frac{a_1^2\sigma_1^2t^2}{2}\right)}e^{\left(\mu_2a_2t + \frac{a_2^2\sigma_2^2t^2}{2}\right)}\cdots e^{\left(\mu_n a_nt + \frac{a_n^2\sigma_n^2t^2}{2}\right)}\\\\
		  &=&e^{\left[t\sum\limits_{i=1}^n a_i\mu_i+\frac{\left(t^2\sum\limits_{i=1}^n a_i^2\sigma_i^2\right)}{2}\right]}.
      \end{array}$$
      Por lo tanto, $Y$ se encuentra normalmente distribuida con media $\sum_{i=1}^na_i\mu_i$ y varianza $\sum_{i=1}^n a_i^2 \sigma_i^2.$
\end{teo}

Del teorema 7.2 se desprende que si $a_i=1$ para $i=1,2,\ldots,n$, entonces la suma de variables aleatorias independientes normalmente distribuidas también posee una distribución normal con media y varianza igual a la suma de las medias y las varianzas de cada una de las variables aleatorias. El resultado anterior se conoce como la propiedad aditiva de la distribución normal. Debe notarse que la hipótesis de normalidad no es necesaria para obtener las fórmulas de la media y la varianza de $Y$ en el teorema 7.2. De hecho, con base en el teorema 6.1, si $X_1,X_2,\ldots , X_n$ es un conjunto de $n$ variables aleatorias IID con medias $E(X_i)=\mu_i$ y varianzas $Var(X_i)=\sigma_i^2$, $i=1,2,\ldots , n$ entonces para $Y=a_1X_1+a_2X_2+\ldots +a_nX_n,$
$$E(Y)=\sum_{i=1}^{n}a_i\mu_i$$
y
$$Var(Y)=\sum_{i=1}^n a_i^2 \sigma_i^2.$$
en donde $a_1,a_2,\ldots,a_n$ son constantes.


\section{\boldmath La distribución de muestreo de $\overline{X}$}

Sea $X_1,X_2,\ldots , X_n$ una muestra aleatoria que consiste en $n$ variables aleatorias IID tales que $E(X_i)=\mu$ y $Var(X_i)=\sigma^2$ para toda $i=1,2,\ldots,n$. Entonces la estadística
$$\overline{X}=\dfrac{X_1+X_2+\ldots + X_n}{n}$$
se define como la media de las $n$ variables aleatorias IDD o, sencillamente, media muestral. Nótese que una vez que se conocen las realizaciones $x_1,x_2\ldots,x_n$ de $X_1,X_2,\ldots,X_n$, respectivamente, realización $\overline{x}$ de $\overline{X}$ se obtiene promediando los datos muestrales. \\
Si en, $E(Y)=\sum_{i=1}^{n}a_i\mu_i$ y $Var(Y)=\sum_{i=1}^n a_i^2 \sigma_i^2$, $a_i=1/n$, $i=1,2,\ldots,n$ entonces el valor esperado y la varianza de $\overline{X}$ son
\begin{tcolorbox}
$$E(\overline{X})=\sum_{i=1}^n \dfrac{1}{n}\mu=n\left(\dfrac{\mu}{n}\right)=\mu.$$
\end{tcolorbox}
y
\begin{tcolorbox}
$$\Var(\overline{X}) = \sum_{i=1}^n \dfrac{1}{n^2} \sigma^2 = n \left(\dfrac{\sigma^2}{n^2}\right)=\dfrac{\sigma^2}{n},$$
\end{tcolorbox}
respectivamente, en donde $\mu$ y $\sigma^2$ son la media y la varianza de la distribución de la población a partir de la cual se obtuvo la muestra. Luego de esta última ecuación de $Var(\overline{X})$ la desviación estándar de $\overline{X}$ es
\begin{tcolorbox}
    $$d.e.(\overline{X})=\dfrac{\sigma}{\sqrt{n}}.$$
\end{tcolorbox}
la cual, en algunas ocasiones recibe el nombre de \textbf{error estándar de la media}. Si el tamaño de la muestra crece, la precisión de la media muestral para estimar la media poblacional aumenta. 

%-------------------- Teorema 7.3
\begin{teo}
    Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria que consiste de $n$ variables aleatorias independientes y normalmente distribuidas con media $E(X_i)=\mu$ y varianzas $Var(X_i)=\sigma^2$, $i=1,2,\ldots,n$. Entonces la distribución de la media muestral $\overline{X}$ es normal con media $\mu$ y varianza $\sigma^2/n.$\\\\
	Demostración.-\; Este teorema es un corolario del teorema 7.2. Esto es, sea $a_i=1/n$; dado que las medias y las varianzas son iguales, respectivamente, la función generadora de momentos de $\overline{X}$ es
	$$\begin{array}{rcl}
	    m\overline{X}(t) &=&  e^{\left(t\displaystyle\sum_{i=1}^n \dfrac{1}{n}\mu+\dfrac{t^2\displaystyle\sum_{i=1}^n \frac{1}{n^2}\sigma^2}{2}\right)}\\\\
			     &=&e^{\left(t\mu+\dfrac{t^2\sigma^2}{2n}\right)},
	\end{array}$$
	que es la función generadora de momentos de una variable aleatoria normalmente distribuida con media $\mu$ y varianza $\sigma^2/n$. De esta forma, la \textbf{\boldmath función de densidad de probabilidad de $\overline{X}$ cuando se muestrea una población cuya distribución es normal}, está dada por
	\begin{tcolorbox}
	    $$f\left(\overline{x},\mu,\sigma/\sqrt{n}\right)=\dfrac{\sqrt{n}}{\sqrt{2\pi}\sigma}e^{\left[-\dfrac{n(\overline{x}-\mu)^2}{2\sigma^2}\right]}, \quad -\sigma<\overline{x}<\sigma.$$
	\end{tcolorbox}
\end{teo}

% -------------------  ejemplo 
\begin{ejem}

    Demostrar que si $X_1,X_2,\ldots,X_n$ son $n$ variables aleatorias independientes  exponencialmente distribuidas con función de densidad de probabilidad
    $$f(x;\theta)=\dfrac{1}{\theta}e^{-x/\theta}\qquad x>0,$$
    entre $\overline{X}$ tiene una distribución gama.\\\\
	Demostración.-\; Recuérdese que la función generadora de momentos de una variable aleatoria exponencialmente distribuida es $(1-\theta t)^{-1}$. De esta manera para cada $X_i$ de la muestra,
	$$m_{X_i}(t)=(1-\theta t)^{-1}.$$
	Del teorema 7.1 con $a_i=\frac{1}{n},\; i=1,2,\ldots , n$ se desprende que la función generadora de momentos de la media muestral $\overline{X}$ es 
	$$
	\begin{array}{rcl}
	    m_{\overline{X}}(t) &=& m_{X_i}\left(\dfrac{t}{n}\right)m_{X_2}\left(\dfrac{t}{n}\right)\ldots m_{X_n}\left(\dfrac{t}{n}\right)\\\\
				&=& \left[1-\left(\dfrac{\theta t}{n}\right)\right]^{-1}\left[1-\left(\dfrac{\theta t}{n}\right)\right]^{-1}\ldots \left[1-\left(\dfrac{\theta t}{n}\right)\right]^{-1}\\\\
				&=& \left[1-\left(\dfrac{\theta t}{n}\right)\right]^{-n}.
	\end{array}
	$$
	Pero la expresión anterior es la función generadora de momentos de una distribución gama con parámetro de forma $n$ y parámetro de escala $\theta/n$. De acuerdo con lo anterior, cuando se muestrea una población cuya distribución de probabilidad es exponencial, la densidad de probabilidad de $\overline{X}$ está dada por
	$$f(\overline{x};n,\theta/n)=\dfrac{n^n}{\Gamma (n) \theta^n}\overline{x}^{n-1}e^{-n\overline{x}/\theta},\qquad \overline{x}>0.$$
    \end{ejem}
Nótese que si en las expresiones $E(X)=\alpha\theta$ y $\Var(X)=\alpha\theta^2$ se reemplaza $\alpha$ con $n$ y $\theta$ con $\theta/n$ se obtiene
\begin{tcolorbox}
    $$E(\overline{X})=n\dfrac{\theta}{n}=\theta$$
\end{tcolorbox}
y
\begin{tcolorbox}
    $$Var(\overline{X})=n\dfrac{\theta^2}{n^2}=\dfrac{\theta^2}{n},$$
\end{tcolorbox}
como era de esperarse ya que $\theta$ y $\theta^2$ son la media y la varianza de una variable aleatoria con distribución exponencial.\\

De la sección 5.5, recuérdese que si el parámetro de forma de una distribución gama tiene un valor grande, entonces los valores de probabilidad para una variable aleatoria gama pueden aproximarse, en forma adecuada, por una distribución normal. Dado que $\Gamma^m,$ muestrear una distribución exponencial con parámetro $\theta$, $\overline{X}$ tiene una distribución gama con media $\theta$, y desviación estándar $\dfrac{\theta}{\sqrt{n}}$. Entonces, para $n$ grande
$$Z=\dfrac{\overline{X}-\theta}{\dfrac{\theta}{\sqrt{n}}}.$$
es, en forma aproximada, $N(0,1)$.

Para ser que para un valor grande $n$, la distribución de $\overline{X}$ es aproximadamente normal. De hecho, no importa el tipo de modelo de probabilidad a partir del cual se obtenga la muestra; muestras la media y la varianza existan, la distribución de muestreo de $\overline{X}$ se encontrará aproximada por $N\left(\mu, \sigma/\sqrt{n}\right)$ para valores grandes de $n$. Lo anterior constituye uno de los más importantes teoremas en inferencia estadística, y se conoce como \textbf{teorema central del límite}.

% -------------------  Toeorema 7.4
\begin{teo}
    Sean $X_1,X_2,\ldots , X_n$ variables aleatorias $IID$ con una distribución de probabilidad no especificada y que tiene una media $\mu$ y varianza $\sigma^2$ finita. El promedio muestral $\overline{X}=\left(X_1+X_2+\ldots + X_n\right)/n$ tiene una distribución con media $\mu$ y varianza $\sigma^2/n$ que tiende hacia una distribución normal conforme $n$ tiende a $\infty$. En otras palabras, la variable aleatoria $(\overline{X}-\mu)/(\sigma/\sqrt{n})$ tiene como límite una distribución normal estándar.\\\\
    Demostración.-\; Se quiere demostrar que la función generadora de momentos de $(\overline{X}-\mu)/(\sigma/\sqrt{n})$ tiende a la de una distribución normal estándar conforme $n$ tiende al infinito. Sean
    $$Z_i=\dfrac{X_i-\mu}{\sigma},\qquad i=1,2,\ldots,n$$
    y
    $$Y=\dfrac{\overline{X}-\mu}{\dfrac{\sigma}{\sqrt{n}}}.$$
    Dado que 
    $$\dfrac{1}{n}\sum_{i=1}^n \left(\dfrac{X_i-\mu}{\dfrac{\sigma}{\sqrt{n}}}\right)=\dfrac{1}{n}\dfrac{1}{\dfrac{\sigma}{\sqrt{n}}}\sum_{i=1}^n (X_i-\mu)=\dfrac{1}{n}\dfrac{1}{\dfrac{\sigma}{\sqrt{n}}}(n\overline{X}-n\mu)=\dfrac{\overline{X}-\mu}{\dfrac{\sigma}{\sqrt{n}}}.$$
    Entonces,
    $$Y=\dfrac{1}{\sqrt{n}}\sum_{i=1}^n Z_i.$$
    Como resultado se tiene que la función generadora de momentos de $Y$ es igual a la función generadora de momentos de $(1/\sqrt{n})\sum_{i=1}^n Z_i$. Del teorema 7.1,
    $$
    \begin{array}{rcl}
	m_Y(t)&=&\left[m_{Z_i}(t/\sqrt{n})\right]^n\\\\
	      &=& \left\{E\left[e^{\frac{tZ_i}{\sqrt{n}}}\right]\right\}^n,
    \end{array}
    $$
    dado que las $Z_i$ son variables aleatorias independientes.\\
    Al expandir $(tZ_i/\sqrt{n})$ en una serie de Taylor:
    $$e^{\frac{tZ_i}{\sqrt{n}}}=1+\dfrac{t}{\sqrt{n}}Z_i + \dfrac{t^2}{2n}Z_i^2 + \dfrac{t^3}{3!n^{\frac{3}{2}}}Z_i^3 + \ldots .$$
    Si se toma los valores esperados y se recuerda que $E(Z_i)=0$ y $Var(Z_i)=1$, $i=1,2,\ldots,n$ se tiene
    $$E\left[e^{\frac{tZ_i}{\sqrt{n}}}\right]=1+\dfrac{t^2}{2n}+\dfrac{t^3}{3!n^{\frac{3}{2}}}E\left(Z_i^3\right)+\ldots.$$
    De acuerdo con lo anterior
    $$
    \begin{array}{rcl}
	m_Y(t) &=& \left[1+\dfrac{t^2}{2n}+\dfrac{t^3}{3!n^{\frac{3}{2}}}E\left(Z_i^3\right)+\ldots\right]^n\\\\
	       &=& \left\{1+\dfrac{1}{n}\left[\dfrac{t^2}{2}+\dfrac{t^3}{3!\sqrt{n}}E\left(Z_i^3\right)+\ldots\right]\right\}^n\\\\
	       &=& \left(1+\dfrac{u}{n}\right)^n
    \end{array}
    $$
    donde
    $$u=\dfrac{t^2}{2}+\dfrac{t^3}{3!\sqrt{n}}E\left(Z_i^3\right)+\ldots.$$
    Ahora
    $$\lim_{n\to \infty}m_Y(t)=\lim_{n\to \infty}\left(1+\dfrac{u}{n}\right)^n.$$
    Pero por definición 
    $$\lim_{n\to \infty}\left(1+\dfrac{u}{n}\right)=e^u.$$
    Lo anterior da como resultado una situación idéntica a la que se tiene en la demostración del teorema 5.1. Esto es, conforme $n\to \infty$, todos los términos en $u$, excepto el primero, tienden hacia cero debido a que todos tienen potencias positivas de $n$ en sus denominadores. Por lo tanto, se puede deducir que
    $$\lim_{x\to \infty}m_Y(t)=e^{\frac{t^2}{2}},$$
    o la distribución límite de $Y=(\overline{X}-\mu)/(\sigma/\sqrt{n})$ es la normal estándar para valores grandes de $n$.
\end{teo}

La esencia del teorema central del límite recae en el hecho de que para $n$ grande, la distribución $(\overline{X}-\mu)/(\sigma/\sqrt{n})$es, en forma aproximada, normal con media cero y desviación estándar uno, sin importar cuál sea el modelo de probabilidad a partir del que se obtuvo la muestra. Debe notarse que si el modelo de probabilidad de la población es semejantes a una distribución normal (esto es, si es simétrico y existe una concentración relativamente alta alrededor del punto de simetría), la aproximación normal será buena aun para pequeñas muestras. Por otro lado, si el modelo de la población tiene muy poco parecido a una distribución normal (por ejemplo existe una alta asimetría), la aproximación normal sólo será adecuada para valores relativamente grandes a $n$. En muchas casos, puede concluirse de forma segura, que la aproximación será buena mientras $n>30$. Por lo tanto, la variable aleatoria
$$Z=\dfrac{\overline{X}-\mu}{\dfrac{\sigma}{\sqrt{n}}}$$
se emplea para formular inferencias acerca de $\mu$ cuando se conoce el valor de la varianza población $\sigma^2.$

\section{La distribución de muestro de \boldmath$S^2$}

Otra estadística importante empleada para formular inferencias con respecto a las varianzas de la población es la varianza denotada por $S^2$. Para comenzar, es necesario suponer que $\mu$ es conocida y $\sigma^2$ no. Así, $\sigma^2$ se encuentra definida por
$$S^2=\sum_{i=1}^n \dfrac{(X_i-\mu)^2}{n},$$
en donde $X_1,X_2,\ldots,X_n$ constituye una muestra aleatoria de una distribución normal con media $\mu$ y varianza $\sigma^2$ desconocida. Para determinar una distribución de muestreo que permita hacer inferencias sobre $\sigma^2$ con base en $S^2$ definida por (7.13), se enuncia y demuestra el siguiente teorema.

%-------------------- teorema 7.5
\begin{teo}
    Sean $X_1,X_2,\ldots ,X_n$ una muestra aleatoria de una distribución normal con media $\mu$ y varianza $\sigma^2$. La distribución de la variable aleatoria 
    $$Y=\sum_{i=1}^n \dfrac{(X_i-\mu)^2}{\sigma^2}$$
    es del tipo chi-cuadrada con $n$ grados de libertad.\\\\
	Demostración.-\; Dado que $X_i\sim N(\mu,\sigma)$, $i=1,2,\ldots,n$, $Z_i=\dfrac{X_i-\mu}{\sigma}$ define $n$ variables aleatorias normales estándar independientes, se tiene:
	$$Y=\sum_{i=1}^n Z_i^2.$$
	Del teorema 7.1,
	$$
	\begin{array}{rcl}
	    m_Y(t)&=&m_{Z^2_i}(t)m_{Z^2_2}(t)\ldots m_{Z^2_n}(t)\\\\
		  &=& (1-2t)^{-\frac{1}{2}}(1-2t)^{-\frac{1}{2}}\ldots (1-2t)^{-\frac{1}{2}}\\\\
	\end{array}
	$$
	dado que el cuadrado de una variable aleatoria normal estándar tiene una distribución chi-cuadrada con un grado de libertad (véase el ejemplo 5.14). De esta forma se tiene
	$$m_Y(t)=(1-2t)^{-\frac{n}{2}}$$
	que es la función generadora de momentos de una distribución chi-cuadrada con $n$ grados de libertad. De acuerdo con lo anterior, $Y\sim X_n^2$.
\end{teo}

Desde un punto de vista práctica, la varianza muestra tal como se encuentra definida por 
$$S^2=\sum\limits_{i=1}^n \dfrac{(X_i-\mu)^2}{n}$$
tiene poco uso, ya que es muy raro que se conozca el valor de la media poblacional $\mu$. De acuerdo con lo anterior, si se muestra una distribución normal con media $\mu$ y varianza $\sigma^2$, la varianza muestral se define por
\begin{tcolorbox}
    $$S^2=\sum_{i=1}^n \dfrac{\left(X_i-\overline{X}\right)^2}{n-1}$$
\end{tcolorbox}
En el capítulo 8 se verá por qué se emplea el divisor $(n-1)$. Para determinar la distribución de muestreo de $S^2$, y con base en una muestra aleatoria proveniente de una distribución normal, debe tomarse en cuenta el promedio de la muestra $\overline{X}$. Como resultado se tiene que la distribución de muestreo de $\dfrac{(n-1)S^2}{\sigma^2}$ es también una distribución chi-cuadrada con $n-1$ grados de libertad. Para ello probaremos primero un teorema útil que involucra suma de dos variables aleatorias independientes chi-cuadrada y entonces se escribirá la anterior expresión en una forma equivalente, con objeto de aprovechar este teorema.

%-------------------- teorema 7.6
\begin{teo}
    Si $X_1$ y $X_2$ son dos variables aleatorias independientes y cada una tiene una distribución chi-cuadrada con $v_1$ y $v_2$ grados de libertad respectivamente, entonces:
    $$Y=X_1+X_2$$
    también tiene una distribución chi-cuadrada con $v_1+v_2$ grados de libertad.\\\\
	Demostración.-\; Del teorema 7.1, la función generadora de momentos de $Y$ es
	$$
	\begin{array}{rcl}
	    m_Y(t) &=& m_{X_i}(t)m_{X_2}(t)\\\\
		   &=& (1-2t)^{-v_1/2} (1-2t)^{-v_2/2}\\\\
		   &=& (1-2t)^{-(v_1+v_2)/2}
	\end{array}
	$$
	que es la función generadora de momentos de una variable aleatoria chi-cuadrada con $v_1+v_2$ grados de libertad.
\end{teo}

Ahora se deducirá la distribución de muestreo de $\dfrac{(n-1)S^2}{\sigma^2}$. De  
$$S^2=\sum_{i=1}^n \dfrac{\left(X_i-\overline{X}\right)^2}{n-1}$$
se tiene que
$$(n-1)S^2=\sum_{i=1}^n \left(X_i-\overline{X}\right)^2$$
pero
$$
\begin{array}{rcl}
    \displaystyle\sum_{i=1}^n \left(X_i-\overline{X}\right)^2 &=& \displaystyle\sum_{i=1}^n \left(X_i-\mu-\overline{X}+\mu\right)^2\\\\
							      &=& \displaystyle\sum_{i=1}^n \left[(X_i-\mu)-(\overline{X}-\mu)\right]^2\\\\
							      &=& \displaystyle\sum_{i=1}^n \left[\left(X_i-\mu\right)^2-2\left(X_i-\mu\right)\left(\overline{X}-\mu\right)+\left(\overline{X}-\mu\right)^2\right]\\\\
							      &=& \displaystyle\sum_{i=1}^n \left(X_i-\mu\right)^2-2\left(\overline{X}-\mu\right) \displaystyle\sum_{i=1}^n \left(X_i-\mu\right)+n\left(\overline{X}-\mu\right)^2\\\\
							      &=&\displaystyle\sum_{i=1}^n\left(X_i-\mu\right)^2-2\left(\overline{X}-\mu\right)n\left(\overline{X}-\mu\right)+n\left(\overline{X}-\mu\right)^2\\\\
							      &=& \displaystyle\sum_{i=1}^n \left(X_i-\mu\right)^2-n\left(\overline{X}-\mu\right)^2.
\end{array}
$$

De esta forma

$$(n-1)S^2+n\left(\overline{X}-\mu\right)^2=\sum_{i=1}^n \left(X_i-\mu\right)^2.$$

Al dividir ambos miembros de la expresión anterior por la varianza poblacional $\sigma^2$ se tiene

\begin{tcolorbox}
    $$\dfrac{(n-1)S^2}{\sigma^2}+\dfrac{n\left(\overline{X}-\mu\right)^2}{\sigma^2}=\dfrac{(n-1)S^2}{\sigma^2}+\left(\dfrac{\overline{X}-\mu}{\dfrac{\sigma}{\sqrt{n}}}\right)^2=\dfrac{\displaystyle\sum_{i=1}^n \left(X_i-\mu\right)^2}{\sigma^2}$$
\end{tcolorbox}

Del teorema 7.5 se desprende que $\sum_{i=1}^n (X_i-\mu)^2/\sigma^2$ tiene una distribución chi-cuadrada con $n$ grados de libertad. De manera similar, $\left[(\overline{X}-\mu)/\sigma/\sqrt{n}\right]^2$ también posee una distribución chi-cuadrada con un grado de libertad, dado que $(\overline{X}-\mu)/(\sigma/\sqrt{n})$ es $N(0,1)$. Por lo tanto, si se supone que $(n-1)S^2/\sigma^2$ y $\left[(\overline{X}-\mu)/(\sigma/\sqrt{n})\right]^2$ son variables aleatorias independientes, entonces por el teorema 7.6, cuando se muestrea una población cuya distribución es normal con media y varianza desconocida, la distribución de $(n-1)S^2/\sigma^2$, es chi-cuadrada con $n$-grados de libertad. La función de densidad de probabilidad de $Y=(n-1)S^2/\sigma^2$ se desprende de (5.58) y está dada por:
$$
f(y;n-1)=
\left\{
    \begin{array}{lc}
	\dfrac{1}{\Gamma\left[(n-1)/2\right]2^{(n-1)/2}}y^{\left[(n-1)/2\right]-1}e^{-y/2} & y>0,\\\\
	0&\mbox{para cualquier otro valor.}
    \end{array}
\right.
$$
Nótese, que dado $Y\sim X_{n-1}^2$, $E(Y)=n-1$ y $Var(Y)=2(n-1)$. Además, ya que $Y=(n-1)S^2/\sigma^2$, $S^2=\sigma^2Y/(n-1)$. Por lo tanto
\begin{tcolorbox}
    $$\E\left(S^2\right)=E\left(\dfrac{\sigma^2 Y}{n-1}\right)=\dfrac{\sigma^2}{n-1}E(Y)=\sigma^2,$$
\end{tcolorbox}

y 

\begin{tcolorbox}
    $$\Var\left(S^2\right)=Var\left(\dfrac{\sigma^2Y}{n-1}\right)=\dfrac{\sigma^4}{(n-1)^2}Var(Y)=\dfrac{2\sigma^4}{n-1}.$$
\end{tcolorbox}


\section{La distribución t de Student}
Se recordará que cuando se muestrea una distribución normal con desviación estándar conocida $\sigma$, la distribución de $Z=(\overline{X}-\mu)/(\sigma/\sqrt{n})$ es $N(0,1)$. Desde un punto de vista práctico, la necesidad de conocer $\sigma$ impide formular inferencias con respecto a $\mu$ debido a que generalmente no se conoce el valor de la desviación estándar de la población. Dada la disponibilidad de una muestra aleatoria, el camino lógico que se sigue en este caso es reemplazar $\sigma$ con una estimación $s$, que es el valor de la desviación estándar muestral $S$. Desafortunadamente, cuando lo anterior se lleva acabo, la distribución $(\overline{X}-\mu)/(S/\sqrt{n})$ no es $N(0,1)$. Sin embargo es posible determinar la distribución de muestreo exacta cuando se muestra $N(\mu,\sigma)$, con $\mu$ y $\sigma^2$ desconocidos. Acá se examinará los aspectos teóricos de lo que se conoce como la distribución t de Student.\\

Supóngase que se realiza un experimento en que se observan dos variables aleatorias $X$ y $Z$; $X$ tiene una distribución chi-cuadrada con $v$ grados de libertad y $Z$ una distribución normal con media cero y desviación estándar uno. Sea $T$ otra variable aleatoria que es función de $X$ y $Z$, de manera tal que
$$T=\dfrac{Z}{\sqrt{\dfrac{X}{v}}}.$$
Puesto que los valores de $Z$ se encuentran entre $(-\infty,\infty)$ y los valores de $X$ son positivos. El valor
$$t=\dfrac{z}{\sqrt{\dfrac{x}{v}}}$$
recibe el nombre de valor de la variable aleatoria de $t$ de Student. Lo anterior lleva al siguiente teorema.

%-------------------- Teorema 7.7
\begin{teo}
    Sea $Z$ una variable aleatoria normal estándar y $X$ una variable aleatoria chi-cuadrada con $v$ grados de libertad. Si $Z$ y $X$ son independientes, entonces la variable aleatoria
$$t=\dfrac{z}{\sqrt{\dfrac{x}{v}}}$$
    tiene una distribución t de Student con $v$ grados de libertad y una función de densidad de probabilidad dada por
    $$f(t,v)=\dfrac{\Gamma\left(\dfrac{v+1}{2}\right)}{\sqrt{\pi v}\Gamma\left(\dfrac{v}{2}\right)}\left[1+\left(\dfrac{t^2}{v}\right)\right]^{\frac{-(v+1)}{2}}, \quad -\infty<t<\infty,\; v<0.$$\\
	Deducción.-\; Sea $T$ una variable aleatoria definida por  $T=\dfrac{Z}{\sqrt{X/v}}$. Considere la densidad de probabilidad  de $T$ cuando $X$ se mantiene fija en un valor $x$. Dado que
	$$f_Z(z)=\dfrac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}},$$
	la densidad de probabilidad condicional de 
	$$T=\dfrac{Z}{(z/v)^{1/2}}$$
	se obtiene al considerar la relación inversa
	$$Z=(x/v)^{1/2}T$$
	y al sustituir en $f_Z(z),$ en donde el jacobiano de la transformación es
	$$\dfrac{dz}{dt}=(x/v)^{1/2}.$$
	De esta forma
	$$f(t|x)=\dfrac{(x/v)^{1/2}}{\sqrt{2\pi}}e^{-xt^2/2v},\quad -\infty t<\infty,\quad x>0.$$

	Se sabe que la densidad conjunta de $T$ y $X$ es
	$$f(t,x)=f(t|x)f_X(x).$$

	Dado que $X\sim X_v^2$,
	$$f_X(x)=\dfrac{1}{2^{v/2}\Gamma(v/2)}x^{(v-1)/2}e^{-x/2},\quad x>0.$$

	De esta forma

	$$
	\begin{array}{rcl}
	f(t,x)&=&\dfrac{1}{\sqrt{2\pi v} 2^{v/2}\Gamma(v/2)} x^{(v-1)/2}e^{-\frac{x}{2}-\frac{xt^2}{2v}}\\\\
	      &=& c_1x^{(v-1)/2}e^{-c_2x/2},
	\end{array}
	$$

	en donde $c_1=\dfrac{1}{\sqrt{2\pi v}2^{v/2}\Gamma(v/2)}$ y $c_2=\left[1+\left(t^2/v\right)\right]$. Integrando $f(t,x)$ con respecto a $x$, se obtiene la función de densidad de probabilidad de la distribución t de Student. De acuerdo con lo anterior
	$$
	\begin{array}{rcl}
	f_T(t)&=&c_1\displaystyle\int_0^\infty x^{(v-1)/2} e^{-c_2/2}\; dx\\\\
	&=& c_1 \displaystyle\int_0^\infty (2y/c_2)^{(v-1)/2}e^{-y}(2/c_2)\; dy, \mbox{ donde } y=c_2x/2 \mbox{ y } dx=(2/c_2)\; dy\\\\
	&=& c_1(2/c_2)^{(v+1)/2} \displaystyle\int_0^\infty y^{(v-1)/2}e^{-y}\; dy\\\\
	&=& c_1(2/c_2)^{(v+1)/2} \Gamma\left[(v+1)/2\right]\\\\
	&=& \dfrac{1}{\sqrt{2\pi v}2^{v/2}\Gamma(v/2)}\cdot \dfrac{2^{(v+1)/2}}{\left[1+(t^2/v)\right]^{(v+1)/2}}\Gamma\left[(v+1)/2\right]\\\\
	&=& \dfrac{\Gamma\left[(v+1)/2\right]}{\sqrt{\pi v}\Gamma\left[v/2\right]}\left[1+\left(\dfrac{t^2}{v}\right)\right]^{\frac{-(v+1)}{2}} \quad -\infty<t<\infty.
	\end{array}
	$$

\end{teo}

Conforme se tiene un número mayor de grados de libertad, la distribución t de Student tiende hacia la normal estándar. \\

Puede demostrarse que el valor esperado de $T$ es
$$E(t)=0,\qquad v>1$$

y la varianza está dada por

$$Var(T)=\dfrac{v}{v-2},\quad v>2.$$

En la tabla $F$ se encuentra los valores cuantiles $t_{1-\alpha,2}$ tales que:
$$P(T\leq t_{1-\alpha,v})=\int_{-\infty}^{t_{1-\alpha,v}}f(t;v)\; dt = 1-\alpha,\qquad 0\leq \alpha\leq 1.$$
para los distintos valores de $v$ y de las proporciones acumulativas seleccionadas $1-\alpha$. Por ejemplo, si $v=15$,
$$
\begin{array}{rclcl}
    P(T\leq t_{0.90,15})&=&P(T\leq 1.341)&=&0.90,\\
    P(T\leq t_{0.95,15})&=&P(T\leq 1.753)&=&0.95,\\
    P(T\leq t_{0.99,15})&=&P(T\leq 2.602)&=&0.99.
\end{array}
$$

Dado que la distribución t es simétrica con respecto al cero, para $\alpha>0.5$ los valores cuantiles $t_{1-\alpha,v}$ serán negativos pero sus magnitudes serán las mismas que las de los correspondientes valores que se encuentran en el lado derecho. De esta forma, para $v=15$,
$$
\begin{array}{rclcl}
	P(T\geq t_{0.10,15})&=&P(T\leq -1.341)&=&0.10,\\
	P(T\geq t_{0.05,15})&=&P(T\leq -1.753)&=&0.05,\\
	P(T\geq t_{0.01,15})&=&P(T\leq -2.602)&=&0.01.
\end{array}
$$

Desde un punto de vista práctico, es muy poca la ganancia que se tiene al emplear la distribución t de Student en lugar de la normal estándar cuando $v\geq 30.$\\

Recuérdese que para formular inferencias con respecto a $\mu$ cuando el muestreo se lleva a cabo sobre una distribución normal con media y varianza desconocida, se necesita determinar la distribución $(\overline{X}-\mu)(S/\sqrt{n})$. Cuando se muestrea una distribución $N(\mu,\sigma)$ se sabe, del teorema 7.3, que la distribución de $(\overline{X}-\mu)/(\sigma/\sqrt{n})$ es $N(0,1)$. Para la misma condición, se sabe que, de 
$$\dfrac{(n-1)S^2}{\sigma^2}+\dfrac{n\left(\overline{X}-\mu\right)^2}{\sigma^2}=\dfrac{(n-1)S^2}{\sigma^2}+\left(\dfrac{\overline{X}-\mu}{\dfrac{\sigma}{\sqrt{n}}}\right)^2=\dfrac{\displaystyle\sum_{i=1}^n \left(X_i-\mu\right)^2}{\sigma^2}$$
y del teorema 7.6, la distribución de $(n-1)S^2/\sigma^2$ es chi-cuadrada con $n-1$ grados de libertad. Dado que puede demostrarse que $\overline{X}$ y $S^2$ son independientes, del teorema 7.7 se desprende que la distribución de 
\begin{tcolorbox}
    $$
    \dfrac{\dfrac{\overline{X}-\mu}{\dfrac{\sigma}{\sqrt{n}}}}{\sqrt{\dfrac{(n-1)\dfrac{S^2}{\sigma^2}}{n-1}}}=\dfrac{\overline{X}-\mu}{\dfrac{\sigma}{n}}\cdot \dfrac{\sigma}{\sqrt{S^2}},\quad \mbox{o}\quad T=\dfrac{\overline{X}-\mu}{\dfrac{S}{\sqrt{n}}},
    $$
\end{tcolorbox}
    es la t de Student con $n-1$ grados de libertad.


\section{La distribución de la diferencia entre dos medias muestrales}
Se necesita obtener la distribución $\overline{X}$ e $\overline{Y}$ cuando el muestreo se lleva a cabo sobre dos poblaciones normales independientes con varianzas iguales. Si se supone que el valor de la varianza $\sigma^2$ se conoce del teorema 7.3, se sabe que la distribuciòn de $\overline{X}$ es normal con media $\mu_{Y}$ y varianza $\dfrac{\sigma^2}{n_X}$. La distribución de $\overline{Y}$ también es normal pero con media $\mu_Y$ y varianza $\dfrac{\sigma^2}{n_Y}$. Dado que $\overline{X}$ y $\overline{Y}$ son variables aleatorias independientes normalmente distribuidas, si $a_1=1$ y $a_2=-1$ en el teorema 7.2, la distribución de $\overline{X}-\overline{Y}$ también es normal con media $\mu_X-\mu_Y$ y varianza $\left(\dfrac{\sigma^2}{n}\right)+\left(\dfrac{\sigma^2}{n_Y}\right)=\sigma^2\left(\dfrac{1}{n_X}+\dfrac{1}{n_Y}\right)$. Por tanto, si se conoce el valor de $\sigma^2$, la distribución de 
$$Z=\dfrac{\overline{X}-\overline{Y}-\left(\mu_X-\mu_Y\right)}{\sigma\sqrt{\dfrac{1}{n_X}+\dfrac{1}{n_Y}}}$$

es $N(0,1)$ con varianzas iguales.\\

En este desarrollo se supone que que el valor de $\sigma^2$ era conocido. Sin embargo, es poco probable conocer el valor de $\sigma^2$ para una situación real. Así pues, debe obtenerse la distribución de $\overline{X}-\overline{Y}$ cuando el muestreo se lleve a cabo sobre dos poblaciones normales independientes con varianzas iguales pero desconocidas. Para cada una de las dos muestras aleatorias, pueden definirse las varianzas muestrales $S_x^2$ y $S_Y^2$ dadas por $S^2=\sum\limits_{i=1}^n\dfrac{\left(X_i-\overline{X}\right)^2}{n-1}$. Dado que $\dfrac{(n_X-1)S_X^2}{\sigma^2}$ y $\dfrac{\left(n_Y-1\right)S_Y^2}{\sigma^2}$ son dos variable independientes chi-cuadrada, con $n_X-1$ y $n_Y-1$ grados de libertad respectivamente, por el teorema 7.6, la distribución de 
$$W=\dfrac{(n_X-1)S_X^2}{\sigma^2}+\dfrac{(n_Y-1)S_Y^2}{\sigma^2}$$
también es chi-cuadrada con $n_X+n_Y-2$ grados de libertad. De la expresión $T=\dfrac{Z}{\sqrt{X/v}}$ se desprende el hecho de que el cociente de $Z$ en 
$$Z=\dfrac{\overline{X}-\overline{Y}-\left(\mu_X-\mu_Y\right)}{\sigma\sqrt{\dfrac{1}{n_X}+\dfrac{1}{n_Y}}}$$
y la raíz cuadrada de $W$ divida entre sus grados de libertad tiene una distribución t de Student con $n_X+n_Y-2$ grados de libertad. Esto es,
$$\dfrac{\dfrac{\overline{X}-\overline{Y}-\left(\mu_X-\mu_Y\right)}{\sigma}\sqrt{\dfrac{1}{n_X}+\dfrac{1}{n_Y}}}{\sqrt{\dfrac{\dfrac{\left(n_X-1\right)S_X^2+\left(n_Y-1\right)S_Y^2}{\sigma^2}}{n_X+n_Y-2}}}=\dfrac{\overline{X}-\overline{Y}-\left(\mu_X-\mu_Y\right)}{\sqrt{\dfrac{\left(n_X-1\right)S_X^2+\left(n_Y-1\right)S_Y^2}{n_X+n_Y-2}\left(\dfrac{1}{n_X}+\dfrac{1}{n_Y}\right)}}$$

o

\begin{tcolorbox}
    $$T=\dfrac{\overline{X}-\overline{Y}-\left(\mu_X-\mu_Y\right)}{S_p\sqrt{\dfrac{1}{n_X}+\dfrac{1}{n_Y}}},$$
\end{tcolorbox}

en donde 

$$S_P^2=\dfrac{\left(n_X-1\right)S_X^2+\left(n_Y-1\right)S_Y^2}{n_X+n_Y-2}.$$

Que en general recibe el nombre de \textbf{estimador combinado de la varianza común \boldmath $\sigma^2$}. \\

Si las varianzas $\sigma_X^2$ y $\sigma_Y^2$ no son iguales, pero conocen sus valores, el problema es sencillo. La distribución de
\begin{tcolorbox}
    $$Z=\dfrac{\overline{X}-\overline{Y}-\left(\mu_X-\mu_Y\right)}{\sqrt{\dfrac{\sigma_X^2}{n_X}+\dfrac{\sigma_Y^2}{n_Y}}}$$
\end{tcolorbox}

aún es $N(0,1)$. 


\section{La distribución F}

Recuérdese que las inferencias con respecto a $\sigma^2$ cuando se muestrea una distribución normal, se formulan con base en la estadística $\dfrac{(n-1)S^2}{\sigma^2}$, la que tiene una distribución chi-cuadrada con $n-1$ grados de libertad. En este apartado se formulará inferencias con respecto a las varianzas de dos distribuciones normales independientes con base en las muestras aleatorias de cada uno.\\

Supóngase un experimento en que se observan dos variables aleatorias independientes $X$ e $Y$, cada una con una distribución chi-cuadrada con $v_1$ y $v_2$ grados de libertad respectivamente. Sea $F$ una variable aleatoria que es función de $X$ e $Y$, de manera tal que

$$F=\dfrac{\frac{X}{v_1}}{\frac{Y}{v_2}}.$$

Esto lleva al siguiente teorema

%-------------------- Teorema 7.8
\begin{teo}
    Sean $X$ e $Y$ dos variables aleatorias independientes chi-cuadrada con $v_1$ y $v_2$ grados de libertad, respectivamente. La variable aleatoria

$$F=\dfrac{\frac{X}{v_1}}{\frac{Y}{v_2}}.$$

tiene una distribución $F$ con una función de densidad de probabilidad dada por

$$
g\left(f;v_1,v_2\right) = 
\left\{
    \begin{array}{ll}
	\dfrac{\Gamma\left[\dfrac{v_1+v_2}{2}\right]v_1^{\frac{v_1}{2}}v_2^{\frac{v_2}{2}}}{\Gamma\left(\frac{v_1}{2}\right)\Gamma\left(\frac{v_2}{2}\right)}f^{\frac{v_1+v_2}{2}} & f>0,\\\\
	0 & \mbox{para cualquier otro valor}.
    \end{array}
\right.
$$
\end{teo}

El \textbf{valor esperado} es

\begin{tcolorbox}
    $$\E(F)=\dfrac{v_2}{v_2-2},\qquad v_2>2.$$
\end{tcolorbox}

y la \textbf{varianza} está dada por

\begin{tcolorbox}
    $$\Var(F)=\dfrac{v_2^2\left(2v_2+2v_1-4\right)}{v_1\left(v_2-2\right)^2\left(v_2-4\right)}\qquad v_2>4.$$
\end{tcolorbox}

La distribución $F$ tiene asimetría postiva para cualesquiera valores de $v_1$ y $v_2$, pero esta va disminuyendo conforme $v_1$ y $v_2$ toman valores cada vez más grandes.\\

Los valores \textbf{cuantiles} $f_{1-\alpha,v_1,v_2}$ están dados dados por

$$P(F\leq f_{1-\alpha,v_1,v_2})=\int_0^{f_{1-\alpha,v_1,v_2}}g\left(f;v_1,v_2\right)\; df=1-\alpha, \quad 0\leq a\leq 1.$$\\

Regresando anuestro objetivo. Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria de variables independientes y normalmente distribuidas cada una con media $\mu_X$ y varianza $\sigma^2_X$. Tambièn sea $Y_1,Y_2,\ldots,Y_n$ un conjunto de $n_Y$ variables aleatorias independientes normalmente distribuidas, cada una con media $\mu_Y$ y varianza $\sigma_Y^2$. Si se supone que las $X$ y las $Y$ son independientes, las estadísticas

$$\dfrac{(n_X-1)S_X^2}{\sigma_X^2}$$

y

$$\dfrac{(n_Y-1)S_Y^2}{\sigma_Y^2}$$

son dos variables aleatorias chi-cuadrada independientes con $n_X-1$ y $n_Y-1$ grados de libertad respectivamente. Entonces, por el teorema 7.9, se despenrede que la variable aleatoria

$$\dfrac{\dfrac{\dfrac{(n_X-1)S_X^2}{\sigma_X^2}}{n_X-1}}{\dfrac{\dfrac{(n_Y-1)S_Y^2}{\sigma_Y^2}}{n_Y-1}}=\dfrac{\dfrac{S_X^2}{\sigma_X^2}}{\dfrac{S_Y^2}{\sigma_Y^2}}.$$

Si la suposición de que $\sigma_X^2=\sigma_Y^2$ es correcta, la estadística $F$, se deduce a
$$F=\dfrac{S_X^2}{S_Y^2}.$$

Si las dos varianzas son iguales, la probabilidad de observar un valor de $F$ distinto, de manera sufientes, es pequeña.

tiene una distribución $F$ con $n_X-1$ y $n_Y-1$ grados de libertad.\\

Para finalizar debe notarse que en esta sección, así como en las secciones 7.5 y 7.7 se desarrolló el material que se presentó bajo la hipótesis de realizar un muestreo aleatorio sobre poblaciones que tienen una distribución normal.


