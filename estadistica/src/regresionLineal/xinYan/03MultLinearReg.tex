\chapter{Regresión lineal múltiple}
La regresión múltiple permite a los investigadores examinar el efecto de más de una variable independiente en la respuesta al mismo tiempo. Para algunas preguntas de investigación, la regresión se puede utilizar para examinar en qué medida un conjunto particular de variables independientes puede explicar suficientemente el resultado.

\section{Espacio vectorial y Proyección}

\subsection{Espacio vectorial}
Un espacio vectorial es un conjunto de vectores  que es cerrado bajo  la adición y la multiplicación por un escalar de vectores finitos. Suponga $V$ es cerrado bajo el vector adición en $\mathbb{R}^n$: si $u,v\in V$ entonces $u+v\in V$. Por otro lado $V$ es cerrado bajo la multiplicación escalar: Si $a\in \mathbb{R}^1$ y $v\in V$, entonces $av\in V$. Por lo que $V$ es un espacio vectorial en $\mathbb{R}^n$. \\

Una base hace posible expresar cada vector del espacio como una tupla única de los elementos del campo, aunque se debe tener precaución cuando un espacio vectorial no tiene una base finita. En álgebra lineal, una base es un conjunto de vectores que, en una combinación lineal, pueden representar cada vector en un espacio vectorial dado, y tales que ningún elemento del conjunto pueda representarse como una combinación lineal de los demás. En otras palabras, una base es un conjunto generador linealmente independiente. Es decir, cualquier vector $x'=(x_1,x_2,\ldots,x_n)$ en $\mathbb{R}^n$ puede ser una combinación lineal de $e_1,e_2,\ldots,e_n$. En efecto,

$$x=x_1e_1+x_2e_2+x_3e_3,+\cdots+x_ne_n.$$

Esta representación es única. Que quiere decir que no existe otra representación tal que

$$x=x_1^*e_1+x_2^*e_2+x_3^*e_3,+\cdots+x_n^*e_n.$$

Ya que,

$$0=\textbf{x}-\textbf{x}=(x_1-x_1^*)e_1+(x_2-x_2^*)+\ldots+(x_n-x_n^*)e_n$$

Entonces, $x_i=x_i^*$ para todo $i=1,2,\cdots , n.$\\ 

Dado un espacio vectorial $V$, un subconjunto no vacío $W$ de $V$ que es cerrado bajo la suma y la multiplicación escalar se llama subespacio de $V$. La intersección de todos los subespacios que contengan un conjunto dado de vectores es llamado SPAN o generador. Si no se puede eliminar ningún vector sin cambiar el SPAN (generador), se dice que los vectores en este conjunto son linealmente independientes. Un conjunto linealmente independiente cuyo intervalo es $V$ se llama base de $V$. Un vector generado (SPAN) por dos vectores $v$ y $w$ (ambas no necesariamente independientes) se pueden definir como: $x:x=av+bw,$ para todo $(a,b)\in \mathbb{R}^2$. Si un espacio vectorial $S$ está generado por un conjunto de vectores independientes $v_1,v_2,\ldots , v_p$. Es decir, $S$ es el conjunto de vectores 
$$\left\{x:x=a_1+v_1+a_2v_2+\ldots+a_pv_p,\mbox{ para todo } (a_1,a_2,\ldots,a_p)\in \mathbb{R}^p\right\},$$

Entonces la dimensión de $S$ es $p$. Los vectores $v_1, v_2, \ldots , v_p$ son la base del espacio vectorial $S$. La dimensión de un espacio vectorial $S$ es el mayor número de un conjunto de vectores independientes en $S$. Si la dimensión de un espacio lineal $S$ es $p$, se escribe $Dim(S) = p$.
 

\subsection{Vectores linealmente independientes}

Si existe un número finito de vectores distintos $v_1,v_2,\ldots,v_n$ en el espacio vectorial $V$ y los escalares $a_1,a_2,\ldots,a_n$ no todos cero, tal que
$$a_1v_1+a_2v_2+\ldots,a_nv_n=0,$$
entonces los vectores $v_1,v_2,\ldots,v_n$ se dice que son linealmente dependientes. Si $v_1,v_2,\ldots,v_n$ son dependientes entonces de estos $n$ vectores hay al menos un vector que puede expresarse como una combinación lineal de otros vectores. Tenga en cuenta que el cero a la derecha es el vector cero, no el número cero. Si no existen tales escalares, entonces se dice que los vectores $v_1, v_2, \ldots , v_n$ son linealmente independientes. Esta condición se puede reformular de la siguiente manera: Siempre que $a_1 , a_2 , \ldots , a_n$ sean escalares tales que

$$a_1v_1+a_2v_2+\ldots,a_nv_n=0,$$

tenemos $a_i=0$ para $i=1,2,\ldots,n$. Entonces, $v_1,v_2,\ldots,v_n$ son linealmente independientes.\\

Una base de un espacio vectorial $V$ se define como un subconjunto de vectores en $V$ que son linealmente independientes y estos vectores generan el espacio $V$. En consecuencia, si $(v_1 , v_2 , \ldots , v_n)$ es una lista de vectores en $V$, entonces estos vectores forman una base si y solo si todo vector $x \in V$ puede expresarse de forma única mediante una combinación lineal de $v_1 , v_2 , \ldots , v_p$. Es decir, 
$$x = a_1 v_1 + a_2 v_2 + \ldots + a_n v_n,\mbox{ para cualquier } \textbf{x}\in V.$$ 
El número de vectores base en $V$ se denomina dimensión del espacio lineal $V$. Tenga en cuenta que un espacio vectorial puede tener más de una base, pero el número de vectores que forman la base del espacio vectorial $V$ siempre es fijo. Es decir, la dimensión del espacio vectorial $V$ es fija pero habrá más de una base. De hecho, si la dimensión del espacio vectorial $V$ es $n$, entonces cualquier n vector linealmente independiente en $V$ forma su base.


\subsection{Producto punto y Proyección}

Si $\textbf{x}=(x_1,x_2,\ldots,x_n)$ e $\textbf{y}=(y_1,y_2,\ldots, y_n)$ dos vectores en un espacio Euclidiano vectorial $\textbf{R}^n$. El producto punto de los vectores $\textbf{x}$ e $\textbf{y}$ de define como

$$\textbf{x}\cdot \textbf{y} = x_1y_1+x_2y_2+\cdots + x_ny_n.$$

Dos vectores se llaman ortogonales si su  producto vectorial es $0$. Si $\theta$ es el ángulo entre dos vectores, el coseno del angulo se define por

\begin{tcolorbox}
    \begin{equation}
	\cos(\theta)=\dfrac{\textbf{x}\cdot \textbf{y}}{|\textbf{x}||\textbf{y}|}=\dfrac{x_1y_1+x_2y_2+\ldots  +x_ny_n}{\sqrt{x_1^2+x_2^2+\ldots + x_n^2}\sqrt{y_1^2+y_2^2+\ldots + y_n^2}}.
    \end{equation}
\end{tcolorbox}

Dos vectores ortogonales forman $90^\circ$; Es decir, son perpendiculares. Una aplicación importante del producto punto es la proyección. La proyección de un vector \textbf{y} sobre otro vector \textbf{x} forma un nuevo vector que tiene la misma dirección que el vector \textbf{x} y la longitud $|y|\cos(\theta)$, donde $|\textbf{y}|$ denota la longitud del vector \textbf{y} y $\theta$ es el ángulo entre los vectores. Escribimos la proyección como $P_{\textbf{x}}\textbf{y}$. La proyección vectorial puede ser expresado como

$$
\begin{array}{rcl}
    P_{\textbf{x}}\textbf{y} &=& |\textbf{y}|\cos(\theta)\dfrac{\textbf{x}}{|\textbf{x}|}\\\\
			     &=& |\textbf{y}|\dfrac{\textbf{x}\cdot \textbf{y}}{|\textbf{x}||\textbf{y}|}\dfrac{\textbf{x}}{|\textbf{x}|} \\\\
			     &=& \dfrac{x_1y_1+x_2y_2+\cdots + x_ny_n}{x_1^2+x_2^2+\cdots + x_n^2}\textbf{x}\\\\
			     &=& \lambda \textbf{x}.\\\\
\end{array}
$$

Donde $\lambda$ es un escalar, 

\begin{tcolorbox}
    \begin{equation}
	\lambda=\dfrac{x_1y_1+x_2y_2+\cdots x_ny_n}{x_1^2+x_2^2 + \cdots + x_n^2}=\dfrac{\textbf{x}\cdot \textbf{y}}{\textbf{x}\cdot \textbf{x}}.
    \end{equation}
\end{tcolorbox}

Así, la proyección de \textbf{y} sobre el vector \textbf{x} es un vector \textbf{x} que multiplica un escalar $\lambda$ donde $\lambda$ es el $\cos(\theta)$ y $\theta$ es el ángulo entre los vectores \textbf{x} e \textbf{y}.\\

Si \textbf{x} e \textbf{y} son dos vectores en $\mathbb{R}^n$. Consideremos la diferencia vector entre el vector \textbf{e}, $\textbf{e}=\lambda \textbf{x}-\textbf{y}$, y $\;\lambda = \textbf{x}\cdot\textbf{y}/\textbf{x}\cdot \textbf{x}.$ El vector \textbf{e} es perpendicular para el vector \textbf{x} donde $\lambda=(\textbf{x}\cdot \textbf{y})/(\textbf{x}\cdot \textbf{x})$. Para ver esto, simplemente calculamos el producto escalar de \textbf{e} y \textbf{x}:

$$\textbf{e}\cdot \textbf{x} = (\lambda \textbf{x}-\textbf{y})\cdot\textbf{x}=\lambda \textbf{x}\cdot \textbf{x}-\textbf{x}\cdot \textbf{y}=\left(\dfrac{\textbf{x}\cdot \textbf{y}}{\textbf{x}\cdot \textbf{x}}\right)\textbf{x}\cdot \textbf{x}-\textbf{x}\cdot\textbf{y}=0.$$

Por lo tanto, el ángulo entre \textbf{e} y \textbf{x} es $90^\circ$. Es decir, estos vectores son perpendiculares. Además, dado que \textbf{e} es perpendicular a \textbf{x}, es el vector con la distancia más corta entre todos los vectores que comienzan desde el final de \textbf{y}, y terminan en cualquier punto de \textbf{x}.\\

Si un espacio vectorial tiene una base y la longitud de los vectores base es una unidad, entonces esta base es una base ortonormal. Cualquier base dividida por su longitud forma una base ortonormal. Si $S$ es un subespacio p-dimensional de un espacio vectorial $V$, entonces es posible proyectar vectores en $V$ sobre $S$. Si el subespacio $S$ tiene una base ortonormal $(w_1, w_2, \ldots , w_p)$, para cualquier vector \textbf{y} en $V$, la proyección de \textbf{y} sobre el subespacio $S$ es

\begin{equation}
    P_{S}\textbf{y}=\sum_{i=1}^p (\textbf{y}\cdot w_i)w_i.
\end{equation}

Sean dos subespacios $S$ y $T$ del espacio vectorial $V$ y la unión $S\cup T = V$. Si para cualquier vector $\textbf{x}\in S$ y cualquier $\textbf{y}\in T$, tal que el producto punto $\textbf{x}\cdot \textbf{y}=0$, entonces se dice que los dos espacios vectoriales $S$ y $T$ son ortogonales. O podemos decir que $T$ es el espacio ortogonal de $S$, denotado por 
$$T=S^{\perp}.$$

Así, para un espacio vectorial $V$, si $S$ es un subespacio vectorial en $V$, entonces $V=S\cup S^{\perp}$. Cualquier vector $\textbf{y}\in V$ puede ser escrito de manera única como $\textbf{y}_{S}+\textbf{y}_S^{\perp}$, donde $\textbf{y}_S\in S$ e $\textbf{y}_S^{\perp}$ es en $S^{\perp}$, el subespacio ortogonal de $S$.\\

Una proyección de un vector sobre un espacio lineal $S$ es en realidad una transformación lineal del vector y se puede representar mediante una matriz de proyección multiplicada por el vector. Una matriz de proyección $P$ es una matriz cuadrada de $n \times n$ que da la proyección de $\mathbb{R}^n$ al subespacio $S$. Las columnas de $P$ son las proyecciones de los vectores base estándar, y $S$ es la imagen de $P$. Para la matriz de proyección tenemos los siguientes teoremas.

%-------------------- Teorema 3.1 --------------------
\begin{teo}
    Un matriz cuadrada $P$ es una matriz de proyección si y solo si es idempotente. Es decir, $P^2=P$.
\end{teo}

%-------------------- Teorema 3.2 --------------------
\begin{teo}
    Sea $U=(u_1,u_2,\cdots , u_k)$ una base ortonormal para un subespacio $W$ del espacio lineal $V$. La matriz $UU'$ es una matriz proyección de $V$ sobre $W$. Es decir, para cualquier valor $v\in V$ la proyección  de $v$ sobre $W$ es $\Proj_W v=UU'v$.
\end{teo}

La matriz $UU'$ es llamada matriz proyección  par el subespacio $W$. No depende de la elección de la base ortonormal. Si no partimos de una base ortonormal de $W$, podemos construir la matriz de proyección. Esto se puede resumir en el siguiente teorema.

%-------------------- Lema 3.1 --------------------
\begin{lema}
    Suponga que $A$ es una matriz $a\times k$ donde sus columnas son linealmente independientes. Entonces, $AA'$ es invertible.\\\\
	Demostración.-\; Considere la transformación $A:\mathbb{R}^k\to \mathbb{R}^k$ determinado por $A$. Ya que las columnas de $A$ son linealmente independientes, esta transformación es uno a uno. Además, el espacio nulo de $A'$ es ortogonal al espacio columna de $A$. Por lo tanto, $A'$ es uno a uno en el espacio columna de $A$ y como resultado, $A'A$ es una transformación uno a uno $\mathbb{R}^k\to \mathbb{R}^k$. Por el teorema de matriz invertida, $A'A$ es invertible.
\end{lema}

%-------------------- Teorema 3.3 --------------------
\begin{teo}
    Sea $A=(a_1,a_2,\ldots,a_k)$ cualquier base para un subespacio $W$ de $V$. La matriz $A(A'A)^{-1}A'$ es una matriz proyección de $V$ sobre $W$. Es decir, para cualquier vector $v\in V$ la proyección de $v$ sobre $W$ es
    \begin{equation}
	\Proj_W v=A(A'A)^{-1}A'v.
    \end{equation}
	Demostración.-\; Derivemos ahora la matriz de proyección para el espacio de columnas de $A$. Obsérvese que cualquier elemento del espacio de columnas de $A$ es una combinación lineal de las columnas de $A$, es decir, $x_1 a_1 + x_2 a_2 + \cdots + x_k a_k$ . Si escribimos
	$$
	x=
	\left(
	    \begin{array}{c}
		x_1\\
		x_2\\
		\vdots\\
		x_k
	    \end{array}
	\right),
	$$

	entonces, tenemos

	$$w=(a_1,a_2,\ldots,a_k)
	\left(
	    \begin{array}{c}
		x_1\\
		x_2\\
		\vdots\\
		x_k
	    \end{array}
	\right)
	=x_1a_1+x_2a_2+\cdot + x_ka_k=A\textbf{x}.$$

	Ahora, para cualquier vector $v\in \mathbb{R}^n$, denotemos la proyección de $v$ sobre $W$ por $x_p$.

	$$\Proj_w v = Ax_p.$$

	La matriz de proyección se puede encontrar calculando $x_p$. La proyección del vector $v$ sobre $W$ se caracteriza por el hecho de que $v - \Proj_W v$ es ortogonal a cualquier vector $w$ en $W$. Así tenemos

	$$w\cdot (v-\Proj_w v)=0$$

	para todo $w$ in $W$. Ya que $w=A\textbf{x}$ para algún $\textbf{x}$, tenemos

	$$A\textbf{x}(v-Ax_p)=0$$

	para todo $\textbf{x}\in \mathbb{R}^n$. Escribir este producto escalar en términos de matrices da como resultado

	$$(A\textbf{x})'(v-A\textbf{x}_p)=0$$

	el cual es equivalente a

	$$(\textbf{x}'A')(v-A\textbf{x}_p)=0.$$

	Convirtiendo a producto punto, tenemos

	$$\textbf{x}\cdot A'(v-A\textbf{x}_p)=0.$$

	De donde,

	$$\textbf{x}\cdot A'v-\textbf{x}\cdot A'A\textbf{x}_p \quad \Rightarrow \quad A'v=A'A\textbf{x}_p.$$

	Luego, ya que $A'A$ es invertible, se tiene

	$$(A'A)^{-1}A'v = \textbf{x}_p.$$

	Después, ya que $A\textbf{x}_p$ es la proyección deseada,

	$$A(A'A)^{-1}A'v=A\textbf{x}_p=\Proj_w v$$

	Por lo tanto, concluimos que la matriz proyección para $W$ es $A(A'A)^{-1}A'.$
\end{teo}

La matriz proyección es muy útil en las discusiones posteriores del modelo de regresión $Y=\textbf{X}\beta+\epsilon$. Una matriz cuadrada $P=\textbf{X}(\textbf{X}\textbf{X}')^{-1}X'$, se construye utulizando la matriz de diseño. Se puede verificar fácilmente que $P$ es una matriz idempotente:

$$
\begin{array}{rcl}
    P^2&=&\textbf{X}(\textbf{X}\textbf{X}')^{-1}\textbf{X}'\textbf{X}(\textbf{X}\textbf{X}')^{-1}\textbf{X}'\\\\
       &=&\textbf{X}(\textbf{X}\textbf{X}')^{-1}(\textbf{X}'\textbf{X})(\textbf{X}\textbf{X}')^{-1}\textbf{X}'\\\\
       &=&\textbf{X}(\textbf{X}\textbf{X}')^{-1}(\textbf{X}\textbf{X}')(\textbf{X}\textbf{X}')^{-1}\textbf{X}'\\\\
       &=& \textbf{X}(I)(\textbf{XX}')^{-1}\textbf{X}'\\\\
       &=&P
\end{array}
$$

Así, $P=\textbf{X}(\textbf{X}\textbf{X}')^{-1}\textbf{X}'$ es una matriz proyección. Además, si definimos una matriz como $I-P=I-\textbf{X}(\textbf{X}\textbf{X}')^{-1}\textbf{X}'$. Es fácil ver que $I-P$ es también idempotente. De hecho,

$$(I-P)^2=I-2P+P^2=I-2P+P=I-P.$$

Por lo tanto, $I-P=I-\textbf{X}(\textbf{X}\textbf{X}')^{-1}\textbf{X}'$ es una matriz proyección. En las secciones siguientes veremos cómo se utilizan estas matrices de proyección para obtener el mejor estimador lineal insesgado (BLUE) para el modelo de regresión lineal y cómo se utilizan en el diagnóstico del modelo de regresión.


\section{Forma matricial de regresión lineal múltiple}

% -------------------- definición 3.1
\begin{def.}
    Un modelo lineal es definido como un modelo que es lineal en los parámetros de regresión, es decir, lineal en los $\beta_i'$s.
\end{def.}


\section{Forma cuadrática de variables aleatorias}

% -------------------- definición 3.2
\begin{def.}
    Sea $y'=(y_1,y_2,\cdots,y_n)$, $n$ variables reales y $a_{ij}$ $n\times n$ números reales, donde $i,j=1,2,\cdots,n.$ Una forma cuadrática de $y_1,y_2,\cdots,y_n$ es definida como
    $$f(y_1,y_2,\cdots,y_n)=\sum_{i=1}^n\sum_{j=1}^n a_{ij}y_iy_j=\textbf{y}'A\textbf{y}.$$
\end{def.}

Esta forma cuadrática puede ser escrita en la forma matricial: $\textbf{y}'A\textbf{y}$, donde $A$ es un matriz $n\times n$, $A=(a_{ij})_{n\times n}$.\\

En el entorno clásico, los parámetros de un modelo de regresión lineal se estiman minimizando la suma de los residuos al cuadrado:

$$
\begin{array}{rcl}
    \textbf{b} &=& (b_0,b_1,\cdots, b_k)\\\\
	       &=& \arg\min_{(\beta_0,\beta_1,\cdots,\beta_k)}\displaystyle\sum_{i=1}^n \left[y_i-\left(\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+\cdots+\beta_{k}x_{ki}\right)\right]^2.
\end{array}
$$

Este residuo al cuadrado es en realidad una forma cuadrática. Por lo tanto, es importante discutir algunas propiedades generales de esta forma cuadrática que se usarán en las discusiones subsiguientes.


\section{Matrices idempotentes}

% -------------------- definición 3.3
\begin{def.}
    Una matriz simétrica $n\times n$, $A$ es idempotente si $A^2=A.$
\end{def.}

Sea $\alpha=(\alpha_1,\alpha_2,\ldots,\alpha_k)$ un vector k-dimensional y $A$ es un matriz $k\times k$ . $\alpha'A\alpha$ es una forma cuadrática de $\alpha_1,\alpha_2,\cdots,\alpha_k$. Cuando $A$ es una matriz indempotente, la forma cuadrática correspondiente tiene propiedades particulares. La forma cuadrática con matrices idempotentes se usa ampliamente en el análisis de regresión lineal. Ahora discutimos las propiedades de la matriz idempotente.

% -------------------- teorema 3.4
\begin{teo}
    Sea $A_{n\times n}$ una matriz idempotente de rango $p$, entonces los valores propios de $A$ son $0$ o $1$.\\\\
	Demostración.-\; Sea $\lambda_i$ y $v_i$ el valor propio y el correspondiente vector propio normalizado de la matriz $A$, respectivamente. Entonces, tenemos $Ab_i=\lambda_iv_i$, y $v_i' Av_i=\lambda_iv_i' v_i=\lambda_i$. Por otro lado, ya que $A^2=A$, podemos escribir
	$$\lambda_i=v_i'Av_i=v_i'A^2v_i=v_i'A'v_i=(Av_i)'Av_i=(\lambda_iv_i)(\lambda_iv_i)=\lambda_i^2.$$
	Dado que, tenemos $\lambda_i(\lambda_i-1)=0$, se tiene $\lambda_i=1$ o $\lambda_i=0$. Esto completo la demostración.
\end{teo}

Es fácil saber que $p$ valores propios de $A$ son 1 y $n - p$ valores propios de $A$ son cero. Por lo tanto, el rango de una matriz idempotente $A$ es la suma de sus valores propios distintos de cero.

% -------------------- Definição 3.4
\begin{def.}
    Sea $A=(a_{i,j})_{n\times n}$ una matriz $n\times n$, la traza de $A$ se define como la suma de los elementos ortogonales; es decir,

    $$tr(A)=a_{11}+a_{22}+\cdots+a_{nn}.$$
\end{def.}

Si $A$ es una matriz simétrica. Entonces, la suma de todos los elementos cuadrados de $A$ puede ser expresado mediante $tr\left(A^2\right)$. Es decir, $\sum_{i,j}A_{ij}^2=tr\left(A^2\right).$ Es fácil verificar que $tr(AB)=tr(BA)$ para cualquieras dos matrices $A$ y $B$. El siguiente teorema da la relación entre el rango de la matriz $A$ y la traza de $A$ cuando $A$ es una matriz idempotente.

% -------------------- teorema 3.5
\begin{teo}
    Si $A$ es una matriz idempotente, entonces $tr(A)=rango(A)=p.$\\\\
	Demostración.-\; Si el rango de una matriz $A$ idempotente $n\times n$ es $p$. entonces $A$ tiene $p$ valores propios de $1$ y $n-p$ valores propios de $0$. Por lo tanto, podemos escribir $rango(A)=\sum_{i=1}^n\lambda_i=p$. Ya que $A^2=A$, los valores propios de la matriz indempotente $A$ es $1$ o $0$. De la teoría de matrices existe una matriz ortogonal $V$ tal que
	$$
	V' A V=
	\left(
	    \begin{array}{cc}
		I_p & 0 \\
		0 & 0
	    \end{array}
	\right).
	$$
	Así, tenemos
	$$tr(V'AV)=tr(VV'A)=tr(A)= 
	\left(
	    \begin{array}{cc}
		I_p & 0 \\
		0 & 0
	    \end{array}
	\right)
	=p=rango(A).
	$$
	Aquí, usamos el simple hecho: $tr(AB)=tr(BA)$ para cualesquier matrices $A_{n\times n}$ y $B_{n\times n}$.
\end{teo}

Una forma cuadrática de un vector aleatorio $\textbf{y}' = (y_1 , y_2 , \cdots , y_n)$ se puede escribir en una forma matricial $\textbf{y}'A\textbf{y}$, donde $A$ es una matriz $n \times n$. Es de interés encontrar la esperanza y la varianza de $\textbf{y}'A\textbf{y}$. El siguiente teorema da  el valor esperado de $\textbf{y}'A\textbf{y}$ cuando los componentes de $\textbf{y}$ son independientes.

% -------------------- teorema 3.6
\begin{teo}
    Sea $\textbf{y}'=(y_1,y_2,\ldots,y_n)$ un vector aleatorio $n\times 1$ con media $\mu'=(\mu_1,\mu_2,\cdots,\mu_n)$ y varianza $\sigma^2$ para cada componente. Además, se supone que $y_1,y_2,\cdots,y_n$ son independientes. Sea $A$ una matriz $n\times n$, $\textbf{y}'A\textbf{y}$ es una forma cuadrática de variables aleatorias. La esperanza de esta forma cuadrática viene dada por
    $$\E(\textbf{y}'A\textbf{y})=\sigma^2tr(A)+\mu'A\mu.$$\\
	Demostración.-\; Primero observemos que
	$$\textbf{y}'A\textbf{y}=(\textbf{y}-\mu)'A(\textbf{y}-\mu)+2\mu'A(\textbf{y}-\mu)+\mu'A\mu.$$
	De donde podemos escribir,
	$$
	\begin{array}{rcl}
	    \E\left(\textbf{y}'A\textbf{y}\right) &=& \E\left[(\textbf{y}-\mu)'A(\textbf{y}-\mu)\right] + 2\E\left[\mu'A(\textbf{y}-\mu)\right]+\mu'A\mu\\\\
						  &=& \E\left[\displaystyle\sum_{i,j=1}^n a_{ij}(y_i-\mu_i)(u_j-\mu_j)\right]+2\mu'A\E(\textbf{y}-\mu)+\mu'A\mu\\\\
						  &=& \displaystyle\sum_{i=1}^n a_{ii}E(y_i-\mu_i)^2+\mu'A\mu\\\\
						  &=& \sigma^2tr(A)+\mu'A\mu.
	\end{array}
	$$
\end{teo}

Ahora veremos la varianza de la forma cuadrática de $\textbf{y}'A\textbf{y}$.

% -------------------- teorema 3.7
\begin{teo}
    Sea $\textbf{y}$ un vector aleatorio $n\times 1$ con media $\mu'=(\mu_1,\mu_2,\cdots,\mu_n)$ y varianza $\sigma^2$ para cada componente. Se asume que $y_1,y_2,\ldots,y_n$ son independientes. Sea $A$ una matriz simétrica $n\times n$, $\E(y_i-\mu_i)^4=u_i^{(4)}$, $\E(y_i-\mu_i)^3=u_i^{(3)}$, y $a'=(a_{11},a_{22},\cdots,x_{nn})$. La varianza de la forma cuadrática $\textbf{y}A\textbf{y}$ es dada por 
    $$
    \begin{array}{rcl}
	\Var\left(\textbf{y}'A\textbf{y}\right) &=& \left(\mu^{(4)}-3\sigma^2\right)a'a+\sigma^4\left(2tr\left(A^2\right)+\left[tr(A)\right]^2\right) + 4\sigma^2\mu'A^2\mu+4\mu^{(3)}a'A\mu.
    \end{array}
    $$
    \vspace{.3cm}

	Demostración.-\; Sea $Z=\textbf{y}-\mu,$ $A(A_1,A_2,\ldots,A_n)$ y $b=(b_1,b_2,\ldots,b_n)=\mu'(A_1,A_2,\ldots,A_n)=\mu'A$, de donde podemos escribir
    $$
    \begin{array}{rcl}
	\textbf{y}'A\textbf{y} &=& (\textbf{y}-\mu)A(\textbf{y}-\mu)+2\mu'A(\textbf{y}-\mu)+\mu'A\mu\\\\
			       &=& Z'AZ+2b'Z+\mu'A\mu\\\\
    \end{array}
    $$
    Así,
    $$\Var(\textbf{y}'A\textbf{y})=\Var\left(Z'AZ\right)+4\Var(bZ)+4\Cov\left(Z'AZ,bZ\right)$$
    Calculando cada termino por separado, se tiene

    $$
    \begin{array}{rcl}
	\left(ZA'Z\right)^2 &=& \displaystyle\sum_i\sum_j a_{ij}a_{lm} Z_iZ_jZ_lZ_m\\\\
	\E\left(ZA'Z\right)^2 &=& \displaystyle\sum_i\sum_j\sum_l\sum_m a_{ij}a_{lm} \E\left(Z_iZ_jZ_lZ_m\right)\\\\
    \end{array}
    $$

    Notemos que

    $$
    \E\left(Z_iZ_jZ_lZ_m\right) = 
    \left\{
	\begin{array}{rcl}
	    \mu^{(4)}, &\text{si} & i=j=l=m;\\\\
	    \sigma^4, &\text{si} & i=j, l=k\; \mbox{o}\; i=l, j=k, \mbox{o}\; i=k, j=l;\\\\
	    0, & &\mbox{los demás}.
	\end{array}
    \right.
    $$

    De donde se tiene

    $$
    \begin{array}{rcl}
	\E\left(Z'AZ\right)^2 &=& \displaystyle\sum_i\sum_j\sum_l\sum_m a_{ij}a_{lm}\E\left(Z_iZ_jZ_lZ_m\right)\\\\
			      &=& \mu^{(4)}\displaystyle\sum_{i=1}^n a_{i}^2 +\sigma^4 \left(\sum_{i\neq j}a_{ii}a_{kk}+\sum_{i\neq j}a_{ij}^2+\sum_{i\neq j}a_{ij}a_{ji}\right).
    \end{array}
    $$

    Ya que $A$ es simétrica. $a_{ij}=a_{ji}$, tenemos

    $$
    \begin{array}{rcl}
	\displaystyle\sum a_{ij}^2 + \sum_{i\neq j} a_{ij}a_{ji} &=& 2\displaystyle\sum_{i\neq j}a_{ij}^2\\\\
								 &=& 2\displaystyle\sum_{i,j} a_{ij}^2 - 2\sum_{i=j}a_{ij}^2\\\\
								 &=& 2tr\left(A^2\right)-2\displaystyle\sum_{i=1}^n a_{ii}^2\\\\
								 &=& 2tr\left(A^2\right)-2a'a.
    \end{array}
    $$

    Y

    $$
    \begin{array}{rcl}
	\displaystyle\sum_{i\neq k} a_{ii}a_{kk} &=& \displaystyle\sum_{i,k}a_{ii}a_{kk} - \sum_{i=k}a_{ii}a_{kk}\\\\
						 &=& \left[tr(A)\right]^2 - \displaystyle\sum_{i=1} a_{ii}^2\\\\
						 &=& \left[tr(A)\right]^2 - a'a.
    \end{array}
    $$

    Luego,

    $$E\left(Z'AZ\right)^2 = \left(\mu^{(4)}-3\sigma^4\right)a'a+\sigma^4\left\{2tr\left(A^2\right)+\left[tr(A)\right]^2\right\}.$$

    Para $\Var(bZ)$, entonces

    $$\Var(bZ)=b\Var(Z)b' = bb'\sigma^2 = \left(\mu' A\right)\left(\mu'A\right)\sigma^2 = \mu'A^2\mu\sigma^2.$$\\
    Para calcular $\Cov\left(Z'AZ,bZ\right)$, notemos que $\E(Z)=0$, por lo que

    $$
    \begin{array}{rcl}
	\Cov\left(Z'AZ,bZ\right) &=& \Cov\left(\displaystyle\sum_{i,j}a_{ij}Z_iZ_j,\sum_k b_k Z_k\right)\\\\
				 &=& \displaystyle\sum_{i,j,k} \Cov(Z_iZ_j,Z_k)\\\\
				 &=& \displaystyle\sum_{i,j,k} a_{ij}b_k \E\left\{\left[Z_iZ_j-\E(Z_iZ_j)Z_k\right]\right\}\\\\
				 &=& \displaystyle\sum a_{ij}b_k \left[\E\left(Z_iZ_jZ_k\right)-\E(Z_k)\right]\\\\
				 &=& \displaystyle\sum a_{ij}b_k \left[\E\left(Z_iZ_jZ_k\right)\right]\quad \quad (\mbox{ya que }\E\left(Z_k\right)=0).
    \end{array}
    $$

    Es fácil saber que

    $$
    E(Z_iZ_jZ_k) =
	\begin{array}{rcl}
	    \mu^{(3)}, &\text{si} & i=j=k;\\\\
	    0, & &\mbox{los demás}.
	\end{array}
    $$

    Por lo tanto,

    $$
    \begin{array}{rcl}
	\Cov\left(Z'AZ,bZ\right) &=& \displaystyle\sum_{i=1}^n a_{ii}b_i \mu^{(3)}\\\\
				 &=& \mu^{(4)}\displaystyle\sum_{i=1}^n a_{ii}\mu'A_i \mu^{(3)}\\\\
				 &=& \displaystyle\sum_{i=1}^n a_{ii}A'_i\mu\mu^{(3)}\\\\
				 &=& a'A\mu\mu^{(3)}\\\\
    \end{array}
    $$
    La combinación de los resultados anteriores completa la demostración.
\end{teo}


\section{Distribución normal multivariante}

Una variable aleatorio $Y$ sigue una distribución normal $N(\mu,\sigma^2)$ si y sólo si la función de densidad de probabilidad de $Y$ es

\begin{tcolorbox}
    $$f(y)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y-\mu)^2}{\sigma^2}} \quad \mbox{para}\quad -\infty<y<\infty.$$
\end{tcolorbox}

La distribución acumulada de $Y$ se define como

\begin{tcolorbox}
    $$F(y)=P(Y\leq y) = \dfrac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^y e^{-\frac{(y-\mu)^2}{\sigma^2}}dy.$$
\end{tcolorbox}

La función generadora de momentos de la variable aleatoria normal $Y\sim N(\mu,\sigma)$ es

\begin{tcolorbox}
    $$M(t)=\E\left(e^{tY}\right)=e^{\mu t+\frac{1}{2}\sigma^2t^2}.$$
\end{tcolorbox}

Un vector aleatorio $\textbf{y}'=(y_1,y_2,\ldots,y_p)$ se dice que sigue la distribución normal multivariante si y sólo si su función de densidad de probabilidad tiene la siguiente forma

$$f(y_1,y_2,\ldots,y_p)=\dfrac{1}{(2\pi)^{p/2}|\sum|^{1/2}}e^{-\frac{1}{2}(y-\mu)'\sum^{-1}(y-\mu)},$$

donde $\sum=(\sigma_{ij})_{p\times p}$ es la matriz de covarianza de $\textbf{y}$, y la matriz inversa $\sum^{-1}$ existe.  $\mu'=(\mu_1,\mu_2,\ldots,\mu_p)$ es el vector medio de $\textbf{y}$.

Cuando $\sum$ es una matriz diagonal $\sum \mbox{diag}\left(\sigma_1^2,\sigma_2^2,\ldots,\sigma_p^2\right)$, o $\sigma_{ij}=0$ para todo $i\neq j$, entonces $y_1,y_2,\ldots,y_p$ no están correlacionados ya que es fácil saber que la función de densidad de $\textbf{y}$ puede ser escrito como un producto de $p$ funciones de densidad normal univariante:
$$\dfrac{1}{(2\pi)^{p/2}|\sum|^{1/2}}e^{-\frac{1}{2}(\textbf{y}-\mu)'\sum^{-1}(\textbf{y}-\mu)} = \prod_{i=1}^p \dfrac{1}{\sqrt{2\pi}\sigma_i}e^{-\frac{(y_i-\mu_i)^2}{\sigma_i^2}}.$$

Dado que la función de densidad del vector normal multivariante $\textbf{y}$ es un producto de las funciones de densidad de $y_1 , y_2 , \ldots , y_p$, son conjuntamente independientes. Para las variables normales multivariantes, las variables aleatorias normales no correlacionadas son conjuntamente independientes. Resumimos esto en el siguiente teorema:

%--------------------- Teorema 3.8
\begin{teo}
    Si un vector aleatorio $\textbf{y}'=(y_1,y_2,\ldots,y_p)$ siguiendo una distribución normal multivariante $N(\mu,\sum)$ y la matriz covarianza $\sum = \left(\sigma_{ij}\right)_{p\times p}$ es una matriz diagonal $\mbox{diag}\left(\sigma_{11},\sigma_{22},\ldots,\sigma_{pp}\right)$, entonces $y_1,y_2,\ldots,y_p$ son conjuntamente independientes.\\\\
	Demostración.-\; 
\end{teo}

