\chapter{Regresión lineal simple}

\section{Introducción}
Fue introducido por Francis Galton (1908). El modelo de regresión lineal simple está formado típicamente por:

$$y=\beta_0+\beta_1x+\epsilon.$$

Donde:\\

\begin{tabular}{rcl}

    $y$ &=& variable dependiente o variable de respuesta.\\
    $x$ &=& variable independiente o explicativo o predictor.\\ 
    $\beta_0$ &=& intercepto  $y$.\\
    $\beta_1$ &=& pendiente.\\ 
    $\epsilon$ &=& error aleatorio.

\end{tabular}

Una presentación más general de un modelo de regresión sería:

$$y=E(y)+\epsilon,$$

Donde: $E(y)$ es la esperanza matemática de la variable respuesta.Cuando $E(y)$ es una combinación lineal  de las variables explicativas $x_1,x_2,\ldots, x_k$ la regresión es una regresión lineal. \textbf{La supuestas clásicos sobre el término de error son \boldmath $\E(\epsilon)=0$ y varianza constante $\Var(\epsilon)=\sigma^2$}. El experimento típico para la regresión lineal simple es que observamos $n$ pares de datos $(x_1, y_1),(x_2, y_2), \ldots ,(x_n, y_n)$ de un experimento científico, y modelamos en términos de los $n$ pares de los datos se puede escribir como
$$y_i=\beta_0+\beta_1x_i+\epsilon_i\qquad \mbox{para}\; i=1,2,\ldots,n,$$
con $E(\epsilon_i)=0$ y $Var(\epsilon_i)=\sigma^2$. Todos los $\epsilon_i$ son independientes. Generalmente $\sigma^2$ no se conoce. 
Ahora debemos hallar buenos estimadores para $\beta_0$ y $\beta_1$.\\

\textbf{Obs:}
Se sabe que $\E(e)=0$ en un modelo de regresión lineal simple debido a la suposición de que, en promedio, los errores de regresión son igualmente positivos y negativos y se cancelan entre sí.


\section{Estimaciones por mínimos cuadrados}
El principal objetivo de los mínimos cuadrados para un modelo de regresión lineal simple es hallar los estimadores $b_0$ y $b_1$ tales que la suma de la distancia al cuadrados de la respuesta real $y_i$ y las respuesta de las pronosticadas $\hat{y}_i=\beta_0+\beta_1x_i$ alcanza el mínimo entre todas las opciones posibles de coeficientes de regresión $\beta_0$ y $\beta_1$. Es decir,
$$(b_0,b_1)=\arg \min_{(\beta_0,\beta_1)}\sum_{i=1}^n \left[\beta_0+\beta_1 x_i\right]^2.$$

Matemáticamente, las estimaciones de mínimos cuadrados de la regresión lineal simple se obtienen resolviendo el siguiente sistema:

\begin{equation}
    \dfrac{\partial}{\partial\beta_0}\sum_{i=1}^n \left[y_i-\left(\beta_0+\beta_ix_i\right)\right]^2=0
\end{equation}

\begin{equation}
    \dfrac{\partial}{\partial\beta_1}\sum_{i=1}^n \left[y_i-\left(\beta_0+\beta_ix_i\right)\right]^2=0
\end{equation}
\vspace{0.5cm}

Supongamos que $b_0$ y $b_1$ son soluciones del sistema de arriba, podemos describir la relación entre $x$ e $y$ por la regresión lineal $\hat{y}=b_0+b_1x$, el cual es llamado la \textbf{recta de regresión ajustada}. Es más conveniente resolver para $b_0$ y $b_1$ usando el modelo lineal centralizado:
$$y_i= \beta_0+\beta_1\overline{x}-\beta_1\overline{x}+\beta_1x_i+\epsilon_i \quad \Rightarrow \quad y_i=\beta_0^* + \beta_1(x_i-\overline{x})+\epsilon_i,$$
donde $\beta_0=\beta_0^*-\beta_1\overline{x}$. Necesitamos resolver para

$$\dfrac{\partial}{\partial\beta_0^*}\sum_{i=1}^n \left[y_i-\left(\beta_0^*+\beta_1(x_i+\overline{x})\right)\right]^2=0$$

$$\dfrac{\partial}{\partial\beta_1}\sum_{i=1}^n \left[y_i-\left(\beta_0^*+\beta_1(x_i+\overline{x})\right)\right]^2=0$$

Realizando la derivada parcial para $\beta_0$ y $\beta_1$ tenemos

$$\sum_{i=1}^n \left[y_i-(\beta_0^* + \beta_1(x_i-\overline{x}))\right]=0$$
$$\sum_{i=1}^n \left[y_i-(\beta_0^* + \beta_1(x_i-\overline{x}))\right](x_i-\overline{x})=0$$

Notemos que

\begin{equation}
    \sum_{i=1}^n y_i= n\beta_0^*+\sum_{i=1}^n \beta_1(x_i-\overline{x})=n\beta_0^*
\end{equation}

Por lo tanto, tenemos 

$$\beta_0^*=\dfrac{1}{n}\sum\limits_{i=1}^n y_i=\overline{y}.$$

Luego, sustituyendo $\beta_0^*$ por $\overline{y}$ en (2.3) obtenemos

$$\sum_{i=1}^n \left[y_i-(\overline{y} + \beta_1(x_i-\overline{x}))\right](x_i-\overline{x})=0$$

Después denotamos $b_0$ y $b_1$ las soluciones de los sistemas (2.1) y (2.2). Ahora, es fácil ver que
\begin{tcolorbox}
    \begin{equation}
	b_1=\dfrac{\sum\limits_{i=1}^n (y_i-\overline{y})(x_i-\overline{x})}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}=\dfrac{\dfrac{1}{n}\sum\limits_{i=1}^n (y_i-\overline{y})(x_i-\overline{x})}{\dfrac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x})^2}=\dfrac{S_{xy}}{S_{xx}}
    \end{equation}
\end{tcolorbox}

y

\begin{tcolorbox}
    \begin{equation}
	b_0=b_0^*-b_1\overline{x}=\overline{y}-b_1\overline{x}
    \end{equation}
\end{tcolorbox}
\vspace{0.5cm}

El valor ajustado de la regresión lineal simple es definida como $\hat{y}_i=b_0+b_1x_i.$ La diferencia entre $y_i$ y el valor ajustado $\hat{y}_i$ es $e_i=y_i-\hat{y}_i$, que se refiere al residuo de la regresión. Los residuos de regresión se pueden calcular a partir de las respuestas observadas $y_i$ y los valores ajustados $y_i$, por lo tanto, los residuos son observables. Cabe señalar que el término de error $\epsilon_i$ en el modelo de regresión no es observable. El error de regresión es la cantidad por la cual una observación difiere de su valor esperado; este último se basa en la población total de la que se eligió aleatoriamente la unidad estadística. El valor esperado, el promedio de toda la población, normalmente no es observable.\\

Un residual, por otro lado, es una estimación observable de un error no observable. El caso más simple implica una muestra aleatoria de n hombres cuyas alturas se miden. El promedio de la muestra se utiliza como una estimación del promedio de la población. Entonces, la diferencia entre la altura de cada hombre de la muestra y el promedio de la población no observable es un error, y la diferencia entre la altura de cada hombre de la muestra y el promedio de la muestra observable es un residuo. Dado que los residuales son observables, podemos usar los residuales para estimar el error del modelo no observable. La discusión detallada se proporcionará más adelante.

\section{Propiedades estadísticas de la estimación por mínimos cuadrados}

Primero discutiremos las propiedades estadísticas sin el supuesto de distribución del termino de error. Pero asumiremos que $\E(\epsilon_i)=0$, $\Var(\epsilon_i)=\sigma^2$ y $\epsilon_i$ para $i=1,2,\ldots,n$ son independientes.\\


%-------------------- Teorema 2.1
\begin{teo}
    El estimador de mínimos cuadrados $b_0$ es un estimador insesgado de $\beta_0$.\\\\

	Demostración.-\; 

	$$
	\begin{array}{rcl}
	    E(b_0) &=& E(\overline{y}-b_1\overline{x})\\\\
	    &=& E\left(\dfrac{1}{n}\displaystyle\sum_{i=1}^n y_i\right)-E\left(b_1\overline{x}\right)\\\\
		       &=& \dfrac{1}{n}\displaystyle\sum_{i=1}^n E(y_i)-\overline{x}E(b_1)\\\\
		   &=&\dfrac{1}{n}\displaystyle\sum_{i=1}^n (\beta_0+\beta_1x_i)-\beta_1\overline{x}\\\\
		   &=&\dfrac{1}{n} \displaystyle\sum_{i=1}^n \beta_0 + \beta_1 \dfrac{1}{n}\displaystyle\sum_{i=1}^nx_i-\beta_1\overline{x}\\\\
		   &=&\dfrac{1}{n} \displaystyle\sum_{i=1}^n \beta_0 + \beta_1 \dfrac{1}{n}\displaystyle\sum_{i=1}^nx_i-\beta_1\dfrac{1}{n}\displaystyle\sum_{i=1}^n x_i\\\\
		   &=& \dfrac{n\beta_0}{n}\\\\
		       &=&\beta_0.
	\end{array}
	$$
\end{teo}

%-------------------- Teorema 2.2
\begin{teo}
    El estimador de mínimos cuadrados $b_1$ es un estimador insesgado  de $\beta_1$.\\\\
	Demostración.-\;
	$$
	\begin{array}{rcll}
	    \E(b_1) &=& \E\left(\dfrac{S_{xy}}{S_{xx}}\right)&\\\\
		    &=& \dfrac{1}{S_{xx}} \E\left[\dfrac{1}{n}\displaystyle\sum_{i=1}^n(y_i-\overline{y})(x_i-\overline{x})\right]&\\\\
		    &=& \dfrac{1}{s_{xx}}\dfrac{1}{n}\E\left[\displaystyle\sum_{i=1}^n(x_i-\overline{x})y_i-\displaystyle\sum_{i=1}^n(x_i-\overline{x})\overline{y}\right]&\\\\
		    &=& \dfrac{1}{s_{xx}}\dfrac{1}{n}\E\left[\displaystyle\sum_{i=1}^n(x_i-\overline{x})y_i-\overline{y}\displaystyle\sum_{i=1}^n(x_i-\overline{x})\right]&\mbox{ya que } \overline{y} \mbox{ es constante}.\\\\
	\end{array}
	$$
	Sabemos que $\sum\limits_{i=1}^n (x_i-\overline{x})=\sum\limits_{i=1}^n x_i - n\overline{x}=\sum\limits_{i=1}^n x_i-n\left(\dfrac{1}{n}\sum\limits_{i=1}^n x_i\right)=0$, por lo que
	$$
	\begin{array}{rcl}
		   &=& \dfrac{1}{S_{xx}}\dfrac{1}{n}\displaystyle\sum_{i=1}^n (x_i-\overline{x}) \E(y_i)\\\\
		   &=& \dfrac{1}{S_{xx}}\dfrac{1}{n}\displaystyle\sum_{i=1}^n (x_i-\overline{x}) (\beta_0+\beta_1x_i)\\\\
		   &=& \dfrac{1}{S_{xx}}\dfrac{1}{n}\left[\beta_0\displaystyle\sum_{i=1}^n (x_i-\overline{x})  + \displaystyle\sum_{i=1}^n(x_i-\overline{x})\beta_1 x_i\right]\\\\
		   &=& \dfrac{1}{S_{xx}}\dfrac{1}{n}\displaystyle\sum_{i=1}^n (x_i-\overline{x})\beta_1x_i\\\\
		   &=& \dfrac{1}{S_{xx}}\dfrac{1}{n}\left[\displaystyle\sum_{i=1}^n\left(x_i-\overline{x}\right)\beta_1x_i-\displaystyle\sum_{i=1}^n\left(x_i-\overline{x}\right)\beta_1\overline{x}\right]\\\\
		   &=& \dfrac{1}{S_{xx}}\dfrac{1}{n}\displaystyle\sum_{i=1}^n (x_i-\overline{x})\beta_1(x_i-\overline{x})\\\\
		   &=& \dfrac{1}{S_{xx}}\dfrac{1}{n}\displaystyle\sum_{i=1}^n (x_i-\overline{x})^2\beta_1\\\\
		   &=& \dfrac{S_{xx}}{S_{xx}}\beta_1\\\\
		       &=&\beta_1.
	\end{array}
	$$
\end{teo}

%-------------------- Teorema 2.3
\begin{teo}
    $\Var(b_1) = \dfrac{\sigma^2}{nS_{xx}}.$\\\\

	Demostración.-\; Sea $X_1,X_2,\ldots,X_n$ IDD, con $Var(X_i)=\sigma_i^2$ para $i=1,2,\ldots,n$. Si $\displaystyle\sum_{i=1}^n a_iX_i.$ Entonces, 
	$$\Var\left(\displaystyle\sum_{i=1}^n a_iX_i\right)=\displaystyle\sum_{i=1}^n a_i^2\sigma_i^2.$$ 
	Por lo tanto,
	$$
	\begin{array}{rcll}
	    \Var(b_1) &=& \Var\left(\dfrac{S_{xy}}{S_{xx}}\right)&\\\\
		      &=& \left(\dfrac{1}{S_{xx}}\right)^2\Var\left[\dfrac{1}{n}\displaystyle\sum_{i=1}^n(y_i-\overline{y})(x_i-\overline{x})\right]&\\\\
		      &=& \dfrac{1}{S_{xx}^2}\Var\left[\dfrac{1}{n}\displaystyle\sum_{i=1}^n y_i(x_i-\overline{x})\right]&\\\\
		      &=& \dfrac{1}{S_{xx}^2}\dfrac{1}{n^2}\displaystyle\sum_{i=1}^n (x_i-\overline{x})^2\Var(y_i)&\\\\
		      &=& \dfrac{1}{S_{xx}^2}\dfrac{1}{n^2}\displaystyle\sum_{i=1}^n (x_i-\overline{x})^2\sigma^2\\\\
		      &=& \dfrac{\sigma^2}{nS_{xx}} = \dfrac{\sigma^2}{n\sum\limits_{i=1}^n \left(x_i-\overline{x}\right)^2}.
	\end{array}
	$$
\end{teo}

Es importante destacar que, en general, se espera que la varianza de $b_1$ sea menor a medida que aumenta el tamaño de la muestra, ya que se tienen más datos disponibles para estimar el coeficiente de regresión. Además, la varianza de $b_1$ también puede disminuir si la variabilidad de los valores de $x$ se reduce. Por lo tanto, para mejorar la precisión del estimador $b_1$, se pueden tomar medidas como aumentar el tamaño de la muestra o reducir la variabilidad de $x$.

%-------------------- Teorema 2.4
\begin{teo}[\boldmath El estimador de mínimos cuadrados $b_1$ e $\overline{y}$ no están correlacionados] Bajo el supuesto de normalidad de $y_i$ para $i = 1, 2, \ldots , n$, $b_1$ e $\overline{y}$ se distribuyen normalmente y son independientes.\\\\
    Demostración.-\; 
    $$
    \begin{array}{rcl}
	\Cov(b_1,\overline{y}) &=& \Cov\left(\dfrac{S_{xy}}{S_{xx}},\overline{y}\right)\\\\
			       &=& \dfrac{1}{S_{xx}}\Cov(S_{xy},\overline{y})\\\\
			       &=& \dfrac{1}{nS_{xx}}\Cov\left[\displaystyle\sum_{i=1}^n(x_i-\overline{x})(y_i-\overline{y}),\overline{y}\right]\\\\
			       &=& \dfrac{1}{n^2S_{xx}}\Cov\left[\displaystyle\sum_{i=1}^n (x_i-\overline{x})y_i,\displaystyle\sum_{i=1}^n y_i\right]\\\\
			       &=& \dfrac{1}{n^2S_{xx}}\displaystyle\sum_{i=1}^n (x_i-\overline{x})\Cov(y_i,y_j)\\\\
    \end{array}
    $$
    Notemos que $E(\epsilon_i)=0$ y $\epsilon_i$ son independientes. De donde por definición de covarianza, podemos escribir
    $$\Cov(y_i,y_j)=E\left\{[y_i-E(y_j)][y_j-E(y_j)]\right\}=E(\epsilon_i,\epsilon_j)=
    \left\{
	\begin{array}{rcl}
	    \sigma^2 &si& i=j\\\\
	    0 &si& i\neq j.
	\end{array}
    \right.
    $$
    Concluimos que
    $$\Cov(b_1,\overline{y})=\dfrac{1}{n^2}{S_{xx}}\sum_{i=1}^n (x_i-\overline{x})\sigma^2=0.$$
    Recuerde que \textbf{la correlación cero es equivalente a la independencia entre dos variables normales. Por lo tanto, concluimos que \boldmath $b_1$ e $\overline{y}$ son independientes}.
\end{teo}

 si se utiliza $b_1$ para estimar la pendiente de la recta de regresión e $\overline{y}$ para estimar el valor medio de la variable dependiente $Y$, la variabilidad en los errores de estas dos estimaciones no estarán correlacionadas.

%-------------------- Teorema 2.5
\begin{teo}
    $\Var(b_0) = \left(\dfrac{1}{n}+\dfrac{\overline{x}^2}{nS_{xx}}\right)\sigma^2$.\\\\
    Demostración.-\; Sea, $X_1,X_2,\ldots X_n$ IDD, y  $\E(X)=\mu$ y $\Var(X)=\sigma^2$. Entonces, la media muestral $\overline{X}$ es normal con media $\mu$ y varianza $\dfrac{\sigma^2}{n}.$ Por lo tanto,
	$$
	\begin{array}{rcl}
	    \Var(b_0) &=& \Var(\overline{y}-b_1\overline{x})\\\\
		      &=& \Var(\overline{y})+(\overline{x})^2\Var(b_1)\\\\
		      &=& \dfrac{\sigma^2}{n}+ \overline{x}^2 \dfrac{\sigma^2}{nS_{xx}}\\\\
		      &=& \left(\dfrac{1}{n}+\dfrac{\overline{x}^2}{nS_{xx}}\right)\sigma^2
	\end{array}
	$$
\end{teo}

Las propiedades 1-5, especialmente las varianzas de $b_0$ y $b_1$, son importantes cuando queremos hacer inferencias estadísticas sobre la intersección y la pendiente de la regresión lineal simple.\\

Dado que las varianzas de los estimadores de mínimos cuadrados $b_0$ y $b_1$ involucran la varianza del término de error en el modelo de regresión simple. Esta variación de error es desconocida para nosotros. Por lo tanto, necesitamos estimarlo. Ahora discutimos cómo estimar la varianza del término de error en el modelo de regresión lineal simple. Sea $y_i$ la variable de respuesta observada y $\hat{y}_i=b_0+b_1x_i$ , el valor ajustado de la respuesta. Tanto $y_i$ como $\hat{y}_i$ están disponibles para nosotros. El verdadero error $\sigma_i$ en el modelo no es observable y nos gustaría estimarlo. \textbf{La cantidad \boldmath$y_i - \hat{y}_i$ es la versión empírica del error $\epsilon_i$ . Esta diferencia es un residuo de regresión que juega un papel importante en el diagnóstico del modelo de regresión.} Proponemos la siguiente estimación de la varianza del error basada en $e_i$:
$$s^2 = \dfrac{1}{n-2}\sum_{i=1}^n \left(y_i-\hat{y}_i\right)^2$$
Tenga en cuenta que en el denominador es $n-2$. Esto hace que $s^2$ sea un estimador insesgado de la varianza del error $\sigma^2$. El modelo lineal tiene dos parámetros, por lo que, $n-2$ puede verse como $n-$ números de parámetros simples. En particular, en un modelo de regresión lineal múltiple con $p$ parámetros, el denominador debe ser $n - p$ para construir un estimador insesgado de la varianza del error $\sigma^2$.\\

El estimador insesgado $s^2$ para la regresión lineal simple será  demostrado en las siguientes derivaciones.

$$y_i-\hat{y}_i=y_i-b_0-b_ix_i=y_i-(\overline{y}-b_i\overline{x})-b_ix_i$$

Estamos suponiendo que $E(\epsilon)=0$; de lo que se sigue,

$$\sum_{i=1}^n\left(y_i-\hat{y}_i\right)=\sum_{i=1}^n \left(y_i-\overline{y}\right)=\sum_{i=1}^n \left(y_i-\overline{y}\right)-b_i\sum_{i=1}^n\left(x_i-\overline{x}\right)=0.$$

Demostremos este supuesto. Note que $\left(y_i-\hat{y}_i\right)x_i=\left[\left(y_i-\overline{y}\right)-b_i\left(x_i-\overline{x}\right)\right]x_i$, de donde
$$
\begin{array}[t]{rcl}
    \displaystyle\sum_{i=1}^n\left(y_i-\hat{y}_i\right)x_i &=& \displaystyle\sum_{i=1}^n\left[\left(y_i-\overline{y}\right)x_i-b_i\left(x_i-\overline{x}\right)\right]x_i\\\\
    \displaystyle\sum_{i=1}^n\left(y_i-\hat{y}_i\right) &=& \displaystyle\sum_{i=1}^n\left[\left(y_i-\overline{y}\right)x_i-b_i\left(x_i-\overline{x}\right)\right]\left(x_i-\overline{x}\right)\\\\
							&=& \displaystyle\sum_{i=1}^n\left(y_i-\overline{y}\right)\left(x_i-\overline{x}\right)-b_i\sum_{i=1}^n\left(x_i-\overline{x}\right)^2\\\\
							&=& n\left(S_{xy}-b_1S_{xx}\right)\\\\
							&=&n\left(S_{xy}-\dfrac{S_{xy}}{S_{xx}}S_{xx}\right)\\\\
							&=& 0.
\end{array}
$$

Para demostrar que $s^2$ es un estimador insesgado de la varianza del error, primero veamos que

$$\left(y_i-\hat{y}_i\right)^2=\left[\left(y_i-\overline{y}\right)-b_i\left(x_i-\overline{x}\right)\right]^2,$$

Por lo que

$$
\begin{array}{rcl}
    \displaystyle\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2 &=& \displaystyle\sum_{i=1}^n\left[\left(y_i-\overline{y}\right)-b_i\left(x_i-\overline{x}\right)\right]^2\\\\
							  &=& \displaystyle\sum_{i=1}^n\left(y_i-\overline{y}\right)^2-2b_i\displaystyle\sum_{i=1}^n\left(x_i-\overline{x}\right)\left(y_i-\overline{y}\right)+b_i^2\displaystyle\sum_{i=1}^n\left(x_i-\overline{x}\right)^2\\\\
							  &=& \displaystyle\sum_{i=1}^n\left(y_i-\overline{y}\right)^2-2nb_iS_{xy}+n b_i^2S_{xx}\\\\
							  &=& \displaystyle\sum_{i=1}^n\left(y_i-\overline{y}\right)^2-2n\dfrac{S_{xy}}{S_{xx}}S_{xy}+n\dfrac{S_{xy}^2}{S_{xx}^2}S_{xx}\\\\
							  &=& \displaystyle\sum_{i=1}^n\left(y_i-\overline{y}\right)^2-n\dfrac{S_{xy}^2}{S_{xx}}\\\\
\end{array}
$$

Después, ya que 

$$
\begin{array}{rcl}
    \left(y_i-\overline{y}\right)^2&=&\left[\beta_0+\beta_1x_i+\epsilon -\left(\beta_0+\beta_1\overline{x}+\overline{\epsilon}\right)\right]^2\\\\
				   &=&\left[\beta_1\left(x_i-\overline{x}\right)+\left(\epsilon_i-\overline{\epsilon}\right)\right]^2\\\\
				   &=& \beta_1^2\left(x_i-\overline{x}\right)^2+\left(\epsilon_i-\overline{\epsilon}\right)^2+2\beta_1\left(x_i-\overline{x}\right)\left(\epsilon_i-\overline{\epsilon}\right)\\\\
\end{array}
$$

Entonces, en vista que $E\left(\epsilon_i-\overline{\epsilon}\right)=0$ tenemos 

$$
\begin{array}{rcl}
    \E\left(y_i-\overline{y}\right)^2 &=& \beta_1^2\left(x_i-\overline{x}\right)^2+\E\left(\epsilon_i-\overline{\epsilon}\right)^2\\\\
				      &=& \beta_1^2\left(x_i-\overline{x}\right)^2 + \E\left(\epsilon_i-\overline{\epsilon}\right)^2 - \E^2\left(\epsilon_i-\overline{\epsilon}\right) + \E^2\left(\epsilon_i-\overline{\epsilon}\right)\\\\
				      &=& \beta_1^2\left(x_i-\overline{x}\right)^2 + \Var\left(\epsilon_i-\overline{\epsilon}\right)\\\\
				      &=& \beta_1^2\left(x_i-\overline{x}\right)^2 + \Var(\epsilon_i)+\Var(\overline{\epsilon})-2\Cov\left(\epsilon_i,\overline{\epsilon}\right)\\\\
				      &=& \beta_1^2\left(x_i-\overline{x}\right)^2 + \sigma^2+\dfrac{\sigma^2}{n}-2\dfrac{\sigma^2}{n}\\\\
				      &=& \beta_1^2\left(x_i-\overline{x}\right)^2+\dfrac{n-1}{n}\sigma^2,
\end{array}
$$

y

$$
\begin{array}{rcl}
    \displaystyle\sum_{i=1}^n \E\left(y_i-\overline{y}\right)^2 &=& n\beta_1^2 S_{xx} + \displaystyle\sum_{i=1}^n \dfrac{n-1}{n}\sigma^2\\\\
								&=& n\beta_1^2 S_{xx}+(n-1)\sigma^2.
\end{array}
$$

Además, se tiene

$$
\begin{array}{rcl}
    \E\left(S_{xy}\right) &=& \E\left[\dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)\left(y_i-\overline{y}\right)\right]\\\\
			  &=& \dfrac{1}{n}\E\left[\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)y_i\right]\\\\
			  &=& \dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)\E\left(y_i\right)\\\\
			  &=& \dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)\left(\beta_0+\beta_1x_i\right)\\\\
			  &=& \dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)\beta_0+\dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)\beta_1x_i\\\\
			  &=& \dfrac{1}{n}\beta_{0}\displaystyle\sum_{i=1}^n x_i-\dfrac{1}{n}\beta_0\displaystyle\sum_{i=1}^n x_i+\dfrac{1}{n}\beta_1\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)x_i\\\\
			  &=& \dfrac{1}{n}\beta_{0}\left[\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)x_i-\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)\overline{x}\right]\\\\
			  &=& \dfrac{1}{n}\beta_{0}\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)^2\\\\
			  &=& \dfrac{1}{n}\beta_{0}S_{xx}\\\\
\end{array}
$$

También; sea $X_1,X_2,\ldots,X_n$ IDD, con $Var(X_i)=\sigma_i^2$ para $i=1,2,\ldots,n$. Si $\displaystyle\sum_{i=1}^n a_iX_i.$ Entonces, 
	$$\Var\left(\displaystyle\sum_{i=1}^n a_iX_i\right)=\displaystyle\sum_{i=1}^n a_i^2\sigma_i^2.$$ 
	Por lo tanto,

$$
\begin{array}{rcl}
    \Var\left(S_{xy}\right)&=&\Var\left[\dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)y_i\right]\\\\
			   &=& \dfrac{1}{n^2}\displaystyle\sum_{i=1}^n\left(x_i-\overline{x}\right)^2\Var\left(y_i\right)\\\\
			   &=& \dfrac{1}{n}S_{xx}\sigma^2.
\end{array}
$$

Así, podemos escribir

$$
\begin{array}{rcl}
    \Var\left(S_{xy}^2\right) &=& \E\left(S_{xy}^2\right)-E^2\left(S_{xy}\right)\\\\
    \E\left(S_{xy}^2\right) &=& \Var\left(S_{xy}\right)+E^2\left(S_{xy}\right)\\\\
			    &=& \dfrac{1}{n}S_{xx}\sigma^2+\beta_{1}^2 S^2_{xx}.

\end{array}
$$

y

$$
\E\left(S_{xy}^2\right)=\dfrac{1}{n}S_{xx}\left(\sigma^2+n\beta_{1}^2 S_{xx}\right)
$$

Dado que $E\left(S_{xx}\right)=S_{xx}$, entonces

$$
\E\left(\dfrac{nS_{xy}^2}{S_{XX}}\right)=\sigma^2+n\beta_{1}^2 S_{xx}.
$$

Finalmente, $\E\left(\hat{\sigma}^2\right)$ es dado por:

$$
\begin{array}{rcl}
    E\left[\displaystyle\sum_{i=1}^n \left(y_i-\hat{y}_i\right)^2\right] &=& \E\left[\displaystyle\sum_{i=1}^n\left(y_i-\overline{y}\right)^2\right]-\E\left[n\dfrac{S_{xy}^2}{S_{xx}}\right]\\\\
    &=&n\beta_1^2 S_{xx}+(n-1)\sigma^2-n\beta_1^2S_{xx}-\sigma^2\\\\
								      &=&(n-2)\sigma^2.\\\\
\end{array}
$$

En otras palabras, probamos que

\begin{tcolorbox}
    $$\E\left(s^2\right)=\E\left[\dfrac{1}{n-2}\sum_{i=1}^2\left(y_i-\hat{y}\right)^2\right]=\sigma^2.$$
\end{tcolorbox}

Por lo tanto, $s^2$, la estimación de la varianza del error, es un estimador insesgado de la varianza del error $\sigma^2$ en la regresión lineal simple. Otra vista de elegir $n - 2$ es que en el modelo de regresión lineal simple hay $n$ observaciones y dos restricciones sobre estas observaciones:

\begin{enumerate}
    \item $\displaystyle\sum_{i=1}^n \left(y_i-\hat{y}\right)=0$,\\\\
    \item $\displaystyle\sum_{i=1}^n \left(y_i-\hat{y}\right)x_i=0$.\\\\
\end{enumerate}

Por lo tanto, la estimación de la varianza del error tiene $n - 2$ grados de libertad, que también es el número total de observaciones - el número total de parámetros en el modelo. Veremos características similares en la regresión lineal múltiple.


\section{Estimación de máxima verosimilitud}

El estimador de màxima versimilitud de una regresión lineal simple puede ser desarrollado si se supone que la variable dependiente $y_i$ tiene una distribución normal $y_i\sim N\left(\beta_0+\beta_1x_i,\sigma^2\right)$. La funciòn de similitud para $\left(y_1,y_2,\ldots,y_n\right)$ es dada por:

$$L=\prod_{i=1}^n f\left(y_i\right)=\dfrac{1}{(2\pi)^{\frac{n}{2}}\sigma^n}e^{\left(-\frac{1}{2\sigma^2}\right)\sum\limits_{i=1}^n \left(y_i-\beta_0-\beta_1x_i\right)^2}$$

Los estimadores de $\beta_0$ y $\beta_1$ que máximiza la función de similitud $L$ son equivalentes a los estimadores que minimizan la parte exponencial de la función de verosimilitud lo que produce los mismos estimadores que los estimadores de mìnimos cuadrados de la regresión lineal. Por lo tanto, bajo el supuesto de normalidad del término de error, los MLE de $\beta_0$ y $\beta_1$ y los estimadores de mínimos cuadrados de $\beta_0$ y $\beta_1$ son exactamente iguales.\\

Después de obtener $b_0$ y $b_1$, los valores MLE de los parámetros $\beta_0$ y $\beta_1$, podemos calcular el valor ajustado $\hat{y}_i$ y la función de probabilidad en términos de los valores ajustados.

$$L=\prod_{i=1}^n f\left(y_i\right)=\dfrac{1}{(2\pi)^{\frac{n}{2}}\sigma^n}e^{\left(-\frac{1}{2\sigma^2}\right)\sum\limits_{i=1}^n \left(y_i-\hat{y}_i\right)^2}.$$

Luego, tomamos la derivada parcial con respecto a $\sigma^2$ en la función logarítmica de verosimilitud $\log(L)$ y la igualamos a cero:

$$\dfrac{\partial \log(L)}{\partial \sigma^2}=-\dfrac{n}{2\sigma^2}+\dfrac{1}{2\sigma^4}\sum_{i=1}^n \left(y_i-\hat{y}_i\right)^2=0$$

La estimación de máxima verosimilitud MLE de $\sigma^2$ es $\hat{\sigma}^2=\dfrac{1}{n}\displaystyle\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2$. Notemos que es un estimador sesgado de $\sigma^2$. No así, 
$$s^2=\dfrac{1}{n-2}\displaystyle\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2$$ 
que es un estimador insesgado del error de la varianza $\sigma^2$. $\dfrac{n}{n-2}\hat{\sigma}^2$ es un estimador insesgado de $\sigma^2$. Observe también  que $\hat{\sigma}^2$ es una estimación asintóticamente insesgada de $\sigma^2$ , lo que coincide con la teoría clásica de MLE.


\section{Intervalo de confianza sobre la media de regresión y la predicción de regresión}

\begin{tcolorbox}
    Los modelos de regresión suelen construirse basándose en determinadas condiciones que deben verificarse para que el modelo se ajuste bien a los datos y pueda predecir la respuesta para un determinado regresor con la mayor precisión posible. Uno de los principales objetivos del análisis de regresión es utilizar el modelo de regresión ajustado para realizar predicciones.
\end{tcolorbox}

El intervalo de confianza de la predicción de regresión permite evaluar la calidad de la predicción. A menudo interesan los siguientes intervalos de confianza de la predicción de regresión:

\begin{itemize}
    \item Un intervalo de confianza para una sola línea de regresión.  
    \item Un intervalo de confianza para un solo valor futuro de y correspondiente a un valor elegido de $x$. 
    \item Una región de confianza para la línea de regresión como un todo.
\end{itemize}

Para analizar el intervalo de confianza para la línea de regresión, consideramos el valor ajustado de la línea de regresión en $x=x_0$, que es $\hat{y}\left(x_0\right)=b_0+b_1x_0$ y el valor medio de $x=x_0$ es $E\left(\hat{y}|x_0\right)=\beta_0+\beta_1x_0$. Tenga en cuenta que $b_1$ es independiente de $\overline{y}$ de donde,

$$
\begin{array}{rcl}
    \Var\left[\hat{y}\left(x_0\right)\right] &=& \Var\left(b_0+b_1x_0\right) \\\\
					     &=& \Var\left[\overline{y}-b_1\left(x_0-\overline{x}\right)\right]\\\\
					     &=& \Var\left(\overline{y}\right)+\left(x_0-\overline{x}\right)^2 \Var\left(b_1\right)\\\\
					     &=& \dfrac{1}{n}\sigma^2+\left(x_0-\overline{x}\right)^2\dfrac{1}{S_{xx}}\sigma^2\\\\
					     &=& \sigma^2\left[\dfrac{1}{n}+\dfrac{\left(x_0-\overline{x}\right)^2}{S_{xx}}\right]
\end{array}
$$

Reemplazando $\sigma$ por $s$, el error estandar de la predicción de regresión en $x_0$ está dado por:

$$s_{\hat{y}}\left(x_0\right)=s\sqrt{\dfrac{1}{n}+\dfrac{\left(x_0-\overline{x}\right)^2}{S_{xx}}}$$

Si $\epsilon\sim N\left(0, \sigma^2\right)$ el $(1-\alpha)100\%$ del intervalo de confianza en $\E\left(\hat{y}|x_0\right)=\beta_0+\beta_1x_0$ puede escribirse como:


$$\hat{y}\left(x_0\right)\pm t_{\alpha/2,n-2}\;s\sqrt{\dfrac{1}{n}+\dfrac{\left(x_0-\overline{x}\right)^2}{S_{xx}}}.$$

Ahora, analicemos el intervalo de confianza en la predicción de la regresión. Denotemos la predicción de la regresión en $x_0$ por $y_0$ y supongamos que $y_0$ es independiente de $\hat{y}\left(x_0\right)$, donde $y\left(x_0\right)=b_0+b_1x_0$, y $\E\left[y-\hat{y}\left(x_0\right)\right]=0$. Ya que $\Var(X-Y)=\Var(X)+\Var(Y)-2\Cov(X,Y)$ con $\Cov(X,Y)=0$. Entonces,

$$
\begin{array}{rcl}
    \Var\left[y_0-\hat{y}\left(x_0\right)\right] &=& \sigma^2+\sigma^2\left[\dfrac{1}{n}+\dfrac{\left(x_0-\overline{x}\right)^2}{S_{xx}}\right]\\\\
						 &=& \sigma^2\left[1+\dfrac{1}{n}+\dfrac{\left(x_0-\overline{x}\right)^2}{S_{xx}}\right]
\end{array}
$$

Bajo el supuesto de normalidad del término de error

$$\dfrac{y_0-\hat{y}\left(x_0\right)}{\sigma\sqrt{1+\dfrac{1}{n}+\dfrac{\left(x_0-\overline{x}\right)^2}{S_{xx}}}}\sim N(0,1).$$

Luego, sustituyendo $\sigma$ con $s$ se tiene

$$\dfrac{y_0-\hat{y}\left(x_0\right)}{s\sqrt{1+\dfrac{1}{n}+\dfrac{\left(x_0-\overline{x}\right)^2}{S_{xx}}}}\sim t_{n-2}.$$

Así, el $(1-\alpha)100\%$ del intervalo de confianza en la predicción de la regresión en $y_0$ puede ser expresada como

$$\hat{y}_0\pm t_{\alpha/2,n-2}\; s \sqrt{1+\dfrac{1}{n}+\dfrac{\left(x_0-\overline{x}\right)^2}{S_{xx}}}$$


\section{Inferencia estadística sobre parámetros de regresión}

Para dividir la varianza total $\sum\limits_{i=1}^n\left(y_i-\overline{y}\right)^2$, consideremos la ecuación de regresión ajustada $\hat{y}_i=b_0+b_1x_i$, donde $b_0=\overline{y}-b_1\overline{x}$ y $b_1=\dfrac{S_{xy}}{S_{xx}}.$ Podemos escribir

$$
\begin{array}{rcll}
    \overline{\hat{y}}&=&\dfrac{1}{n}\displaystyle\sum_{i=1}^n\hat{y}_i&\\\\
		      &=&\dfrac{1}{n}\displaystyle\sum_{i=1}^n\left[\left(\overline{y}-b_1\overline{x}\right)+b_1x_i\right]&\\\\
		      &=&\dfrac{1}{n}\displaystyle\sum_{i=1}^n\left[\overline{y}+b_1\left(x_i-\overline{x}\right)\right]& \mbox{ ya que } \frac{1}{n}\sum\limits x_i-\frac{1}{n}\sum\limits \overline{x}=0\\\\
		      &=&\overline{y}.&
\end{array}
$$

Para la respuesta de la regresión $y_i$, la varianza total es $\dfrac{1}{n}\sum\limits_{i=1}^n\left(y_i-\overline{y}\right)^2$. Note que la varianza total puede ser dividida en dos partes:
$$
\begin{array}{rcl}
    \dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(y_i-\overline{y}\right)^2 &=& \dfrac{1}{n}\displaystyle\sum_{i=1}^n \left[\left(y_i-\hat{y}_i\right)+\left(\hat{y}_i-\overline{y}\right)\right]^2\\\\
									  &=& \dfrac{1}{n}\displaystyle\sum_{i=1}^n \left[\left(y_i-\hat{y}_i\right)^2+\left(\hat{y}_i-\overline{y}\right)^2+2\left(\hat{y}_i-\overline{y}\right)\left(y_i-\hat{y}_i\right)\right]\\\\
									  &=& \dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(\hat{y}_i-\overline{y}\right)^2 + \dfrac{1}{n}\displaystyle\sum_{i=1}^n \left(y_i-\hat{y}_i\right)^2\\\\
									  &=&SS_{Reg} + SS_{Res}\\\\
									  &=& \mbox{Varianza explicada por la regresión + Varianza no explicada (del residuo).}
\end{array}
$$

Se puede demostrar que $\left[2\left(y_i-\hat{y}_i\right)\left(\hat{y}_i-\overline{y}\right)\right]$ es cero. Usando el hecho de que $\sum\limits_{i=1}^n\left(y_i-\hat{y}_i\right)=0$, tenemos

$$
\begin{array}{rcl}
    \displaystyle\sum_{i=1}^n \left(\hat{y}_i-\overline{y}\right)\left(y_i-\hat{y}_i\right) &=& \displaystyle\sum_{i=1}^n \hat{y}_i\left(y_i-\hat{y}_i\right)\\\\
											    &=& \displaystyle\sum_{i=1}^n \left[\left(b_0+b_1x_i\right)\left(y_i-\hat{y}_i\right)\right]\\\\
											    &=& b_0\displaystyle\sum_{i=1}^n \left(y_i-\hat{y}_i\right)+b_1\displaystyle\sum_{i=1}^n x_i\left(y_i-\hat{y}_i\right)\\\\
											    &=& b_1\displaystyle\sum_{i=1}^n x_i\left(y_i-\hat{y}_i\right)\\\\
											    &=& b_1\displaystyle\sum_{i=1}^n x_i\left(y_i-b_0-b_1x_i\right)\\\\
											    &=& b_1\displaystyle\sum_{i=1}^n x_i\left[\left(y_i-\overline{y}\right)-b_1\left(x_i-\overline{x}\right)\right]\\\\
											    &=& b_1\left[\displaystyle\sum_{i=1}^n x_i\left(y_i-\overline{y}\right)-\overline{x}\left(y_i-\overline{y}\right)-b_1x_i\left(x_i-\overline{x}\right)-b_1\overline{x}\left(x_i-\overline{x}\right)\right]\\\\
											    &=& b_1\left[\displaystyle\sum_{i=1}^n\left[\left(x_i-\overline{x}\right)\left(y_i-\overline{y}\right)\right]-b_1 \displaystyle\sum_{i=1}^n \left(x_i-\overline{x}\right)^2\right]\\\\
											    &=& b_1\left(S_{xy}-b_1S_{xx}\right)\\\\
											    &=& b_1\left[S_{xy}-\left(\dfrac{S_{xy}}{S_{xx}}\right)S_{xx}\right]\\\\
											    &=& 0.
\end{array}
$$

Los grados de libertad para $SS_{Reg}$ y $SS_{Res}$ se muestran a continuación:

$$
\begin{array}{ccccc}
    \hline\\
    SS_{Total} & = & SS_{Reg} & + & SS_{Res}\\\\
    n-1 & = & 1 & + & n-2\\\\
    \hline
\end{array}
$$

Para probar la hipótesis $H_0:\beta_1=0$ contra $H_1:\beta_1\neq 0$ es necesario asumir que $\epsilon_i\sim N\left(0,\sigma^2\right)$. La siguiente tabla contiene las distribuciones de $SS_{Reg}$, $SS_{Res}$ y $SS_{Total}$ bajo la hipótesis $H_0$.\\

$$
\begin{array}{ccc}
    \hline\\
    SS & df & distribucion\\\\
    \hline\\
    SS_{Reg} & 1 & \sigma^2\mathcal{X}^2_1\\\\
    SS_{Res} & n-2 & \sigma^2\mathcal{X}^2_{n-2}\\\\
    SS_{Total} & n-1 & \sigma^2\mathcal{X}^2_{n-1}\\\\
    \hline
\end{array}
$$

El test estadístico está dado por:

\begin{tcolorbox}
    $$F=\dfrac{\dfrac{SS_{Reg}}{1}}{\dfrac{SS_{Res}}{n-2}}\sim F_{1,n-2},$$
\end{tcolorbox}

que es una prueba $F$ unilateral superior. La tabla de abajo muestra un típico análisis de varianza (anova) de regresión 

$$
\begin{array}{ccccc}
    \hline\\
    Fuente & SS & df & MS & F\\\\
    \hline\\
    Regresion & SS_{Reg} & 1 & SS_{Reg}/1 & \dfrac{SS_{Reg}/1}{s^2}\\\\
    Residuo & SS_{Res} & n-2 & SS_{Res}/(n-2)=s^2 & \\\\
    Total & SS_{Total} & n-1 & \\\\
    \hline
\end{array}
$$

Donde:

$$
\begin{array}{rcl}
    SS &=& \mbox{Suma de cuadrados}\\
    df &=& \mbox{Grados de libertad}\\
    MS &=& \mbox{Media de cuadrados}\\
    F &=& \mbox{Estadístico $F$}.
\end{array}
$$
\vspace{0.5cm}

Para comprobar la pendiente de regresión $\beta_1$. Por los teoremas 2.2 y 2.3, se observa que $b_1$ sigue la distribución normal

$$b_1\sim N\left(\beta_1,\dfrac{\sigma^2}{SS_{xx}}\right)$$

y

\begin{tcolorbox}
    $$\left(\dfrac{b_1-\beta_1}{s/\sqrt{S_{xx}}}\right)=\left(\dfrac{b_1-\beta_1}{s}\right)\sqrt{S_{xx}}\sim t_{n-2}$$
\end{tcolorbox}

que puede utilizarse para comprobar $H_0:\beta_1=\beta_{1_0}$ contra $H_1:\beta_1\neq\beta_{1_0}$. Se puede utilizar un enfoque similar para comprobar el intercepto de la regresión. Bajo el supuesto de normalidad normalidad del término de error

$$b_0\sim N\left[\beta_0,\sigma^2\left(\dfrac{1}{n}+\dfrac{\overline{x}^2}{S_{xx}}\right)\right].$$

Así, podemos usar el estadístico  t test con $H_0:\beta_0=\beta_{0_0}$ contra $H_1:\beta_0\neq\beta_{0_0}$.

\begin{tcolorbox}
    $$t=\dfrac{b_0-\beta_0}{s\sqrt{\dfrac{1}{n}+\left(\dfrac{\overline{x}^2}{S_{xx}}\right)}}\sim t_{n-2}$$
\end{tcolorbox}

Es sencillo usar las distribuciones de $b_0$ y $b_1$ para obtener los $(1 - \alpha)100\%$ intervalos de confianza de $\beta_0$ y $\beta_1$:

\begin{tcolorbox}
    $$b_0\pm t_{\alpha/2,n-2}\; s\sqrt{\dfrac{1}{n}+\dfrac{\overline{x}^2}{S_{xx}}},$$
\end{tcolorbox}

y 

\begin{tcolorbox}
    $$b_1\pm t_{\alpha/2,n-2}\; s\sqrt{\dfrac{1}{S_{xx}}}.$$
\end{tcolorbox}

Supongamos que la linea de regresión pasa a través de $(0,\beta_0)$. Es decir, el intercepto en $y$ es una constante conocida $\beta_0$. El modelo es dado por $y_i=\beta_0+\beta_1x_i+\epsilon_i$ con una constante conocida $\beta_0$. Usando el principio de mínimos cuadrados podemos estimar $\beta_1$:

$$b_1=\dfrac{\displaystyle\sum x_iy_i}{\displaystyle\sum x_i^2}$$

En consecuencia, el siguiente estadístico t test se puede utilizar para probar $H_0: \beta_1=\beta_{1_0}$ contra $H_1:\beta_1\neq \beta_{1_0}$. Bajo el supuesto de normalidad sobre $\epsilon_i$

$$t=\dfrac{b_1-\beta_{1_0}}{s}\sqrt{\displaystyle\sum_{i=1}^nx_i^2}\sim t_{n-1}$$

Tenga en cuenta que solo tenemos un parámetro para el modelo de regresión del intercepto $y$ donde la estadística de t test tiene $n-1$ grados de libertad, que es diferente del modelo lineal simple con $2$ parámetros.\\\\

La cantidad $R^2$, definida a continuación, es una medida de ajusto de regresión:

\begin{tcolorbox}
    $$R^2=\dfrac{SS_{Reg}}{SS_{Total}}=\dfrac{\displaystyle\sum_{n=1}^n\left(\hat{y}_i-\overline{y}\right)^2}{\displaystyle\sum_{n=1}^n\left(y_i-\overline{y}\right)^2}=1-\dfrac{SS_{Res}}{SS_{Total}}$$
\end{tcolorbox}

Notemos que $0\leq R^2\leq 1$ que representa la proporción de variación total  explicada para el modelo de regresión.\\
la cantidad $CV=\dfrac{s}{\overline{y}}\times 100$ se llama coeficiente de variación, que también es una medida de la calidad del ajuste y representa la dispersión del ruido alrededor de la línea de regresión. \\

Ahora discutimos la inferencia simultánea en la regresión lineal simple. Tenga en cuenta que hasta ahora hemos discutido la inferencia estadística sobre $\beta_0$ y $\beta_1$ individualmente. La prueba individual significa que cuando probamos $H_0: \beta_0 = \beta_{0_0}$ solo probamos esta $H_0$ independientemente de los valores de $\beta_1$. Asimismo, cuando probamos $H_0: \beta_1 = \beta_{1_0}$ solo probamos $H_0$ independientemente de los valores de $\beta_0$. Si quisiéramos probar si una línea de regresión cae o no en cierta región, necesitamos probar la hipótesis múltiple: 
$$H_0: \beta_0 = \beta_{0_0}, \quad\beta_1 = \beta_{1_0}$$ 
simultáneamente. Esto cae en el ámbito de la inferencia múltiple. Para la inferencia múltiple sobre $\beta_0$ y $\beta_1$ notamos que

\begin{tcolorbox}
$$
\left(b_0-\beta_0,b_1-\beta_1\right)
\left(
    \begin{array}{cc}
	n & \displaystyle\sum_{i=1}^n x_i\\\\
	\displaystyle\sum_{i=1}^n x_i & \displaystyle\sum_{i=1}^n x_i^2
    \end{array}
\right)
{b_0-\beta_0\choose b_1-\beta_1}
\sim 2s^2F_{2,n-2}
$$
\end{tcolorbox}

Así, la región de confianza $(1-\alpha)100\%$ de $\beta_0$ y $\beta_1$ es dado por

\begin{tcolorbox}
$$
\left(b_0-\beta_0,b_1-\beta_1\right)
\left(
    \begin{array}{cc}
	n & \displaystyle\sum_{i=1}^n x_i\\\\
	\displaystyle\sum_{i=1}^n x_i & \displaystyle\sum_{i=1}^n x_i^2
    \end{array}
\right)
{b_0-\beta_0\choose b_1-\beta_1}
\leq 2s^2F_{2,n-2},
$$
\end{tcolorbox}

donde $F_{\alpha,2,n-2}$ es a cola superior del $\alpha$-ésimo punto porcentual de la distribución $F$. Tenga en cuenta que esta región de confianza es una elipse.


\section{Análisis residual y diagnóstico del modelo}
Una forma de verificar el comportamiento de un modelo de regresión es a través del residuo de regresión, es decir, $e_i = y_i - \hat{y}_i$ . Para la regresión lineal simple, un diagrama de dispersión de $e_i$ frente a $x_i$ proporciona un buen diagnóstico gráfico del modelo de regresión. Una distribución uniforme de los residuos en torno a la media cero indica un buen ajuste del modelo de regresión.\\

A continuación analicemos las características de los residuos si un modelo de regresión está mal especificado. Supongamos que el modelo correcto debería adoptar la forma cuadrática:

$$y_i=\beta_0+\beta_1\left(x_i-\overline{x}\right)+\beta_2x_i^2+\epsilon_i$$

con $\E(\epsilon_i)=0$.  Luego, sea un modelo de regresión lineal especificado incorrectamente como:

$$y_i=\beta_0+\beta_1\left(x_i+\overline{x}\right)+\epsilon_i^*$$

Entonces, $\epsilon_i^* = \beta_2x_i^2+\epsilon_i$, el cual es desconocido. Ahora, la media del error es distinto de cero y es una función de $x_i$. Del modelo cuadrático se tiene:

$$\overline{y}=\dfrac{1}{n}\sum\limits_{i=1}^n y_i=\dfrac{1}{n}\sum\limits_{i=1}^n \beta_0+\dfrac{1}{n}\sum\limits_{i=1}^n\left[\beta_1(x_i-\overline{x})\right]+\dfrac{1}{n}\sum\limits_{i=1}^n\left(\beta_2 x_i^2\right)+\dfrac{1}{n}\sum\limits_{i=1}^n \epsilon_i\quad \Rightarrow \quad \overline{y}=\beta_0+\beta_2\overline{x}^2+\overline{\epsilon}$$

tenemos:

$$b_0=\overline{y}-b_1(x_i-\overline{x})=\overline{y}=\beta_0+\beta_2\overline{x}^2+\overline{\epsilon}$$

y

$$
\begin{array}{rcl}
    \dfrac{S_{xy}}{S_{xx}}\; =\; b_1&=&\dfrac{\sum\limits_{i=1}^n(x_i-\overline{x})\left[\beta_0+\beta_1(x_i-\overline{x})+\beta_2x_i^2+\epsilon_i\right]}{S_{xx}}\\\\
    b_1 &=& \beta_1+\beta_2\dfrac{\sum\limits_{i=1}^n\left(x_i-\overline{x}\right)x_i^2}{S_{xx}}+\dfrac{\sum\limits_{i=1}^n\left(x_i-\overline{x}\right)\epsilon_i}{S_{xx}}
\end{array}
$$

Luego, es fácil ver que

$$\E(b_0)=\beta_0+\beta_2\overline{x}^2$$

y

$$E(b_1)=\beta_1+\beta_2\dfrac{\sum\limits_{i=1}^n\left(x_i-\overline{x}\right)x_i^2}{S_{xx}}.$$

Por lo tanto, los estimadores $b_0$ y $b_1$ son estimaciones sesgadas de $\beta_0$ y $\beta_1$. Supongamos que ajustamos el modelo por $\hat{y}_i=b_0+b_1(x_i-\overline{x})$, el residuo de regresión esperado viene dado por

$$
\begin{array}{rcl}
    \E(\epsilon_i) \; = \; \E(y_i-\hat{y}_i) &=& \left[\beta_0+\beta_1(x_i-\overline{x})+\beta_2x_i^2\right] - \left[\E(b_0)+E(b_1)(x_i-\overline{x})\right]\\\\
					     &=&\left[\beta_0+\beta_1(x_i-\overline{x})+\beta_2x_i^2\right] -  \left[\beta_0+\beta_2\overline{x}^2\right]-\left[\beta_1+\beta_2 \dfrac{\sum\limits_{i=1}^n(x_i-\overline{x})x_i^2}{S_{xx}}\right](x_i-\overline{x})\\\\
					     &=& \beta_2 \left[\left(x_i^2-\overline{x}^2\right)-\dfrac{\sum\limits_{i=1}^n(x_i-\overline{x})x_i^2}{S_{xx}}\right]
\end{array}
$$

Si $\beta_2=0$, entonces el modelo ajustado es correcto y $\E(y_i-\hat{y}_i)=0$. De lo contrario, el valor esperado de residual toma la forma cuadrática de $x_i$. Como resultado, la gráfica de residuos contra $x_i$ debe tener una curvatura de apariencia cuadrática.\\

La inferencia estadística en el modelo de regresión se basa en el supuesto de normalidad del término de error. Los estimadores de mínimos cuadrados y los MLE de los parámetros de regresión son exactamente idénticos solo bajo el supuesto de normalidad del término de error. Ahora, la pregunta es ¿cómo verificar la normalidad del término de error? Considere el residuo $y_i - \hat{y}_i$ : tenemos $\E(y_i - \hat{y}_i) = 0$ y

$$
\begin{array}{rcl}
    \Var(y_i-\hat{y}_i) &=& \Var(y_i)+\Var(\hat{y}_i)-2\Cov(y_i,\hat{y}_i)\\\\
			&=& \sigma^2+\sigma^2\left[\dfrac{1}{n}+\dfrac{\left(x_i-\overline{x}\right)^2}{S_{xx}}\right]-2\Cov\left[y_i,\overline{y}+b_1(x_i-\overline{x})\right]\\\\
\end{array}
$$

Calculando $\Cov\left[y_i,\overline{y}+b_1(x_i-\overline{x})\right]$, tenemos

$$
\begin{array}{rcl}
    \Cov\left[y_i,\overline{y}+b_1(x_i-\overline{x})\right] &=& \Cov(y_i,\overline{y}) + (x_i-\overline{x})\Cov(y_i,b_i)\\\\
							    &=& \dfrac{\sigma^2}{n} + (x_i-\overline{x})\Cov\left(y_i,\dfrac{S_{xy}}{S_{xx}}\right)\\\\
							    &=& \dfrac{\sigma^2}{n} + (x_i-\overline{x})\dfrac{1}{S_{xx}}\Cov\left[\displaystyle\sum_{i=1}^n\left(x_i-\overline{x}\right)(y_i-\overline{y})\right]\\\\
							    &=& \dfrac{\sigma^2}{n} + (x_i-\overline{x})\dfrac{1}{S_{xx}}\left[\displaystyle\sum_{i=1}^n\left(x_i-\overline{x}\right)y_i\right]\\\\
							    &=& \dfrac{\sigma^2}{n} + \dfrac{\left(x_i-\overline{x}\right)^2}{S_{xx}}\sigma^2.

\end{array}
$$

Así, la varianza del residuo es
\begin{tcolorbox}
    $$\Var(\epsilon_i)=\Var(y_i-\hat{y}_i)=\sigma^2\left[1-\left(\dfrac{1}{n}+\dfrac{\left(x_i-\overline{x}\right)^2}{S_{xx}}\right)\right],$$
\end{tcolorbox}

que se puede estimar por
\begin{tcolorbox}
    $$s_{e_i}=s\left[1-\left(\dfrac{1}{n}+\dfrac{\left(x_i-\overline{x}\right)^2}{S_{xx}}\right)\right].$$
\end{tcolorbox}

Si el término de error en la regresión lineal simple se especifica correctamente, es decir, el error se distribuye normalmente, los residuos estandarizados deberían comportarse como la variable aleatoria normal estándar. Por lo tanto, el cuantil de los residuos estandarizados en la regresión lineal simple será similar al cuantil de la variable aleatoria normal estandarizada. Así, la gráfica del cuantil de los residuos estandarizados versus el cuantil normal debe seguir una línea recta en el primer cuadrante si la suposición de normalidad en el término de error es correcta. Por lo general, se denomina gráfico normal y se ha utilizado como una herramienta útil para verificar la normalidad del término de error en la regresión lineal simple. Específicamente, podemos

\begin{enumerate}[(1)]
    \item Trazar el residuo ordenado $\dfrac{y_i-\hat{y}_i}{s}$ frente al cuantil de la normal $Z\left(\dfrac{i-0,375}{n+0.25}\right)$.
    \item Trace el residuo estandarizado ordenado $\dfrac{y_i-\hat{y}_i}{s_{e_i}}$ contra el cuantil normal $Z\left(\dfrac{i-0.375}{n+0.25}\right)$.
\end{enumerate}
